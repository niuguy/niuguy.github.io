<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-08-24T21:09:25.230Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Stochastic gradient descent</title>
    <link href="http://yoursite.com/2018/11/08/Stochastic-gradient-descent/"/>
    <id>http://yoursite.com/2018/11/08/Stochastic-gradient-descent/</id>
    <published>2018-11-08T11:14:28.000Z</published>
    <updated>2020-08-24T21:09:25.230Z</updated>
    
    <content type="html"><![CDATA[<p>Stochastic Gradient decent is one of the fundamental algorithm in deep learning. It is used when we perform optimization of the cost function.Suppose the function is  $ f(x) $</p><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/sgd-01.png" alt="sgd" /></p><p>As what illustrated above, we want to approach the minimum value of f(x) which is the point C. If we are now at point A, the derivation of A is f’(x)&gt;0, so we need to go right down which is the opposite direction of f’(x). If we stand at B,the derivation of A is f’(x)&gt;0, we should go left down which is also the opposite direction of f’(x).</p><p>According to the optimization strategy , we should update the parameter like this:</p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>−</mo><mi>η</mi><msup><mi>f</mi><mo mathvariant="normal">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = x - \eta f&#x27;(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span></span></span></span> is so-called the learning rate, it determines the size of steps we take to reach a minimum value of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></p><h2 id="batch-gradient"><a class="markdownIt-Anchor" href="#batch-gradient"></a> Batch Gradient</h2><p>Follow the rules of gradient descent, when we perform one update of parameters the intuitive strategy is to calculate the gradients for all the examples in the dataset and sum them up, this computation process is also called batch gradient.</p><p>The pseudocode described above looks like this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">    param_grad = calculate_gradient(loss_function, [examples]) </span><br><span class="line">    param = param - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>For each update we need to walk through all these examples,  if there are millions of them,  the computation cost would be a problem, let alone there usually are several epochs of updates.</p><h2 id="stochastic-gradient"><a class="markdownIt-Anchor" href="#stochastic-gradient"></a> Stochastic Gradient</h2><p>The inefficiency of the batch gradient leads to another strategy that is widely used in deep learning, which is stochastic gradient descent(SGD).</p><p>Instead of updating the params based on the gradients of all examples, SGD  updates the param after computing the gradient of one example,  what particular is the dataset should be shuffled in each epoch.</p><p>The poseudocode of SGC looks like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs): </span><br><span class="line">    np.random.shuffle(data) </span><br><span class="line">    <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">        param_grad = calculate_gradient(loss_function , example) </span><br><span class="line">        param = param - learning_rate * param_grad</span><br></pre></td></tr></table></figure><p>The algorithm is not as complicated as its name implies. Actually, it’s too simple that we may doubt about its effectiveness in convergence. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, the convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation.</p><p>A simple improvement of SGD is to perform an update for every mini-batch examples, which is called mini-batch gradient. It is a compromise between batch gradient and SGD and is proved effective in practices. The code looks like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs): </span><br><span class="line">    np.random.shuffle(data) </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> batches(data, batch_size = <span class="number">50</span>):</span><br><span class="line">        param_grad = calculate_gradient(loss_function , batch) </span><br><span class="line">        param = param - learning_rate * param_grad</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Stochastic Gradient decent is one of the fundamental algorithm in deep learning. It is used when we perform optimization of the cost func</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="AI算法" scheme="http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Backward propagation of Neural Network explained</title>
    <link href="http://yoursite.com/2018/11/08/Backward-propagation-of-Neural-Network-explained/"/>
    <id>http://yoursite.com/2018/11/08/Backward-propagation-of-Neural-Network-explained/</id>
    <published>2018-11-08T11:14:28.000Z</published>
    <updated>2020-08-23T20:27:48.553Z</updated>
    
    <content type="html"><![CDATA[<p>Backpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of ‘dark magic’ we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way.</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li>Overview of the architecture</li><li>Initialize parameters</li><li>Implement forward propagation</li><li>Compute Loss</li><li>Implement Backward propagation</li><li>Update parameters</li></ul><h2 id="1-The-architecture"><a href="#1-The-architecture" class="headerlink" title="1. The architecture"></a>1. The architecture</h2><p>This neural network I’m going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. </p><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-01.png" alt="image-20181107215309162"></p><p>The architecture in the math formula<br>$$<br>f(X)= Relu( Sigmoid(  X \times W_{ih} +b_{1}) \times W_{ho} + b_{2})<br>$$</p><p>##2.Initialize parameters</p><p>We take one example which has two features like below<br>$$<br>X= [x1, x2]= [0.1, 0.2]<br>$$<br>The parameters are taken randomly.<br>$$<br>W_{is}=</p><p>\begin{bmatrix}<br>W_{i1h1} &amp; W_{i1h2}\<br>W_{i2h1} &amp; W_{i2h2}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>0.7 &amp; 0.6\<br>0.5 &amp; 0.4<br>\end{bmatrix}<br>$$</p><p>$$<br>W_{sr}=<br>\begin{bmatrix}<br>W_{h1o1}&amp;W_{h1o2}\<br>W_{h2o1}&amp;W_{h2o2}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>0.4&amp;0.6\<br>0.9&amp;0.8<br>\end{bmatrix}<br>$$</p><p>$$<br>b_{1}=[b_{11}, b_{12}] =[0.5,0.6]<br>$$</p><p>$$<br>b_{2}= [b_{21}, b_{22}]<br>= [0.7, 0.9]<br>$$</p><h2 id="3-Forward-Propagation"><a href="#3-Forward-Propagation" class="headerlink" title="3. Forward Propagation"></a>3. Forward Propagation</h2><h3 id="3-1-Layer1"><a href="#3-1-Layer1" class="headerlink" title="3.1 Layer1:"></a>3.1 Layer1:</h3><p><img src="https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/bp-02.png" alt="image-20181108110049524"></p><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>$$<br>X \times  W_{ih}  + b_{1}<br>=<br>[x1, x2]<br>\times<br>\begin{bmatrix}<br>W_{i1h1} &amp; W_{i1h2}\<br>W_{i2h1} &amp; W_{i2h2}<br>\end{bmatrix}<br>+<br>[b_{11}, b_{12}]<br>=<br>[0.1, 0.2]<br>\times<br>\begin{bmatrix}<br>0.7 &amp; 0.6\<br>0.5 &amp; 0.4<br>\end{bmatrix}<br>+<br>[0.5, 0.6]<br>=<br>[0.67, 0.74]<br>$$</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>$$<br>Sigmoid(x)=1/(1+e^{-x})<br>$$</p><p>$$<br>[h_{out1},h_{out2}]<br>=<br>Sigmoid(X \times W_{ih} + b_{1}) =<br>[Sigmoid(0.67),Sigmoid(0.74)]<br>=<br>[0.6615,0.6770]<br>$$</p><h3 id="3-2-Layer2"><a href="#3-2-Layer2" class="headerlink" title="3.2 Layer2:"></a>3.2 Layer2:</h3><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-03.png" alt="image-20181108110231696"></p><h3 id="Linear-1"><a href="#Linear-1" class="headerlink" title="Linear"></a>Linear</h3><p>$$<br>[o_{in1}, o_{in2}]<br>=<br>[h_{out1}, h_{out2}] \times W_{ho}  + b_{2}<br>=<br>[h_{out1}, h_{out2}]<br>\times<br>\begin{bmatrix}<br>W_{h1o1} &amp; W_{h1o2}\<br>W_{h2o1} &amp; W_{h2o2}<br>\end{bmatrix}<br>+<br>[b_{21}, b_{22}]<br>=\<br>[0.6615, 0.6770]<br>\times<br>\begin{bmatrix}<br>0.7 &amp; 0.5\<br>0.6 &amp; 0.4<br>\end{bmatrix}<br>+<br>[0.5, 0.6]<br>=</p><p>[1.3693, 1.0391]<br>$$</p><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>$$<br>Softmax(x)<em>{j}= \frac{e^{xj}}{\sum</em>{i=1}^n e^{xi}} (j=1,2…n)<br>$$</p><p>$$<br>[o_{out1},o_{out2}]<br>=<br>Softmax(X\times W_{ho} + b_{2})=<br>[<br>\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} ,<br>\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}}<br>]<br>=<br>[0.5818,0.4182]<br>$$</p><p>##4. Compute Loss</p><p>The Loss function here we use is cross-entropy cost<br>$$<br>Crossentropy= -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right))<br>$$<br>The actual output should be<br>$$<br>[y_{1},y_{2}]<br>=<br>[1.00, 0.00]<br>$$<br>Since we only have one example, that means  ‘m = 1’,  the total loss is computed as follows :<br>$$<br>-\frac{1}{1} \sum\limits_{i = 1}^{1} (y_{1}\log\left(o_{out1}\right) + 0 +0 + 1<em>log(1-o_{out2}))=<br>-(1</em>log(0.5818)+0+0+1*log(1-0.4182))==0.4704<br>$$</p><h2 id="5-Backward-Propagation"><a href="#5-Backward-Propagation" class="headerlink" title="5. Backward Propagation"></a>5. Backward Propagation</h2><p>In this section, we will go through backward propagation stage by stage.</p><h3 id="5-1-Basic-Derivatives"><a href="#5-1-Basic-Derivatives" class="headerlink" title="5.1 Basic Derivatives"></a>5.1 Basic Derivatives</h3><p>####Sigmoid:</p><p>$$<br>\frac{\partial Sigmoid(x)}{\partial x}<br>=<br>\frac{\partial \frac{1}{(1+e^{-x})}}{\partial x}<br>=<br>\frac{ e^{-x}}{(1+e^{-x})^2}<br>=<br>(\frac{1+e^{-x}-1}{1+e^{-x}})\frac{1}{1+e^{-x}}<br>=<br>(1 - Sigmoid(x))\times Sigmoid(x)<br>$$</p><p>####Softmax:</p><p>At first we know:</p><p>For<br>$$<br>f(x)=\frac{g(x)}{h(x)}<br>$$</p><p>$$<br>f’(x) = \frac{g’(x)h(x)-g(x)h’(x)}{[h(x)]^2}<br>$$</p><p>Then the derivation of Softmax is<br>$$<br>\frac{\partial Softmax(x)}{\partial x1}=\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2}<br>$$</p><p>###5.2 The backward Pass</p><h4 id="5-2-1-Layer1-Layer2"><a href="#5-2-1-Layer1-Layer2" class="headerlink" title="5.2.1 Layer1-Layer2"></a>5.2.1 Layer1-Layer2</h4><p>####Weight derivatives with respect to the error</p><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-04.png" alt="image-20181107220124773"></p><p>Consider W<sub>ho</sub> , we want to know how W<sub>ho</sub> will affect the total error, aka the value of<br>$$<br>\frac{\partial E_{total}}{\partial W_{ho}}<br>$$<br><a href="https://en.wikipedia.org/wiki/Chain_rule">Chain Rule</a> states that:<br>$$<br>\frac{\partial z}{\partial x} =<br>\frac{\partial z}{\partial y}\cdot\frac{\partial y}{\partial x}<br>$$<br>So we have<br>$$<br>\frac{\partial E_{total}}{\partial W_{h2o1}}=<br>\frac{\partial E_{total}}{\partial o_{out1}}<br>\cdot \frac{\partial o_{out1}}{\partial o_{int1}}<br>\cdot \frac{\partial o_{int1}}{\partial W_{h2o1}}<br>$$<br>Let’s break this through stage by stage</p><ul><li>Stage1</li></ul><p>$$<br>\frac{\partial E_{total}}{\partial o_{out1}} =<br>\frac{\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\partial o_{out1}}+0<br>=-\frac{1}{o_{out1}}=-1/0.5818=-1.719<br>$$</p><ul><li>Stage2</li></ul><p>$$<br>\frac{\partial o_{1}}{\partial i_{1}}=<br>\frac{\partial Softmax(i_{1})}{\partial i_{1}}<br>=<br>\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}<br>=<br>\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}<br>=<br>\frac{e^{1.3693}\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433<br>$$</p><ul><li>Stage3 </li></ul><p>$$<br>\frac{\partial o_{in1}}{\partial W_{h2o1}} =<br>\frac{\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\partial W_{h2o1}}<br>= h_{out2}=0.6770<br>$$</p><p>Finally we apply the chain rule:<br>$$<br>\frac{\partial E_{total}}{\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831<br>$$<br>Let’s go through all the weights in Layer2<br>$$<br>W_{ho}’=<br>\begin{bmatrix}<br>W_{h1o1}’ &amp; W_{h1o2}’\<br>W_{h2o1}’ &amp; W_{h2o2}’<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial E_{total}}{\partial W_{h1o1}} &amp; \frac{\partial E_{total}}{\partial W_{h1o2}} \<br>\frac{\partial E_{total}}{\partial W_{h2o1}} &amp; \frac{\partial E_{total}}{\partial W_{h2o2}}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial E_{total}}{\partial o_{out1}}<br>\cdot \frac{\partial o_{out1}}{\partial o_{in1}}<br>\cdot \frac{\partial o_{in1}}{\partial W_{h1o1}}<br>&amp;<br>\frac{\partial E_{total}}{\partial o_{out2}}<br>\cdot \frac{\partial o_{out2}}{\partial o_{in2}}<br>\cdot \frac{\partial o_{in2}}{\partial W_{h1o2}}<br>\<br>\frac{\partial E_{total}}{\partial o_{out1}}<br>\cdot \frac{\partial o_{out1}}{\partial o_{in1}}<br>\cdot \frac{\partial o_{in1}}{\partial W_{h2o1}}<br>&amp;<br>\frac{\partial E_{total}}{\partial o_{out2}}<br>\cdot \frac{\partial o_{out2}}{\partial o_{in2}}<br>\cdot \frac{\partial o_{in2}}{\partial W_{h2o2}}<br>\end{bmatrix}<br>\<br>=<br>\begin{bmatrix}<br>-1.719<em>0.2433</em>0.6615 &amp; -2.3912<em>0.2433</em>0.6615 \<br>-1.719<em>0.2433</em>0.6770 &amp; -2.3912<em>0.2433</em>0.6770<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>-0.2767 &amp; -0.3733\<br>-0.2831 &amp; -0.3853<br>\end{bmatrix}<br>$$</p><h4 id="Update-weights-according-to-learning-rate"><a href="#Update-weights-according-to-learning-rate" class="headerlink" title="Update weights according to learning rate"></a>Update weights according to learning rate</h4><p>Our training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02.</p><p>We got the updated weight matrix as folows<br>$$<br>W_{ho}^* =<br>\begin{bmatrix}<br>W_{h1o1} - \eta W_{h1o1}’ &amp; W_{h1o2} - \eta W_{h1o2}’\<br>W_{h2o1} - \eta W_{h2o1}’ &amp; W_{h2o2} - \eta W_{h2o2}’<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>0.4 - 0.02*(-0.2767)&amp;0.6 - 0.02*(-0.3733)\<br>0.9 - 0.02*(-0.2831)&amp;0.8 - 0.02*(-0.3853)<br>\end{bmatrix}\<br>=<br>\begin{bmatrix}<br>0.4055 &amp; 0.6075\<br>0.9057 &amp; 0.8077<br>\end{bmatrix}<br>$$<br>That is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows.</p><p>####5.2.2 Layer0(Input Layer) - Layer1 </p><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-05.png" alt="image-20181107222954838"></p><p>Follow the path of the previous chapter</p><ul><li>Stage1:</li></ul><p>$$<br>\frac{\partial h_{out1}}{\partial h_{in1}}<br>=<br>\frac{\partial Sigmoid(h_{in1})}{\partial h_{in1}}<br>=<br>Sigmoid(h_{in1})<em>(1-Sigmoid(h_{in1}))<br>=\<br>Sigmoid(0.67)</em>(1-Sigmoid(0.67))=0.2239<br>$$</p><ul><li>Stage2:</li></ul><p>$$<br>\frac{\partial h_{in1}}{\partial W_{i2h1}}<br>=<br>\frac{\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\partial W_{i2h1}}<br>=<br>x2=0.2<br>$$</p><p>Apply the chain rule:<br>$$<br>\frac{\partial E_{total}}{\partial W_{i2h1}}=<br>\frac{\partial E_{total}}{\partial h_{out1}}<br>\cdot \frac{\partial h_{out1}}{\partial h_{in1}}<br>\cdot \frac{\partial h_{in1}}{\partial W_{i2h1}}<br>$$<br>We already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction.<br>$$<br>\frac{\partial E_{total}}{\partial h_{out1}}<br>=<br>\frac{\partial E_{total}}{\partial o_{out1}}<br>\frac{\partial o_{out1}}{\partial o_{in1}}<br>\frac{\partial o_{in1}}{\partial h1_{out}}<br>$$<br>We have computed the first and second results, and the third one is merely a deviation of the linear function<br>$$<br>\frac{\partial E_{total}}{\partial h_{out1}}<br>=<br>-1.719<em>0.2433</em><br>\frac{\partial(h_{out1}<em>W_{h1o1} + h_{out2}<em>W_{h2o1})}{\partial h_{out1}}<br>=\<br>-1.719</em>0.2433</em>W_{h1o1}<br>=-1.719<em>0.2433</em>0.4<br>=-0.1673<br>$$<br>Then we got<br>$$<br>\frac{\partial E_{total}}{\partial W_{i2h1}}= -0.1673<em>0.2239 * 0.2=-0.0075<br>$$<br>Similarly , we can get the Layer0-Layer1 derivatives with respective to the total error<br>$$<br>W_{ih}’<br>=<br>\begin{bmatrix}<br>\frac{\partial E_{total}}{\partial W_{i1h1}} &amp; \frac{\partial E_{total}}{\partial W_{i1h2}}\<br>\frac{\partial E_{total}}{\partial W_{i2h1}} &amp; \frac{\partial E_{total}}{\partial W_{i2h2}}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial E_{total}}{\partial h_{out1}}<br>\cdot \frac{\partial h1_{out}}{\partial h1_{in}}<br>\cdot \frac{\partial h1_{in}}{\partial W_{i1h1}}<br>&amp;<br>\frac{\partial E_{total}}{\partial h_{out2}}<br>\cdot \frac{\partial h2_{out}}{\partial h2_{in}}<br>\cdot \frac{\partial h2_{in}}{\partial W_{i1h2}}<br>\<br>\frac{\partial E_{total}}{\partial h_{out1}}<br>\cdot \frac{\partial h_{out1}}{\partial h_{in1}}<br>\cdot \frac{\partial h_{in1}}{\partial W_{i2h1}}<br>&amp;<br>\frac{\partial E_{total}}{\partial h_{out2}}<br>\cdot \frac{\partial h_{out2}}{\partial h_{in2}}<br>\cdot \frac{\partial h_{in2}}{\partial W_{i2h2}}<br>\end{bmatrix}<br>\<br>=<br>\begin{bmatrix}<br>-0.1673*0.2239 * 0.1 &amp;  -0.4654*0.2187</em>0.1\<br>-0.1673<em>0.2239 * 0.2 &amp;  -0.4654*0.2187</em>0.2<br>\end{bmatrix}<br>\<br>=<br>\begin{bmatrix}<br>-0.0037 &amp;  -0.0102\<br>-0.0075 &amp;  -0.0204<br>\end{bmatrix}<br>$$</p><h4 id="Update-weights-according-to-learning-rate-1"><a href="#Update-weights-according-to-learning-rate-1" class="headerlink" title="Update weights according to learning rate"></a>Update weights according to learning rate</h4><p>Update the weights with learning rate 0.02，we got the final weight matrix<br>$$<br>W_{ih}^*=<br>\begin{bmatrix}<br>W_{i1h1} - \eta (W_{i1h1}’) &amp;  W_{i1h2} - \eta (W_{i1h2}’)\<br>W_{i2h1} - \eta (W_{i2h1}’) &amp;  W_{i2h2} - \eta (W_{i2h2}’)<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>(0.7 -0.2*(-0.0037))&amp; (0.6 - 0.2 <em>(-0.0102)) \<br>(0.5 - 0.2</em>(-0.0075)) &amp; (0.4 - 0.2 * (-0.0204))<br>\end{bmatrix}<br>\<br>=<br>\begin{bmatrix}<br>0.7007 &amp; 0.6020\<br>0.5015 &amp; 0.4041<br>\end{bmatrix}<br>$$</p><p>###5.3 Wrap up</p><p>Finally we get all the weights updated<br>$$<br>W_{ho}^* =<br>\begin{bmatrix}<br>0.4055 &amp; 0.6075\<br>0.9057 &amp; 0.8077<br>\end{bmatrix}<br>$$</p><p>$$<br>W_{ih}^*=<br>\begin{bmatrix}<br>0.7007 &amp; 0.6020\<br>0.5015 &amp; 0.4041<br>\end{bmatrix}<br>$$</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><ul><li>Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these “smart” gates make it possible for complicated prediction or classification tasks.</li><li>The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates “vanish” significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice.</li><li>If we look back to the computing process,  a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions.</li></ul><h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><ol><li><a href="https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c">https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c</a>.</li><li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>.  </li><li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></li><li><a href="http://cs231n.github.io/optimization-2/">http://cs231n.github.io/optimization-2/</a> I</li><li><a href="https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/">https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/</a> </li><li><a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb">https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Backpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of ‘dark magic’ we are not able to under</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="AI算法" scheme="http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to plot counts of each value</title>
    <link href="http://yoursite.com/2018/10/22/Pandas-How-to-plot-counts-of-each-value/"/>
    <id>http://yoursite.com/2018/10/22/Pandas-How-to-plot-counts-of-each-value/</id>
    <published>2018-10-22T19:40:47.000Z</published>
    <updated>2020-08-23T20:26:15.045Z</updated>
    
    <content type="html"><![CDATA[<ol><li>With pandas build in function(actually matplotlib)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">% matplotlib inline</span><br><span class="line">col_values = (<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;y&#x27;</span> , <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>)</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;col&#x27;</span>:col_values&#125;)</span><br><span class="line">df[<span class="string">&#x27;col&#x27;</span>].value_counts().plot.bar()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz.png" alt="xyz"></p><ol start="2"><li>With seaborn</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.countplot(df[<span class="string">&#x27;col&#x27;</span>])</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz2.png" alt="xyz"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;With pandas build in function(actually matplotlib)&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to calculate datetime difference in years</title>
    <link href="http://yoursite.com/2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/"/>
    <id>http://yoursite.com/2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/</id>
    <published>2018-10-22T15:43:34.000Z</published>
    <updated>2020-08-23T20:26:45.938Z</updated>
    
    <content type="html"><![CDATA[<p>Suppose you got a person’s regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result.</p><ol><li>With the help of relativedelta</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dateutil <span class="keyword">import</span> relativedelta</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df.index:</span><br><span class="line">    df.loc[i, <span class="string">&#x27;age&#x27;</span>] = relativedelta.relativedelta(df.loc[i, <span class="string">&#x27;regDate&#x27;</span>], df.loc[i, <span class="string">&#x27;DateOfBirth&#x27;</span>]).years</span><br><span class="line">   </span><br></pre></td></tr></table></figure><ol start="2"><li>With the help of np.timedelta64</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;age&#x27;</span>] = (df[<span class="string">&#x27;regDate&#x27;</span>] - df[<span class="string">&#x27;DateOfBirth&#x27;</span>])/np.timedelta64(<span class="number">1</span>, <span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"><span class="comment"># You may need to convert to integer</span></span><br><span class="line">df[<span class="string">&#x27;age&#x27;</span>].apply(np.int64)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Suppose you got a person’s regist time in one column and his birth date in another column, now you need to calculate his age when he did </summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to list all columns</title>
    <link href="http://yoursite.com/2018/10/19/Pandas-How-to-list-all-columns/"/>
    <id>http://yoursite.com/2018/10/19/Pandas-How-to-list-all-columns/</id>
    <published>2018-10-19T21:31:23.000Z</published>
    <updated>2020-08-23T20:26:23.016Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(dataframe.columns.values)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span cl</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas] How to import from Sql Server</title>
    <link href="http://yoursite.com/2018/10/18/Pandas-How-to-import-from-Sql-Server/"/>
    <id>http://yoursite.com/2018/10/18/Pandas-How-to-import-from-Sql-Server/</id>
    <published>2018-10-18T19:08:25.000Z</published>
    <updated>2020-08-23T20:26:28.061Z</updated>
    
    <content type="html"><![CDATA[<p>We need to rely on pyodbc, the sample code is as belows.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyodbc</span><br><span class="line">cnxn = pyodbc.connect(<span class="string">&#x27;DRIVER=&#123;ODBC Driver 13 for SQL Server&#125;;SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Write SQL and execute with pandas.read_sql</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">query = <span class="string">&quot;SELECT * from a&quot;</span></span><br><span class="line">df = pd.read_sql(query, sql_conn)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;We need to rely on pyodbc, the sample code is as belows.&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cl</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Basics of words embedding</title>
    <link href="http://yoursite.com/2018/10/01/Basics-of-words-embedding/"/>
    <id>http://yoursite.com/2018/10/01/Basics-of-words-embedding/</id>
    <published>2018-10-01T10:14:28.000Z</published>
    <updated>2020-08-23T20:27:33.747Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Why-embedding"><a href="#Why-embedding" class="headerlink" title="Why embedding"></a>Why embedding</h3><p>Natural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:</p><ol><li>These encodings provide no useful information regarding the <strong>relationships</strong> that may exist between the individual symbols.</li><li>Discrete ids furthermore lead to <strong>data sparsity</strong>. We may need more data to train statistical models successfully.</li></ol><p>To address these two problems, word embeddings provide a solution to  represent words and their relative meanings densely.</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>Embedding derives from <a href="https://en.wikipedia.org/wiki/Vector_space_model">Vector Space Models(VSMs)</a>,  one of its well-known schemes is <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Tf-Idf</a> weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that <strong>words that appear in the same contexts share semantic meaning</strong>. </p><p>VSMs have two main approaches: Count-based methods and predictive methods.  Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense <em>embedding vectors</em> (considered parameters of the model).</p><p>Among all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular.</p><h3 id="Count-based-Embedding"><a href="#Count-based-Embedding" class="headerlink" title="Count-based Embedding"></a>Count-based Embedding</h3><p>GloVe is an <strong>unsupervised learning algorithm</strong> for obtaining vector representations for words.  The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words  to capture as much as possible the meaning specified by the juxtaposition of two words.</p><p>The <a href="https://nlp.stanford.edu/projects/glove/">project page of glove</a>  gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration:</p><blockquote><p>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.</p><p>Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. </p><p>It is necessary for a model to associate more than a single number to the word pair.</p></blockquote><p>Training GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost.  <a href="https://nlp.stanford.edu/projects/glove/">project page of glove</a> also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>‘’the’’ is followed by 100 floats which are the vector values of this word</p><p>We can build a dict whose key is words and value is their glove vector</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">embeddings_index = dict()</span><br><span class="line">f = open(<span class="string">&#x27;./glove.6B.100d.txt&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = asarray(values[<span class="number">1</span>:], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>When we need to build a embedding layer , we just look up the vectors  for input words in the dict.</p><h3 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h3><p>Word2vec, as illustrated in the first part,  is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. ‘mat’) from source context words (‘the cat sits on the’), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore,  CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. </p><p>We will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model.</p><p><img src="https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png" alt="Skip-gram Neural Network Architecture"></p><p>The Skip-gram model is trained like this,  Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. </p><p>The coding example of how to build and train the Skip-gram model can be found <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">here</a></p><p>Next post I will try to use embedding to solve an interesting real world problem.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Why-embedding&quot;&gt;&lt;a href=&quot;#Why-embedding&quot; class=&quot;headerlink&quot; title=&quot;Why embedding&quot;&gt;&lt;/a&gt;Why embedding&lt;/h3&gt;&lt;p&gt;Natural language processin</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="AI算法" scheme="http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>What is tf.data and how to use</title>
    <link href="http://yoursite.com/2018/09/26/What-is-tf-data-and-how-to-use/"/>
    <id>http://yoursite.com/2018/09/26/What-is-tf-data-and-how-to-use/</id>
    <published>2018-09-26T19:30:31.000Z</published>
    <updated>2020-08-23T20:28:15.157Z</updated>
    
    <content type="html"><![CDATA[<p>Tf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. </p><p>Here is the defination of Dataset given by tensorflow.org</p><blockquote><p>A <code>Dataset</code> can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a “logical plan” of transformations that act on those elements.</p></blockquote><p>To summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed,  the following part will explain that as well as some useful APIs in preprocessing data.</p><h3 id="1-Data-input"><a href="#1-Data-input" class="headerlink" title="1. Data input"></a>1. Data input</h3><p>Dataset can be built from several sources including csv file, numpy array and tensors.</p><h4 id="From-CSV"><a href="#From-CSV" class="headerlink" title="From CSV"></a>From CSV</h4><p>Tf.data provides a convenient API  make_csv_dataset to read records from one or more csv files.</p><p>Suppose the csv file is </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a,b,c,d</span><br><span class="line">how,are,you,mate</span><br><span class="line">I,am,fine,thanks</span><br></pre></td></tr></table></figure><p>We can build a dataset from the above csv in the following way</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>Here batch_size represents how many records would be aquired in a batch</p><p>We can use Iterator to see what contains in this dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = dataset.make_one_shot_iterator().get_next()</span><br><span class="line">print(batch[<span class="string">&#x27;a&#x27;</span>])</span><br></pre></td></tr></table></figure><p>The result is </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([&#39;how&#39; &#39;I&#39;], shape&#x3D;(2,), dtype&#x3D;string)</span><br></pre></td></tr></table></figure><p>make_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">how,are,you,mate</span><br><span class="line">I,am,fine,thanks</span><br></pre></td></tr></table></figure><p>We can set <code>header=False</code>and <code>column_names=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;]</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset2 &#x3D; tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size&#x3D;2, header&#x3D;False,column_names&#x3D;[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;])</span><br></pre></td></tr></table></figure><p>Dataset2 should have the same value with dataset1</p><h4 id="From-Tensor-slices"><a href="#From-Tensor-slices" class="headerlink" title="From Tensor slices"></a>From Tensor slices</h4><p>We can also create a dataset from tensors, the related API is  tf.data.Dataset.from_tensor_slices()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([<span class="number">10</span>, <span class="number">5</span>]))</span><br></pre></td></tr></table></figure><p>Actually the input of this API is not necessarily tensors,  numpy arrays are also adaptable .</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset3 &#x3D; tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5)))</span><br></pre></td></tr></table></figure><h3 id="2-Data-consuming"><a href="#2-Data-consuming" class="headerlink" title="2. Data consuming"></a>2. Data consuming</h3><p>The only way to retrieve the data is Iterator(),  Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator"><code>make_one_shot_iterator</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator"><code>make_initializable_iterator</code></a>.</p><h4 id="make-one-shot-iterator"><a href="#make-one-shot-iterator" class="headerlink" title="make_one_shot_iterator"></a>make_one_shot_iterator</h4><p>The examples can be find in the first part when we show how to import csv files</p><h4 id="make-initializable-iterator"><a href="#make-initializable-iterator" class="headerlink" title="make_initializable_iterator"></a>make_initializable_iterator</h4><p>Compared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using a placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>,<span class="number">2</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(x)</span><br><span class="line">data = np.random.sample((<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">iter = dataset.make_initializable_iterator() <span class="comment"># create the iterator</span></span><br><span class="line">el = iter.get_next()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># feed the placeholder with data</span></span><br><span class="line">    sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) </span><br><span class="line">    print(sess.run(el)) <span class="comment"># output [ 0.11342909, 0.81430183]</span></span><br></pre></td></tr></table></figure><h3 id="3-Data-proprocessing"><a href="#3-Data-proprocessing" class="headerlink" title="3. Data proprocessing"></a>3. Data proprocessing</h3><p>Tf.data provides several tools for data preprocessing such as batch and shuffle</p><h4 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h4><p>dataset.batch(BATCH_SIZE)  given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BATCHING</span></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line">BATCH_SIZE = <span class="number">2</span></span><br><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># make a dataset from a numpy array</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)</span><br><span class="line">iter = dataset.make_one_shot_iterator()</span><br><span class="line">iter.get_next()</span><br></pre></td></tr></table></figure><p>Output</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: id=<span class="number">102</span>, shape=(<span class="number">2</span>,), dtype=int64, numpy=array([<span class="number">1</span>, <span class="number">2</span>])&gt;</span><br></pre></td></tr></table></figure><h4 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h4><p>When preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">2</span></span><br><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># make a dataset from a numpy array</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = <span class="number">10</span>).batch(BATCH_SIZE)</span><br><span class="line">iter = dataset.make_one_shot_iterator()</span><br><span class="line">iter.get_next()</span><br></pre></td></tr></table></figure><p>Output</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: id=<span class="number">115</span>, shape=(<span class="number">2</span>,), dtype=int64, numpy=array([<span class="number">2</span>, <span class="number">3</span>])&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Tf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]Handle missing data</title>
    <link href="http://yoursite.com/2018/09/23/Pandas-Handle-missing-data/"/>
    <id>http://yoursite.com/2018/09/23/Pandas-Handle-missing-data/</id>
    <published>2018-09-23T19:40:37.000Z</published>
    <updated>2020-08-23T20:26:50.597Z</updated>
    
    <content type="html"><![CDATA[<p>Missing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it.</p><p>###Produce some data with missing values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>, <span class="number">3</span>), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;h&#x27;</span>],</span><br><span class="line">                 columns=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>])</span><br><span class="line">df[<span class="string">&#x27;four&#x27;</span>]=<span class="string">&#x27;bar&#x27;</span></span><br><span class="line">df[<span class="string">&#x27;five&#x27;</span>]= df[<span class="string">&#x27;one&#x27;</span>] &gt; <span class="number">0</span></span><br><span class="line">df[<span class="string">&#x27;date&#x27;</span>]= pd.Timestamp(<span class="string">&#x27;2018-01-01&#x27;</span>)</span><br><span class="line">df2 = df.reindex([<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>,<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>])</span><br><span class="line">df2</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>one</th><th>two</th><th>three</th><th>four</th><th>five</th><th>date</th></tr></thead><tbody><tr><td>a</td><td>-0.614301</td><td>0.628725</td><td>-0.903163</td><td>bar</td><td>False</td><td>2018-01-01</td></tr><tr><td>b</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaT</td></tr><tr><td>c</td><td>-0.297019</td><td>1.357266</td><td>-0.665749</td><td>bar</td><td>False</td><td>2018-01-01</td></tr><tr><td>d</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaT</td></tr><tr><td>e</td><td>0.311524</td><td>-0.328388</td><td>0.777467</td><td>bar</td><td>True</td><td>2018-01-01</td></tr><tr><td>f</td><td>0.572373</td><td>-0.309563</td><td>-0.296276</td><td>bar</td><td>True</td><td>2018-01-01</td></tr><tr><td>g</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaT</td></tr><tr><td>h</td><td>-1.303842</td><td>1.239911</td><td>-0.909255</td><td>bar</td><td>False</td><td>2018-01-01</td></tr></tbody></table><p>Notice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime</p><h3 id="Detect-missing-value"><a href="#Detect-missing-value" class="headerlink" title="Detect missing value"></a>Detect missing value</h3><p>pandas provides two methods, isna() and notna()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.isna()</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>one</th><th>two</th><th>three</th><th>four</th><th>five</th><th>date</th></tr></thead><tbody><tr><td>a</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>b</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>c</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>d</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>e</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>f</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>g</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>h</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.notna()</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>one</th><th>two</th><th>three</th><th>four</th><th>five</th><th>date</th></tr></thead><tbody><tr><td>a</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>b</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>c</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>d</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>e</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>f</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>g</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><td>h</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr></tbody></table><h3 id="Cleaning"><a href="#Cleaning" class="headerlink" title="Cleaning"></a>Cleaning</h3><p>drop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.dropna()</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>one</th><th>two</th><th>three</th><th>four</th><th>five</th><th>date</th></tr></thead><tbody><tr><td>a</td><td>-0.614301</td><td>0.628725</td><td>-0.903163</td><td>bar</td><td>False</td><td>2018-01-01</td></tr><tr><td>c</td><td>-0.297019</td><td>1.357266</td><td>-0.665749</td><td>bar</td><td>False</td><td>2018-01-01</td></tr><tr><td>e</td><td>0.311524</td><td>-0.328388</td><td>0.777467</td><td>bar</td><td>True</td><td>2018-01-01</td></tr><tr><td>f</td><td>0.572373</td><td>-0.309563</td><td>-0.296276</td><td>bar</td><td>True</td><td>2018-01-01</td></tr><tr><td>h</td><td>-1.303842</td><td>1.239911</td><td>-0.909255</td><td>bar</td><td>False</td><td>2018-01-01</td></tr></tbody></table><p>Drop columns with missing data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.dropna(axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th></th></tr></thead><tbody><tr><td>a</td></tr><tr><td>b</td></tr><tr><td>c</td></tr><tr><td>d</td></tr><tr><td>e</td></tr><tr><td>f</td></tr><tr><td>g</td></tr><tr><td>h</td></tr></tbody></table><p>All columns are removed..</p><h3 id="Fill-na"><a href="#Fill-na" class="headerlink" title="Fill na"></a>Fill na</h3><p>Na value can be easily filled by fillna()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>one</th><th>two</th><th>three</th><th>four</th><th>five</th><th>date</th></tr></thead><tbody><tr><td>a</td><td>-0.614301</td><td>0.628725</td><td>-0.903163</td><td>bar</td><td>False</td><td>2018-01-01 00:00:00</td></tr><tr><td>b</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0</td><td>0</td><td>0</td></tr><tr><td>c</td><td>-0.297019</td><td>1.357266</td><td>-0.665749</td><td>bar</td><td>False</td><td>2018-01-01 00:00:00</td></tr><tr><td>d</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0</td><td>0</td><td>0</td></tr><tr><td>e</td><td>0.311524</td><td>-0.328388</td><td>0.777467</td><td>bar</td><td>True</td><td>2018-01-01 00:00:00</td></tr><tr><td>f</td><td>0.572373</td><td>-0.309563</td><td>-0.296276</td><td>bar</td><td>True</td><td>2018-01-01 00:00:00</td></tr><tr><td>g</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0</td><td>0</td><td>0</td></tr><tr><td>h</td><td>-1.303842</td><td>1.239911</td><td>-0.909255</td><td>bar</td><td>False</td><td>2018-01-01 00:00:00</td></tr></tbody></table><p>We can also fill data with respect to their column property</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.fillna(value = &#123;<span class="string">&#x27;one&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;four&#x27;</span>: <span class="string">&#x27;missing&#x27;</span>, <span class="string">&#x27;five&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;date&#x27;</span>: pd.Timestamp(<span class="string">&#x27;2000-01-01&#x27;</span>)&#125;)</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>one</th><th>two</th><th>three</th><th>four</th><th>five</th><th>date</th></tr></thead><tbody><tr><td>a</td><td>-0.614301</td><td>0.628725</td><td>-0.903163</td><td>bar</td><td>False</td><td>2018-01-01</td></tr><tr><td>b</td><td>0.000000</td><td>NaN</td><td>NaN</td><td>missing</td><td>False</td><td>2000-01-01</td></tr><tr><td>c</td><td>-0.297019</td><td>1.357266</td><td>-0.665749</td><td>bar</td><td>False</td><td>2018-01-01</td></tr><tr><td>d</td><td>0.000000</td><td>NaN</td><td>NaN</td><td>missing</td><td>False</td><td>2000-01-01</td></tr><tr><td>e</td><td>0.311524</td><td>-0.328388</td><td>0.777467</td><td>bar</td><td>True</td><td>2018-01-01</td></tr><tr><td>f</td><td>0.572373</td><td>-0.309563</td><td>-0.296276</td><td>bar</td><td>True</td><td>2018-01-01</td></tr><tr><td>g</td><td>0.000000</td><td>NaN</td><td>NaN</td><td>missing</td><td>False</td><td>2000-01-01</td></tr><tr><td>h</td><td>-1.303842</td><td>1.239911</td><td>-0.909255</td><td>bar</td><td>False</td><td>2018-01-01</td></tr></tbody></table><p>Remind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2.fillna(value = <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Missing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show t</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to select data</title>
    <link href="http://yoursite.com/2018/09/20/Pandas-How-to-select-data/"/>
    <id>http://yoursite.com/2018/09/20/Pandas-How-to-select-data/</id>
    <published>2018-09-20T20:49:51.000Z</published>
    <updated>2020-08-23T20:28:43.534Z</updated>
    
    <content type="html"><![CDATA[<p>This article shows the most common methods regarding data selection</p><p>First, let’s create a dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(np.random.rand(<span class="number">5</span>,<span class="number">4</span>), index = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>], columns= [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>a</td><td>0.122248</td><td>0.581777</td><td>0.972888</td><td>0.869366</td></tr><tr><td>b</td><td>0.476451</td><td>0.453979</td><td>0.004705</td><td>0.644530</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td><td>0.652936</td><td>0.758767</td></tr><tr><td>d</td><td>0.077122</td><td>0.407514</td><td>0.019102</td><td>0.553546</td></tr><tr><td>e</td><td>0.921520</td><td>0.157199</td><td>0.371028</td><td>0.825792</td></tr></tbody></table><h3 id="Use-for-columns-selection"><a href="#Use-for-columns-selection" class="headerlink" title="Use [] for columns selection"></a>Use [] for columns selection</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]]</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th></tr></thead><tbody><tr><td>a</td><td>0.122248</td><td>0.581777</td></tr><tr><td>b</td><td>0.476451</td><td>0.453979</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td></tr><tr><td>d</td><td>0.077122</td><td>0.407514</td></tr><tr><td>e</td><td>0.921520</td><td>0.157199</td></tr></tbody></table><h3 id="Select-a-range"><a href="#Select-a-range" class="headerlink" title="Select a range"></a>Select a range</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># or df[&#x27;b&#x27;:&#x27;c&#x27;]</span></span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>b</td><td>0.476451</td><td>0.453979</td><td>0.004705</td><td>0.644530</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td><td>0.652936</td><td>0.758767</td></tr></tbody></table><h3 id="Use-loc"><a href="#Use-loc" class="headerlink" title="Use .loc"></a>Use .loc</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[<span class="string">&#x27;a&#x27;</span>:<span class="string">&#x27;c&#x27;</span>, [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>]]</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th></tr></thead><tbody><tr><td>a</td><td>0.122248</td><td>0.581777</td></tr><tr><td>b</td><td>0.476451</td><td>0.453979</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td></tr></tbody></table><h3 id="Select-all-columns"><a href="#Select-all-columns" class="headerlink" title="Select all columns"></a>Select all columns</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[<span class="string">&#x27;a&#x27;</span>:<span class="string">&#x27;c&#x27;</span>, :]</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>a</td><td>0.122248</td><td>0.581777</td><td>0.972888</td><td>0.869366</td></tr><tr><td>b</td><td>0.476451</td><td>0.453979</td><td>0.004705</td><td>0.644530</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td><td>0.652936</td><td>0.758767</td></tr></tbody></table><h3 id="Select-with-boolean"><a href="#Select-with-boolean" class="headerlink" title="Select with boolean"></a>Select with boolean</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;A&#x27;</span>]&gt;<span class="number">0.2</span>]</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>b</td><td>0.476451</td><td>0.453979</td><td>0.004705</td><td>0.644530</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td><td>0.652936</td><td>0.758767</td></tr><tr><td>e</td><td>0.921520</td><td>0.157199</td><td>0.371028</td><td>0.825792</td></tr></tbody></table><h3 id="Select-with-callable-lambda"><a href="#Select-with-callable-lambda" class="headerlink" title="Select with callable(lambda)"></a>Select with callable(lambda)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="keyword">lambda</span> df: df[<span class="string">&#x27;A&#x27;</span>]&gt;<span class="number">0.2</span>]</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>b</td><td>0.476451</td><td>0.453979</td><td>0.004705</td><td>0.644530</td></tr><tr><td>c</td><td>0.954790</td><td>0.747131</td><td>0.652936</td><td>0.758767</td></tr><tr><td>e</td><td>0.921520</td><td>0.157199</td><td>0.371028</td><td>0.825792</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This article shows the most common methods regarding data selection&lt;/p&gt;
&lt;p&gt;First, let’s create a dataframe.&lt;/p&gt;
&lt;figure class=&quot;highlight </summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>What is Attention and how to use</title>
    <link href="http://yoursite.com/2018/09/20/What-is-Attention-and-how-to-use/"/>
    <id>http://yoursite.com/2018/09/20/What-is-Attention-and-how-to-use/</id>
    <published>2018-09-20T09:06:45.000Z</published>
    <updated>2020-08-23T20:28:10.855Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Attention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT)  and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow)</p><h3 id="Why-Attention"><a href="#Why-Attention" class="headerlink" title="Why Attention"></a>Why Attention</h3><p>Attention is a mechanism derived from the seq-seq model which started the era of NMT,  in this <a href="https://arxiv.org/pdf/1409.3215.pdf">paper</a> Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation.</p><p><img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Example-of-an-Encoder-Decode-Network.png" alt="Example of an Encoder-Decode Network, from &quot;Sequence to Sequence Learning with Neural Network&quot; 2014"></p><p>The model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus.</p><p>Attention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder.</p><p>Maybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector.</p><h3 id="What-is-Attention"><a href="#What-is-Attention" class="headerlink" title="What is Attention"></a>What is Attention</h3><p>Attention is merely a context vector that provides a richer encoding of the source sequence. <strong>The vector is computed at every decoder time step.</strong></p><p><img src="https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg" alt="img"></p><p>As illustrated in the figure above, the attention computation can be summarized into the following three steps:</p><ol><li><p>Compute attention weights based on the current target hidden state and all source state(Figure 1)</p></li><li><p>The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2)</p></li><li><p>Context vector combined with the current target hidden state yields the attention vector(Figure 3)</p><p>The attention vector is then fed to the next decoding step. </p><p><img src="https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg" alt="img"></p><p>the score in Figure 1 is computed as follows:</p><p><img src="https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg" alt="img"></p></li></ol><p>Regarding the score,  the methods by which it is calculated lead to different performance. </p><h3 id="Coding-Attention-with-Tensorflow"><a href="#Coding-Attention-with-Tensorflow" class="headerlink" title="Coding Attention with Tensorflow"></a>Coding Attention with Tensorflow</h3><p>Suppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transfer encoder_outputs to attention_states </span></span><br><span class="line">attention_states = tf.transpose(encoder_outputs, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply existing attention mechanism </span></span><br><span class="line">attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(</span><br><span class="line">    num_units, attention_states,</span><br><span class="line">    memory_sequence_length=source_sequence_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feed to the decoder</span></span><br><span class="line">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(</span><br><span class="line">    decoder_cell, attention_mechanism,</span><br><span class="line">    attention_layer_size=num_units)</span><br></pre></td></tr></table></figure><p>The rest codes are mostly the same as standard encoder-decoder.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;Attention or Bahdanau Attentio</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="AI算法" scheme="http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to drop columns/rows</title>
    <link href="http://yoursite.com/2018/09/18/Pandas-How-to-drop-columns-rows/"/>
    <id>http://yoursite.com/2018/09/18/Pandas-How-to-drop-columns-rows/</id>
    <published>2018-09-18T20:39:25.000Z</published>
    <updated>2020-08-23T20:26:40.596Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;A#&#x27;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], <span class="string">&#x27;B#&#x27;</span>:[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], <span class="string">&#x27;C#&#x27;</span>:[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>A#</th><th>B#</th><th>C#</th></tr></thead><tbody><tr><td>1</td><td>4</td><td>7</td></tr><tr><td>2</td><td>5</td><td>8</td></tr><tr><td>3</td><td>6</td><td>9</td></tr></tbody></table><h3 id="Drop-columns"><a href="#Drop-columns" class="headerlink" title="Drop columns"></a>Drop columns</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.drop([<span class="string">&#x27;B#&#x27;</span>, <span class="string">&#x27;C#&#x27;</span>], axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment">#or</span></span><br><span class="line">df.drop(columns=[<span class="string">&#x27;B#&#x27;</span>, <span class="string">&#x27;C#&#x27;</span>])</span><br></pre></td></tr></table></figure><table><thead><tr><th>A#</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr></tbody></table><h3 id="Drop-rows"><a href="#Drop-rows" class="headerlink" title="Drop rows"></a>Drop rows</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop([<span class="number">0</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><table><thead><tr><th>A#</th><th>B#</th><th>C#</th></tr></thead><tbody><tr><td>3</td><td>6</td><td>9</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to rename columns</title>
    <link href="http://yoursite.com/2018/09/17/Pandas-How-to-rename-columns/"/>
    <id>http://yoursite.com/2018/09/17/Pandas-How-to-rename-columns/</id>
    <published>2018-09-17T21:11:55.000Z</published>
    <updated>2020-08-23T20:28:57.830Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;A#&#x27;</span>:[<span class="number">1</span>,<span class="number">2</span>], <span class="string">&#x27;B#&#x27;</span>:[<span class="number">3</span>,<span class="number">4</span>], <span class="string">&#x27;C#&#x27;</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>A#</th><th>B#</th><th>C#</th></tr></thead><tbody><tr><td>1</td><td>3</td><td>5</td></tr><tr><td>2</td><td>4</td><td>6</td></tr></tbody></table><p><strong>Rename with DataFrame.rename()</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.rename(index= str, columns=&#123;<span class="string">&quot;A#&quot;</span>: <span class="string">&quot;a&quot;</span>, <span class="string">&quot;B#&quot;</span>: <span class="string">&quot;b&quot;</span>&#125;)</span><br></pre></td></tr></table></figure><table><thead><tr><th>a</th><th>b</th><th>C#</th></tr></thead><tbody><tr><td>1</td><td>3</td><td>5</td></tr><tr><td>2</td><td>4</td><td>6</td></tr></tbody></table><p>Print df again</p><table><thead><tr><th>A#</th><th>B#</th><th>C#</th></tr></thead><tbody><tr><td>1</td><td>3</td><td>5</td></tr><tr><td>2</td><td>4</td><td>6</td></tr></tbody></table><p>Column names did’nt change. </p><p><strong>To change names permanently, use  “inplace=True”</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.rename(index= str, columns=&#123;<span class="string">&quot;A#&quot;</span>: <span class="string">&quot;a&quot;</span>, <span class="string">&quot;B#&quot;</span>: <span class="string">&quot;b&quot;</span>&#125;, inplace = <span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>a</th><th>b</th><th>C#</th></tr></thead><tbody><tr><td>1</td><td>3</td><td>5</td></tr><tr><td>2</td><td>4</td><td>6</td></tr></tbody></table><p><strong>Rename in batch</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.rename(index= str, columns=<span class="keyword">lambda</span> x:x.replace(<span class="string">&#x27;#&#x27;</span>,<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure><table><thead><tr><th>A</th><th>B</th><th>C</th></tr></thead><tbody><tr><td>1</td><td>3</td><td>5</td></tr><tr><td>2</td><td>4</td><td>6</td></tr></tbody></table><p>Lambda is amazing</p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas]How to import CSV</title>
    <link href="http://yoursite.com/2018/09/16/Pandas-How-to-import-csv/"/>
    <id>http://yoursite.com/2018/09/16/Pandas-How-to-import-csv/</id>
    <published>2018-09-16T15:30:37.000Z</published>
    <updated>2020-08-23T20:26:35.426Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Locate-the-file"><a href="#Locate-the-file" class="headerlink" title="Locate the file"></a>Locate the file</h3><p>There are two ways to locate the csv file, “absolute path” or “relative path”<br>e.g.<br>Absolute path:<br>“<code>/work/project/data/great.csv</code>“(Mac) or “<code>C:\work\project\data\great.csv</code>“(Windows)<br>Relative path:<br>“<code>data/greate.csv</code>“(Mac) or “<code>data\great.csv</code>“(Windows)</p><h3 id="Read-with-pandas-read-csv"><a href="#Read-with-pandas-read-csv" class="headerlink" title="Read with pandas.read_csv()"></a>Read with pandas.read_csv()</h3><p>The result is a dataframe Object</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;data/great.csv&#x27;</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>company</th><th>locate</th><th>employees</th><th>avenue</th></tr></thead><tbody><tr><td>orange</td><td>New York</td><td>10000</td><td>4000.0</td></tr><tr><td>banana</td><td>London</td><td>2000</td><td>1000.0</td></tr><tr><td>pinch</td><td>Paris</td><td>4000</td><td>5000.0</td></tr><tr><td>pear</td><td>Berlin</td><td>3000</td><td>NaN</td></tr></tbody></table><h3 id="No-infered-header"><a href="#No-infered-header" class="headerlink" title="No infered header"></a>No infered header</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;data/great.csv&#x27;</span>, header= <span class="literal">None</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>0</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>company</td><td>locate</td><td>employees</td><td>avenue</td></tr><tr><td>orange</td><td>New York</td><td>10000</td><td>4000</td></tr><tr><td>banana</td><td>London</td><td>2000</td><td>1000</td></tr><tr><td>pinch</td><td>Paris</td><td>4000</td><td>5000</td></tr><tr><td>pear</td><td>Berlin</td><td>3000</td><td>NaN</td></tr></tbody></table><h3 id="Filter-rows"><a href="#Filter-rows" class="headerlink" title="Filter rows"></a>Filter rows</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;data/great.csv&#x27;</span>, skiprows=[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>company</th><th>locate</th><th>employees</th><th>avenue</th></tr></thead><tbody><tr><td>orange</td><td>New York</td><td>10000</td><td>4000</td></tr><tr><td>banana</td><td>London</td><td>2000</td><td>1000</td></tr></tbody></table><p>lambda is also welcomed</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;data/great.csv&#x27;</span>, skiprows=<span class="keyword">lambda</span> x:x/<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>company</th><th>locate</th><th>employees</th><th>avenue</th></tr></thead><tbody><tr><td>orange</td><td>New York</td><td>10000</td><td>4000.0</td></tr><tr><td>pear</td><td>Berlin</td><td>3000</td><td>NaN</td></tr></tbody></table><h3 id="Filter-columns-or-keep-wanted"><a href="#Filter-columns-or-keep-wanted" class="headerlink" title="Filter columns(or keep wanted)"></a>Filter columns(or keep wanted)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;data/great.csv&#x27;</span>, usecols=[<span class="string">&#x27;company&#x27;</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure><table><thead><tr><th>company</th></tr></thead><tbody><tr><td>orange</td></tr><tr><td>banana</td></tr><tr><td>pinch</td></tr><tr><td>pear</td></tr></tbody></table><p>Other options refer <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Locate-the-file&quot;&gt;&lt;a href=&quot;#Locate-the-file&quot; class=&quot;headerlink&quot; title=&quot;Locate the file&quot;&gt;&lt;/a&gt;Locate the file&lt;/h3&gt;&lt;p&gt;There are two ways</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow101</title>
    <link href="http://yoursite.com/2018/07/10/Tensorflow101/"/>
    <id>http://yoursite.com/2018/07/10/Tensorflow101/</id>
    <published>2018-07-10T21:25:42.000Z</published>
    <updated>2020-08-23T20:28:03.972Z</updated>
    
    <content type="html"><![CDATA[<p>Tensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning.</p><h3 id="How-to-Install"><a href="#How-to-Install" class="headerlink" title="How to Install"></a>How to Install</h3><p>If you have ‘pip’, everything is simple<br><code>pip install tensorflow</code></p><h3 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h3><blockquote><p>Graph  </p></blockquote><p>Graph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is </p><p><em>h=ReLU(Wx+b)</em></p><p>In the view of Tensorflow, the function looks like this</p><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_01.png" alt="relu_graph"></p><blockquote><p>Nodes  </p></blockquote><p>Variables such as <em>W</em> and <em>b</em> , placeholders such as x, are operations such as <em>MatMul</em>, <em>Add</em> are all nodes in the graph.</p><blockquote><p>Edges  </p><p>The edges between nodes indicate the data which flow between nodes,  in tensorflow data is represented as “tensor” .  </p></blockquote><p>“tensor” + “flow”  = “tensorflow”</p><h3 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line"></span><br><span class="line">b &#x3D; tf.Variable(tf.zeros((10,)))</span><br><span class="line">W &#x3D; tf.Variable(tf.random_uniform((20, 10), -1, 1))</span><br><span class="line"></span><br><span class="line">x &#x3D; tf.placeholder(tf.float32, (10, 20))</span><br><span class="line"></span><br><span class="line">h &#x3D; tf.nn.relu(tf.matmul(x, W) + b)</span><br><span class="line"></span><br><span class="line">writer &#x3D; tf.summary.FileWriter(&#39;.&#x2F;graphs&#39;, tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(h, &#123;x: np.random.random((10, 20))&#125;)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Here are the steps described in the above codes.</p><ol><li>Create a graph using Variables and placeholders.</li><li>Start a tensorflow session and deploy the graph into the session.</li><li>Run the session, let the tensors flow.</li><li>Write processing logs using tools such as tf.summary.</li></ol><p>Session is the so called execution environment. It needs two parameters which are “Fetches” and “Feeds”.</p><p><code>sess.run(fetches, feeds)</code></p><p>Fetches: List of graph nodes</p><p>Feeds: Dictionary mapping from graph nodes to concrete values.</p><p>In the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))}</p><p>It would be interesting to see what happened during the running time. We can try TensorBoard.</p><h2 id="TensorBoard-Result"><a href="#TensorBoard-Result" class="headerlink" title="TensorBoard Result"></a>TensorBoard Result</h2><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_02.png" alt="tensorboard"></p><p><img src="https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_03.png" alt="tensorboard_relu"></p><p>The Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Tensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning a</summary>
      
    
    
    
    <category term="数据&amp;AI" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"/>
    
    
    <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>我所理解的分布式调度</title>
    <link href="http://yoursite.com/2017/03/25/%E6%88%91%E6%89%80%E7%90%86%E8%A7%A3%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6/"/>
    <id>http://yoursite.com/2017/03/25/%E6%88%91%E6%89%80%E7%90%86%E8%A7%A3%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6/</id>
    <published>2017-03-25T19:37:04.000Z</published>
    <updated>2020-08-23T20:28:26.759Z</updated>
    
    <content type="html"><![CDATA[<p>对于规模以上的应用来说，调度系统已经是必不可少的组成部分，尤其在基于数据分析的后台应用大量增长的今天，健壮的调度任务管理已经是非常重要的一环，因此多花些时间来分析研究调度系统的设计对于日常的开发与运维具有比较重要的意义。</p><h2 id="调度问题是怎么来的"><a href="#调度问题是怎么来的" class="headerlink" title="调度问题是怎么来的"></a>调度问题是怎么来的</h2><p>当你的网站是个简单的blog，而且并不需要跟外部交互的时候，你大概不需要调度任务，因为此时网站需要处理的任务仅限于 即时交互 ， 即用户想使用一个功能，你就立即给他就是了，如同你在简书上写一篇文章，一点保存，这篇文章立即就保存到网站的后台服务器中去了，这也是互联网刚出现时候的最早的应用模式。</p><p>之后因为网站发展的不错，用户多了起来，就发现需要大量处理一些非即时的任务，比如要定时将用户访问日志归档清理，这个时候一般情况下会在服务器启动一个定时任务，在每天固定时间发起清理归档，又如你想显示一下网站的访客流量、发表文章数、评论统计，由于并非每次用户或者后台管理员每次需要看的时候都去计算一遍，所以可能又需要启动另一个任务来去处理这些数据，这样的任务多了，就需要思考一个问题，哪些任务要先处理，哪些任务要后处理，每个任务要占用多少资源，从而任务调度问题开始出现。</p><h2 id="调度什么时候变得复杂"><a href="#调度什么时候变得复杂" class="headerlink" title="调度什么时候变得复杂"></a>调度什么时候变得复杂</h2><p>在一个单机的系统，任务并不多的情况下，生活还依然是美好的，利用Linux自带的定时器或者系统框架提供的定时任务，实现好任务执行器，配置好任务触发的时间和频率，然后每天等待它自动触发，数据生成，任务搞定，似乎调度也没那么困难。</p><p>然而好景不长，伴随着网站的发展，你发现任务处理的越来越慢，甚至偶尔会有任务超时的情况，原因是每天用户产生的数据量越来越大，每次任务捞起的数量已经超载，依次执行完每个任务，可能已是最初执行时间的几倍。</p><p>这时稍有点经验你便会想到，任务没必要顺序执行啊，所以弄个线程池，起多个线程同时来捞数据然后执行，同时配置动态调整每次数据捞取的数量，增大执行频率，一系列组合拳打出去之后，调度任务又恢复正常，由于增加了并发数，甚至执行的比开始还更快了，就这样业务高峰便又顺利度过了。</p><h2 id="调度什么时候变得更加复杂"><a href="#调度什么时候变得更加复杂" class="headerlink" title="调度什么时候变得更加复杂"></a>调度什么时候变得更加复杂</h2><p>之前所描述的情况基本上都在单机的情况，网站的QPS（每秒处理的任务数量）基本在500以下，通过一系列的参数调优，串行变并行，任务运行的很平稳。然而当任务变的规模更大，比如十倍于当前，一台机器已经不能处理所有的任务，这时候需要增加更多的机器，将任务分配到不同的机器上，于是便有了所谓的分布式调度的问题。</p><p>分布式是目前稍大型的网站不可避免的问题，这种处理方案有很多好处，比如可以利用廉价的机器，可以（理论上）无限水平拓展，同时也带来了一系列棘手的问题，机器之间的网络通信，如何把流量均匀的分布于不同流量，如果有机器宕机了如何处理，每个问题都已经是一个庞大的技术领域。</p><p>对于调度的分布式问题，首先要解决的便是如何把任务分发到不同的机器上，这要求调度系统通常要至少分为两层，第一层决定一共要处理哪些任务并把任务分发到哪些机器上处理，第二层接到任务后具体执行。虽然描述起来很简单，但是这个处理过程实际上需要大量支撑系统的支持，比如在任务分发时如何判断哪些机器还活着可以处理任务，这可能需要一个可以感知整个集群运行状态的配置中心，又比如任务如何分发，是采用消息还是实时服务接口，如果是用消息派发则需要消息系统，如果是实时服务，又需要类似dubbo这样的分布式服务框架。当系统达到这个复杂度，已经不是将任务捞起并处理这么单纯，而是多个关联系统复杂性的叠加。</p><h2 id="分布式调度的难题"><a href="#分布式调度的难题" class="headerlink" title="分布式调度的难题"></a>分布式调度的难题</h2><p>虽然分布式调度带来了复杂度的上升，但是它的水平拓展能力完美的适配了网站的规模发展，直到有一天你看到了类似这个错误：</p><blockquote><p>org.springframework.transaction.CannotCreateTransactionException</p></blockquote><p>数据库连接池已经打满，由于单个数据库的连接数是一定的，这意味着数据库变成了资源瓶颈，这时候给任务处理happy的加机器已经不能提高系统的整体处理能力，这就如同你要运走一群人，给你提供的车辆是无限的，但是汽油确是一定的。当然数据库也是可以拓展的，但是考虑到数据迁移的复杂性，这几乎将问题的复杂度提高了一个等级。因此，一个分布式系统的吞吐量，在很大程度上是与数据库处理能力做权衡的结果。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>分布式调度是一个巨大的话题，并且还在随着实际任务复杂度的提高而快速的更新，上面这些思考与总结也只是浮光掠影，还需要在实际工作中再深入体会与仔细研究。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;对于规模以上的应用来说，调度系统已经是必不可少的组成部分，尤其在基于数据分析的后台应用大量增长的今天，健壮的调度任务管理已经是非常重要的一环，因此多花些时间来分析研究调度系统的设计对于日常的开发与运维具有比较重要的意义。&lt;/p&gt;
&lt;h2 id=&quot;调度问题是怎么来的&quot;&gt;&lt;a </summary>
      
    
    
    
    <category term="后端" scheme="http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Distributed System" scheme="http://yoursite.com/tags/Distributed-System/"/>
    
  </entry>
  
</feed>
