<!DOCTYPE html>
<html lang=en>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Basics of words embedding | Feng&#39;s note</title>
  <meta name="description" content="Why embeddingNatural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:  These encodings provide no useful inf">
<meta property="og:type" content="article">
<meta property="og:title" content="Basics of words embedding">
<meta property="og:url" content="http://yoursite.com/2018/10/01/Basics-of-words-embedding/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Why embeddingNatural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:  These encodings provide no useful inf">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png">
<meta property="article:published_time" content="2018-10-01T10:14:28.000Z">
<meta property="article:modified_time" content="2020-08-23T20:27:33.747Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="AI算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://yoursite.com/2018/10/01/Basics-of-words-embedding/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.1.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/niuguy" target="_blank">
          <img class="img-circle img-rotate" src="/images/march.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Feng</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Architect &amp; AI</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> London, UK</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/niuguy" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://twitter.com/home" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">Board</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p></p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%90%8E%E7%AB%AF/">后端</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a><span class="category-list-count">15</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI%E7%AE%97%E6%B3%95/" rel="tag">AI算法</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distributed-System/" rel="tag">Distributed System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/" rel="tag">pandas</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/AI%E7%AE%97%E6%B3%95/" style="font-size: 13.67px;">AI算法</a> <a href="/tags/Distributed-System/" style="font-size: 13px;">Distributed System</a> <a href="/tags/pandas/" style="font-size: 14px;">pandas</a> <a href="/tags/tensorflow/" style="font-size: 13.33px;">tensorflow</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a>
              </p>
              <p class="item-title">
                <a href="/2018/11/08/Stochastic-gradient-descent/" class="title">Stochastic gradient descent</a>
              </p>
              <p class="item-date">
                <time datetime="2018-11-08T11:14:28.000Z" itemprop="datePublished">2018-11-08</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a>
              </p>
              <p class="item-title">
                <a href="/2018/11/08/Backward-propagation-of-Neural-Network-explained/" class="title">Backward propagation of Neural Network explained</a>
              </p>
              <p class="item-date">
                <time datetime="2018-11-08T11:14:28.000Z" itemprop="datePublished">2018-11-08</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a>
              </p>
              <p class="item-title">
                <a href="/2018/10/22/Pandas-How-to-plot-counts-of-each-value/" class="title">[Pandas]How to plot counts of each value</a>
              </p>
              <p class="item-date">
                <time datetime="2018-10-22T19:40:47.000Z" itemprop="datePublished">2018-10-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a>
              </p>
              <p class="item-title">
                <a href="/2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/" class="title">[Pandas]How to calculate datetime difference in years</a>
              </p>
              <p class="item-date">
                <time datetime="2018-10-22T15:43:34.000Z" itemprop="datePublished">2018-10-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a>
              </p>
              <p class="item-title">
                <a href="/2018/10/19/Pandas-How-to-list-all-columns/" class="title">[Pandas]How to list all columns</a>
              </p>
              <p class="item-date">
                <time datetime="2018-10-19T21:31:23.000Z" itemprop="datePublished">2018-10-19</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-Basics-of-words-embedding" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Basics of words embedding
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2018/10/01/Basics-of-words-embedding/" class="article-date">
	  <time datetime="2018-10-01T10:14:28.000Z" itemprop="datePublished">2018-10-01</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E6%95%B0%E6%8D%AE-AI/">数据&AI</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/AI%E7%AE%97%E6%B3%95/" rel="tag">AI算法</a>
  </span>


        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2018/10/01/Basics-of-words-embedding/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h3 id="Why-embedding"><a href="#Why-embedding" class="headerlink" title="Why embedding"></a>Why embedding</h3><p>Natural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:</p>
<ol>
<li>These encodings provide no useful information regarding the <strong>relationships</strong> that may exist between the individual symbols.</li>
<li>Discrete ids furthermore lead to <strong>data sparsity</strong>. We may need more data to train statistical models successfully.</li>
</ol>
<p>To address these two problems, word embeddings provide a solution to  represent words and their relative meanings densely.</p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>Embedding derives from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Vector_space_model">Vector Space Models(VSMs)</a>,  one of its well-known schemes is <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Tf-Idf</a> weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that <strong>words that appear in the same contexts share semantic meaning</strong>. </p>
<p>VSMs have two main approaches: Count-based methods and predictive methods.  Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense <em>embedding vectors</em> (considered parameters of the model).</p>
<p>Among all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular.</p>
<h3 id="Count-based-Embedding"><a href="#Count-based-Embedding" class="headerlink" title="Count-based Embedding"></a>Count-based Embedding</h3><p>GloVe is an <strong>unsupervised learning algorithm</strong> for obtaining vector representations for words.  The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words  to capture as much as possible the meaning specified by the juxtaposition of two words.</p>
<p>The <a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">project page of glove</a>  gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration:</p>
<blockquote>
<p>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.</p>
<p>Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. </p>
<p>It is necessary for a model to associate more than a single number to the word pair.</p>
</blockquote>
<p>Training GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost.  <a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">project page of glove</a> also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>‘’the’’ is followed by 100 floats which are the vector values of this word</p>
<p>We can build a dict whose key is words and value is their glove vector</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">embeddings_index = dict()</span><br><span class="line">f = open(<span class="string">&#x27;./glove.6B.100d.txt&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = asarray(values[<span class="number">1</span>:], dtype = <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>

<p>When we need to build a embedding layer , we just look up the vectors  for input words in the dict.</p>
<h3 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h3><p>Word2vec, as illustrated in the first part,  is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. ‘mat’) from source context words (‘the cat sits on the’), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore,  CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. </p>
<p>We will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model.</p>
<p><img src="https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png" alt="Skip-gram Neural Network Architecture"></p>
<p>The Skip-gram model is trained like this,  Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. </p>
<p>The coding example of how to build and train the Skip-gram model can be found <a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">here</a></p>
<p>Next post I will try to use embedding to solve an interesting real world problem.</p>

      
    </div>
    <div class="article-footer">
    </div>
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2018/10/18/Pandas-How-to-import-from-Sql-Server/" title="[Pandas] How to import from Sql Server"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2018/09/26/What-is-tf-data-and-how-to-use/" title="What is tf.data and how to use"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/niuguy" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://twitter.com/home" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   






</body>
</html>