{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2018-09-21T13:50:40.000Z","updated":"2020-08-21T21:56:24.929Z","comments":true,"path":"index.html","permalink":"http://yoursite.com/index.html","excerpt":"","text":"Machine Learning Stochastic Gradient Descent Backward propagation of Neural Network explained What is Attention and how to use Basics of words embedding Pandas How-to [Pandas]How to select data [Pandas]How to import CSV [Pandas]How to rename columns [Pandas]How to drop columns rows [Pandas]Handle missing data [Pandas]How to import from Sql Server [Pandas]How to list all columns [Pandas]How to calculate datetime difference in years [Pandas]How to plot counts of each value Tensorflow How-to Tensorflow101 What is tf.data and how to use"},{"title":"tags","date":"2020-08-24T20:52:19.000Z","updated":"2020-08-24T20:54:35.209Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-08-24T20:52:11.000Z","updated":"2020-08-24T20:54:02.441Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-08-24T20:55:34.000Z","updated":"2020-08-24T21:05:01.858Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"空山新雨后，天气晚来秋，明月松间照，清泉石上流。"}],"posts":[{"title":"Stochastic gradient descent","slug":"Stochastic-gradient-descent","date":"2018-11-08T11:14:28.000Z","updated":"2020-08-24T21:09:25.230Z","comments":true,"path":"2018/11/08/Stochastic-gradient-descent/","link":"","permalink":"http://yoursite.com/2018/11/08/Stochastic-gradient-descent/","excerpt":"","text":"Stochastic Gradient decent is one of the fundamental algorithm in deep learning. It is used when we perform optimization of the cost function.Suppose the function is $ f(x) $ As what illustrated above, we want to approach the minimum value of f(x) which is the point C. If we are now at point A, the derivation of A is f’(x)&gt;0, so we need to go right down which is the opposite direction of f’(x). If we stand at B,the derivation of A is f’(x)&gt;0, we should go left down which is also the opposite direction of f’(x). According to the optimization strategy , we should update the parameter like this: x=x−ηf′(x)x = x - \\eta f&#x27;(x) x=x−ηf′(x) η\\etaη is so-called the learning rate, it determines the size of steps we take to reach a minimum value of f(x)f(x)f(x) Batch Gradient Follow the rules of gradient descent, when we perform one update of parameters the intuitive strategy is to calculate the gradients for all the examples in the dataset and sum them up, this computation process is also called batch gradient. The pseudocode described above looks like this. 123for i in range(epoches): param_grad = calculate_gradient(loss_function, [examples]) param = param - learning_rate * params_grad For each update we need to walk through all these examples, if there are millions of them, the computation cost would be a problem, let alone there usually are several epochs of updates. Stochastic Gradient The inefficiency of the batch gradient leads to another strategy that is widely used in deep learning, which is stochastic gradient descent(SGD). Instead of updating the params based on the gradients of all examples, SGD updates the param after computing the gradient of one example, what particular is the dataset should be shuffled in each epoch. The poseudocode of SGC looks like this: 12345for i in range(epochs): np.random.shuffle(data) for example in data: param_grad = calculate_gradient(loss_function , example) param = param - learning_rate * param_grad The algorithm is not as complicated as its name implies. Actually, it’s too simple that we may doubt about its effectiveness in convergence. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, the convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. A simple improvement of SGD is to perform an update for every mini-batch examples, which is called mini-batch gradient. It is a compromise between batch gradient and SGD and is proved effective in practices. The code looks like this: 12345for i in range(epochs): np.random.shuffle(data) for batch in batches(data, batch_size = 50): param_grad = calculate_gradient(loss_function , batch) param = param - learning_rate * param_grad","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"AI算法","slug":"AI算法","permalink":"http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"}]},{"title":"Backward propagation of Neural Network explained","slug":"Backward-propagation-of-Neural-Network-explained","date":"2018-11-08T11:14:28.000Z","updated":"2020-08-23T20:27:48.553Z","comments":true,"path":"2018/11/08/Backward-propagation-of-Neural-Network-explained/","link":"","permalink":"http://yoursite.com/2018/11/08/Backward-propagation-of-Neural-Network-explained/","excerpt":"","text":"Backpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of ‘dark magic’ we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way. Outline Overview of the architecture Initialize parameters Implement forward propagation Compute Loss Implement Backward propagation Update parameters 1. The architectureThis neural network I’m going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. The architecture in the math formula$$f(X)= Relu( Sigmoid( X \\times W_{ih} +b_{1}) \\times W_{ho} + b_{2})$$ ##2.Initialize parameters We take one example which has two features like below$$X= [x1, x2]= [0.1, 0.2]$$The parameters are taken randomly.$$W_{is}= \\begin{bmatrix}W_{i1h1} &amp; W_{i1h2}\\W_{i2h1} &amp; W_{i2h2}\\end{bmatrix}=\\begin{bmatrix}0.7 &amp; 0.6\\0.5 &amp; 0.4\\end{bmatrix}$$ $$W_{sr}=\\begin{bmatrix}W_{h1o1}&amp;W_{h1o2}\\W_{h2o1}&amp;W_{h2o2}\\end{bmatrix}=\\begin{bmatrix}0.4&amp;0.6\\0.9&amp;0.8\\end{bmatrix}$$ $$b_{1}=[b_{11}, b_{12}] =[0.5,0.6]$$ $$b_{2}= [b_{21}, b_{22}]= [0.7, 0.9]$$ 3. Forward Propagation3.1 Layer1: Linear$$X \\times W_{ih} + b_{1}=[x1, x2]\\times\\begin{bmatrix}W_{i1h1} &amp; W_{i1h2}\\W_{i2h1} &amp; W_{i2h2}\\end{bmatrix}+[b_{11}, b_{12}]=[0.1, 0.2]\\times\\begin{bmatrix}0.7 &amp; 0.6\\0.5 &amp; 0.4\\end{bmatrix}+[0.5, 0.6]=[0.67, 0.74]$$ Sigmoid$$Sigmoid(x)=1/(1+e^{-x})$$ $$[h_{out1},h_{out2}]=Sigmoid(X \\times W_{ih} + b_{1}) =[Sigmoid(0.67),Sigmoid(0.74)]=[0.6615,0.6770]$$ 3.2 Layer2: Linear$$[o_{in1}, o_{in2}]=[h_{out1}, h_{out2}] \\times W_{ho} + b_{2}=[h_{out1}, h_{out2}]\\times\\begin{bmatrix}W_{h1o1} &amp; W_{h1o2}\\W_{h2o1} &amp; W_{h2o2}\\end{bmatrix}+[b_{21}, b_{22}]=\\[0.6615, 0.6770]\\times\\begin{bmatrix}0.7 &amp; 0.5\\0.6 &amp; 0.4\\end{bmatrix}+[0.5, 0.6]= [1.3693, 1.0391]$$ Softmax$$Softmax(x){j}= \\frac{e^{xj}}{\\sum{i=1}^n e^{xi}} (j=1,2…n)$$ $$[o_{out1},o_{out2}]=Softmax(X\\times W_{ho} + b_{2})=[\\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} ,\\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}}]=[0.5818,0.4182]$$ ##4. Compute Loss The Loss function here we use is cross-entropy cost$$Crossentropy= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{L}\\right))$$The actual output should be$$[y_{1},y_{2}]=[1.00, 0.00]$$Since we only have one example, that means ‘m = 1’, the total loss is computed as follows :$$-\\frac{1}{1} \\sum\\limits_{i = 1}^{1} (y_{1}\\log\\left(o_{out1}\\right) + 0 +0 + 1log(1-o_{out2}))=-(1log(0.5818)+0+0+1*log(1-0.4182))==0.4704$$ 5. Backward PropagationIn this section, we will go through backward propagation stage by stage. 5.1 Basic Derivatives####Sigmoid: $$\\frac{\\partial Sigmoid(x)}{\\partial x}=\\frac{\\partial \\frac{1}{(1+e^{-x})}}{\\partial x}=\\frac{ e^{-x}}{(1+e^{-x})^2}=(\\frac{1+e^{-x}-1}{1+e^{-x}})\\frac{1}{1+e^{-x}}=(1 - Sigmoid(x))\\times Sigmoid(x)$$ ####Softmax: At first we know: For$$f(x)=\\frac{g(x)}{h(x)}$$ $$f’(x) = \\frac{g’(x)h(x)-g(x)h’(x)}{[h(x)]^2}$$ Then the derivation of Softmax is$$\\frac{\\partial Softmax(x)}{\\partial x1}=\\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \\frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2}$$ ###5.2 The backward Pass 5.2.1 Layer1-Layer2####Weight derivatives with respect to the error Consider Who , we want to know how Who will affect the total error, aka the value of$$\\frac{\\partial E_{total}}{\\partial W_{ho}}$$Chain Rule states that:$$\\frac{\\partial z}{\\partial x} =\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}$$So we have$$\\frac{\\partial E_{total}}{\\partial W_{h2o1}}=\\frac{\\partial E_{total}}{\\partial o_{out1}}\\cdot \\frac{\\partial o_{out1}}{\\partial o_{int1}}\\cdot \\frac{\\partial o_{int1}}{\\partial W_{h2o1}}$$Let’s break this through stage by stage Stage1 $$\\frac{\\partial E_{total}}{\\partial o_{out1}} =\\frac{\\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\\partial o_{out1}}+0=-\\frac{1}{o_{out1}}=-1/0.5818=-1.719$$ Stage2 $$\\frac{\\partial o_{1}}{\\partial i_{1}}=\\frac{\\partial Softmax(i_{1})}{\\partial i_{1}}=\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}=\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}=\\frac{e^{1.3693}\\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433$$ Stage3 $$\\frac{\\partial o_{in1}}{\\partial W_{h2o1}} =\\frac{\\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\\partial W_{h2o1}}= h_{out2}=0.6770$$ Finally we apply the chain rule:$$\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831$$Let’s go through all the weights in Layer2$$W_{ho}’=\\begin{bmatrix}W_{h1o1}’ &amp; W_{h1o2}’\\W_{h2o1}’ &amp; W_{h2o2}’\\end{bmatrix}=\\begin{bmatrix}\\frac{\\partial E_{total}}{\\partial W_{h1o1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{h1o2}} \\\\frac{\\partial E_{total}}{\\partial W_{h2o1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{h2o2}}\\end{bmatrix}=\\begin{bmatrix}\\frac{\\partial E_{total}}{\\partial o_{out1}}\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}}\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h1o1}}&amp;\\frac{\\partial E_{total}}{\\partial o_{out2}}\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}}\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h1o2}}\\\\frac{\\partial E_{total}}{\\partial o_{out1}}\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}}\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h2o1}}&amp;\\frac{\\partial E_{total}}{\\partial o_{out2}}\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}}\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h2o2}}\\end{bmatrix}\\=\\begin{bmatrix}-1.7190.24330.6615 &amp; -2.39120.24330.6615 \\-1.7190.24330.6770 &amp; -2.39120.24330.6770\\end{bmatrix}=\\begin{bmatrix}-0.2767 &amp; -0.3733\\-0.2831 &amp; -0.3853\\end{bmatrix}$$ Update weights according to learning rateOur training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02. We got the updated weight matrix as folows$$W_{ho}^* =\\begin{bmatrix}W_{h1o1} - \\eta W_{h1o1}’ &amp; W_{h1o2} - \\eta W_{h1o2}’\\W_{h2o1} - \\eta W_{h2o1}’ &amp; W_{h2o2} - \\eta W_{h2o2}’\\end{bmatrix}=\\begin{bmatrix}0.4 - 0.02*(-0.2767)&amp;0.6 - 0.02*(-0.3733)\\0.9 - 0.02*(-0.2831)&amp;0.8 - 0.02*(-0.3853)\\end{bmatrix}\\=\\begin{bmatrix}0.4055 &amp; 0.6075\\0.9057 &amp; 0.8077\\end{bmatrix}$$That is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows. ####5.2.2 Layer0(Input Layer) - Layer1 Follow the path of the previous chapter Stage1: $$\\frac{\\partial h_{out1}}{\\partial h_{in1}}=\\frac{\\partial Sigmoid(h_{in1})}{\\partial h_{in1}}=Sigmoid(h_{in1})(1-Sigmoid(h_{in1}))=\\Sigmoid(0.67)(1-Sigmoid(0.67))=0.2239$$ Stage2: $$\\frac{\\partial h_{in1}}{\\partial W_{i2h1}}=\\frac{\\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\\partial W_{i2h1}}=x2=0.2$$ Apply the chain rule:$$\\frac{\\partial E_{total}}{\\partial W_{i2h1}}=\\frac{\\partial E_{total}}{\\partial h_{out1}}\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}}\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}$$We already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction.$$\\frac{\\partial E_{total}}{\\partial h_{out1}}=\\frac{\\partial E_{total}}{\\partial o_{out1}}\\frac{\\partial o_{out1}}{\\partial o_{in1}}\\frac{\\partial o_{in1}}{\\partial h1_{out}}$$We have computed the first and second results, and the third one is merely a deviation of the linear function$$\\frac{\\partial E_{total}}{\\partial h_{out1}}=-1.7190.2433\\frac{\\partial(h_{out1}W_{h1o1} + h_{out2}W_{h2o1})}{\\partial h_{out1}}=\\-1.7190.2433W_{h1o1}=-1.7190.24330.4=-0.1673$$Then we got$$\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= -0.16730.2239 * 0.2=-0.0075$$Similarly , we can get the Layer0-Layer1 derivatives with respective to the total error$$W_{ih}’=\\begin{bmatrix}\\frac{\\partial E_{total}}{\\partial W_{i1h1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{i1h2}}\\\\frac{\\partial E_{total}}{\\partial W_{i2h1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{i2h2}}\\end{bmatrix}=\\begin{bmatrix}\\frac{\\partial E_{total}}{\\partial h_{out1}}\\cdot \\frac{\\partial h1_{out}}{\\partial h1_{in}}\\cdot \\frac{\\partial h1_{in}}{\\partial W_{i1h1}}&amp;\\frac{\\partial E_{total}}{\\partial h_{out2}}\\cdot \\frac{\\partial h2_{out}}{\\partial h2_{in}}\\cdot \\frac{\\partial h2_{in}}{\\partial W_{i1h2}}\\\\frac{\\partial E_{total}}{\\partial h_{out1}}\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}}\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}&amp;\\frac{\\partial E_{total}}{\\partial h_{out2}}\\cdot \\frac{\\partial h_{out2}}{\\partial h_{in2}}\\cdot \\frac{\\partial h_{in2}}{\\partial W_{i2h2}}\\end{bmatrix}\\=\\begin{bmatrix}-0.1673*0.2239 * 0.1 &amp; -0.4654*0.21870.1\\-0.16730.2239 * 0.2 &amp; -0.4654*0.21870.2\\end{bmatrix}\\=\\begin{bmatrix}-0.0037 &amp; -0.0102\\-0.0075 &amp; -0.0204\\end{bmatrix}$$ Update weights according to learning rateUpdate the weights with learning rate 0.02，we got the final weight matrix$$W_{ih}^*=\\begin{bmatrix}W_{i1h1} - \\eta (W_{i1h1}’) &amp; W_{i1h2} - \\eta (W_{i1h2}’)\\W_{i2h1} - \\eta (W_{i2h1}’) &amp; W_{i2h2} - \\eta (W_{i2h2}’)\\end{bmatrix}=\\begin{bmatrix}(0.7 -0.2*(-0.0037))&amp; (0.6 - 0.2 (-0.0102)) \\(0.5 - 0.2(-0.0075)) &amp; (0.4 - 0.2 * (-0.0204))\\end{bmatrix}\\=\\begin{bmatrix}0.7007 &amp; 0.6020\\0.5015 &amp; 0.4041\\end{bmatrix}$$ ###5.3 Wrap up Finally we get all the weights updated$$W_{ho}^* =\\begin{bmatrix}0.4055 &amp; 0.6075\\0.9057 &amp; 0.8077\\end{bmatrix}$$ $$W_{ih}^*=\\begin{bmatrix}0.7007 &amp; 0.6020\\0.5015 &amp; 0.4041\\end{bmatrix}$$ 6. Conclusion Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these “smart” gates make it possible for complicated prediction or classification tasks. The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates “vanish” significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice. If we look back to the computing process, a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions. 7. Reference https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b http://cs231n.github.io/optimization-2/ I https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/ https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"AI算法","slug":"AI算法","permalink":"http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"}]},{"title":"[Pandas]How to plot counts of each value","slug":"Pandas-How-to-plot-counts-of-each-value","date":"2018-10-22T19:40:47.000Z","updated":"2020-08-23T20:26:15.045Z","comments":true,"path":"2018/10/22/Pandas-How-to-plot-counts-of-each-value/","link":"","permalink":"http://yoursite.com/2018/10/22/Pandas-How-to-plot-counts-of-each-value/","excerpt":"","text":"With pandas build in function(actually matplotlib) 12345import pandas as pd% matplotlib inlinecol_values = (&#x27;x&#x27;, &#x27;x&#x27;, &#x27;y&#x27;, &#x27;y&#x27; , &#x27;y&#x27;, &#x27;z&#x27;)df = pd.DataFrame(&#123;&#x27;col&#x27;:col_values&#125;)df[&#x27;col&#x27;].value_counts().plot.bar() With seaborn 12import seaborn as snssns.countplot(df[&#x27;col&#x27;])","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"[Pandas]How to calculate datetime difference in years","slug":"Pandas-How-to-calculate-datetime-difference-in-years","date":"2018-10-22T15:43:34.000Z","updated":"2020-08-23T20:26:45.938Z","comments":true,"path":"2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/","link":"","permalink":"http://yoursite.com/2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/","excerpt":"","text":"Suppose you got a person’s regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result. With the help of relativedelta 1234from dateutil import relativedeltafor i in df.index: df.loc[i, &#x27;age&#x27;] = relativedelta.relativedelta(df.loc[i, &#x27;regDate&#x27;], df.loc[i, &#x27;DateOfBirth&#x27;]).years With the help of np.timedelta64 12345import numpy as npdf[&#x27;age&#x27;] = (df[&#x27;regDate&#x27;] - df[&#x27;DateOfBirth&#x27;])/np.timedelta64(1, &#x27;Y&#x27;)# You may need to convert to integerdf[&#x27;age&#x27;].apply(np.int64)","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"[Pandas]How to list all columns","slug":"Pandas-How-to-list-all-columns","date":"2018-10-19T21:31:23.000Z","updated":"2020-08-23T20:26:23.016Z","comments":true,"path":"2018/10/19/Pandas-How-to-list-all-columns/","link":"","permalink":"http://yoursite.com/2018/10/19/Pandas-How-to-list-all-columns/","excerpt":"","text":"1list(dataframe.columns.values)","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"[Pandas] How to import from Sql Server","slug":"Pandas-How-to-import-from-Sql-Server","date":"2018-10-18T19:08:25.000Z","updated":"2020-08-23T20:26:28.061Z","comments":true,"path":"2018/10/18/Pandas-How-to-import-from-Sql-Server/","link":"","permalink":"http://yoursite.com/2018/10/18/Pandas-How-to-import-from-Sql-Server/","excerpt":"","text":"We need to rely on pyodbc, the sample code is as belows. 12import pyodbccnxn = pyodbc.connect(&#x27;DRIVER=&#123;ODBC Driver 13 for SQL Server&#125;;SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes&#x27;) Write SQL and execute with pandas.read_sql 123import pandasquery = &quot;SELECT * from a&quot;df = pd.read_sql(query, sql_conn)","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"Basics of words embedding","slug":"Basics-of-words-embedding","date":"2018-10-01T10:14:28.000Z","updated":"2020-08-23T20:27:33.747Z","comments":true,"path":"2018/10/01/Basics-of-words-embedding/","link":"","permalink":"http://yoursite.com/2018/10/01/Basics-of-words-embedding/","excerpt":"","text":"Why embeddingNatural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing: These encodings provide no useful information regarding the relationships that may exist between the individual symbols. Discrete ids furthermore lead to data sparsity. We may need more data to train statistical models successfully. To address these two problems, word embeddings provide a solution to represent words and their relative meanings densely. OverviewEmbedding derives from Vector Space Models(VSMs), one of its well-known schemes is Tf-Idf weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that words that appear in the same contexts share semantic meaning. VSMs have two main approaches: Count-based methods and predictive methods. Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense embedding vectors (considered parameters of the model). Among all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular. Count-based EmbeddingGloVe is an unsupervised learning algorithm for obtaining vector representations for words. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words to capture as much as possible the meaning specified by the juxtaposition of two words. The project page of glove gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration: The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. It is necessary for a model to associate more than a single number to the word pair. Training GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost. project page of glove also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example 12the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ... ‘’the’’ is followed by 100 floats which are the vector values of this word We can build a dict whose key is words and value is their glove vector 12345678embeddings_index = dict()f = open(&#x27;./glove.6B.100d.txt&#x27;)for line in f: values = line.split() word = values[0] coefs = asarray(values[1:], dtype = &#x27;float32&#x27;) embeddings_index[word] = coefsf.close() When we need to build a embedding layer , we just look up the vectors for input words in the dict. Word2vecWord2vec, as illustrated in the first part, is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. ‘mat’) from source context words (‘the cat sits on the’), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore, CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. We will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model. The Skip-gram model is trained like this, Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. The coding example of how to build and train the Skip-gram model can be found here Next post I will try to use embedding to solve an interesting real world problem.","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"AI算法","slug":"AI算法","permalink":"http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"}]},{"title":"What is tf.data and how to use","slug":"What-is-tf-data-and-how-to-use","date":"2018-09-26T19:30:31.000Z","updated":"2020-08-23T20:28:15.157Z","comments":true,"path":"2018/09/26/What-is-tf-data-and-how-to-use/","link":"","permalink":"http://yoursite.com/2018/09/26/What-is-tf-data-and-how-to-use/","excerpt":"","text":"Tf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. Here is the defination of Dataset given by tensorflow.org A Dataset can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a “logical plan” of transformations that act on those elements. To summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed, the following part will explain that as well as some useful APIs in preprocessing data. 1. Data inputDataset can be built from several sources including csv file, numpy array and tensors. From CSVTf.data provides a convenient API make_csv_dataset to read records from one or more csv files. Suppose the csv file is 123a,b,c,dhow,are,you,mateI,am,fine,thanks We can build a dataset from the above csv in the following way 1dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2) Here batch_size represents how many records would be aquired in a batch We can use Iterator to see what contains in this dataset 12batch = dataset.make_one_shot_iterator().get_next()print(batch[&#x27;a&#x27;]) The result is 1tf.Tensor([&#39;how&#39; &#39;I&#39;], shape&#x3D;(2,), dtype&#x3D;string) make_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this 12how,are,you,mateI,am,fine,thanks We can set header=Falseand column_names=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;] 1dataset2 &#x3D; tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size&#x3D;2, header&#x3D;False,column_names&#x3D;[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;]) Dataset2 should have the same value with dataset1 From Tensor slicesWe can also create a dataset from tensors, the related API is tf.data.Dataset.from_tensor_slices() 1dataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([10, 5])) Actually the input of this API is not necessarily tensors, numpy arrays are also adaptable . 1dataset3 &#x3D; tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5))) 2. Data consumingThe only way to retrieve the data is Iterator(), Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are make_one_shot_iterator and make_initializable_iterator. make_one_shot_iteratorThe examples can be find in the first part when we show how to import csv files make_initializable_iteratorCompared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example 12345678910# using a placeholderx = tf.placeholder(tf.float32, shape=[None,2])dataset = tf.data.Dataset.from_tensor_slices(x)data = np.random.sample((100,2))iter = dataset.make_initializable_iterator() # create the iteratorel = iter.get_next()with tf.Session() as sess: # feed the placeholder with data sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) print(sess.run(el)) # output [ 0.11342909, 0.81430183] 3. Data proprocessingTf.data provides several tools for data preprocessing such as batch and shuffle Batchdataset.batch(BATCH_SIZE) given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time. 12345678# BATCHINGtf.enable_eager_execution()BATCH_SIZE = 2x = np.array([1,2,3,4])# make a dataset from a numpy arraydataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()iter.get_next() Output 1&lt;tf.Tensor: id=102, shape=(2,), dtype=int64, numpy=array([1, 2])&gt; ShuffleWhen preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that. 123456BATCH_SIZE = 2x = np.array([1,2,3,4])# make a dataset from a numpy arraydataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = 10).batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()iter.get_next() Output 1&lt;tf.Tensor: id=115, shape=(2,), dtype=int64, numpy=array([2, 3])&gt;","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/tags/tensorflow/"}]},{"title":"[Pandas]Handle missing data","slug":"Pandas-Handle-missing-data","date":"2018-09-23T19:40:37.000Z","updated":"2020-08-23T20:26:50.597Z","comments":true,"path":"2018/09/23/Pandas-Handle-missing-data/","link":"","permalink":"http://yoursite.com/2018/09/23/Pandas-Handle-missing-data/","excerpt":"","text":"Missing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it. ###Produce some data with missing values. 12345678910import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(5, 3), index=[&#x27;a&#x27;, &#x27;c&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;h&#x27;], columns=[&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;])df[&#x27;four&#x27;]=&#x27;bar&#x27;df[&#x27;five&#x27;]= df[&#x27;one&#x27;] &gt; 0df[&#x27;date&#x27;]= pd.Timestamp(&#x27;2018-01-01&#x27;)df2 = df.reindex([&#x27;a&#x27;,&#x27;b&#x27;, &#x27;c&#x27;,&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;,&#x27;g&#x27;, &#x27;h&#x27;])df2 one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 b NaN NaN NaN NaN NaN NaT c -0.297019 1.357266 -0.665749 bar False 2018-01-01 d NaN NaN NaN NaN NaN NaT e 0.311524 -0.328388 0.777467 bar True 2018-01-01 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 g NaN NaN NaN NaN NaN NaT h -1.303842 1.239911 -0.909255 bar False 2018-01-01 Notice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime Detect missing valuepandas provides two methods, isna() and notna() 1df2.isna() one two three four five date a False False False False False False b True True True True True True c False False False False False False d True True True True True True e False False False False False False f False False False False False False g True True True True True True h False False False False False False 1df2.notna() one two three four five date a True True True True True True b False False False False False False c True True True True True True d False False False False False False e True True True True True True f True True True True True True g False False False False False False h True True True True True True Cleaningdrop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0 1df2.dropna() one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 c -0.297019 1.357266 -0.665749 bar False 2018-01-01 e 0.311524 -0.328388 0.777467 bar True 2018-01-01 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 h -1.303842 1.239911 -0.909255 bar False 2018-01-01 Drop columns with missing data 1df2.dropna(axis = 1) a b c d e f g h All columns are removed.. Fill naNa value can be easily filled by fillna() 1df2.fillna(0) one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 00:00:00 b 0.000000 0.000000 0.000000 0 0 0 c -0.297019 1.357266 -0.665749 bar False 2018-01-01 00:00:00 d 0.000000 0.000000 0.000000 0 0 0 e 0.311524 -0.328388 0.777467 bar True 2018-01-01 00:00:00 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 00:00:00 g 0.000000 0.000000 0.000000 0 0 0 h -1.303842 1.239911 -0.909255 bar False 2018-01-01 00:00:00 We can also fill data with respect to their column property 1df2.fillna(value = &#123;&#x27;one&#x27;: 0, &#x27;four&#x27;: &#x27;missing&#x27;, &#x27;five&#x27;: False, &#x27;date&#x27;: pd.Timestamp(&#x27;2000-01-01&#x27;)&#125;) one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 b 0.000000 NaN NaN missing False 2000-01-01 c -0.297019 1.357266 -0.665749 bar False 2018-01-01 d 0.000000 NaN NaN missing False 2000-01-01 e 0.311524 -0.328388 0.777467 bar True 2018-01-01 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 g 0.000000 NaN NaN missing False 2000-01-01 h -1.303842 1.239911 -0.909255 bar False 2018-01-01 Remind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this. 1df2.fillna(value = 0, inplace = True)","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"[Pandas]How to select data","slug":"Pandas-How-to-select-data","date":"2018-09-20T20:49:51.000Z","updated":"2020-08-23T20:28:43.534Z","comments":true,"path":"2018/09/20/Pandas-How-to-select-data/","link":"","permalink":"http://yoursite.com/2018/09/20/Pandas-How-to-select-data/","excerpt":"","text":"This article shows the most common methods regarding data selection First, let’s create a dataframe. 12345import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.rand(5,4), index = [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;], columns= [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;])df A B C D a 0.122248 0.581777 0.972888 0.869366 b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 d 0.077122 0.407514 0.019102 0.553546 e 0.921520 0.157199 0.371028 0.825792 Use [] for columns selection1df[[&#x27;A&#x27;, &#x27;B&#x27;]] A B a 0.122248 0.581777 b 0.476451 0.453979 c 0.954790 0.747131 d 0.077122 0.407514 e 0.921520 0.157199 Select a range1df[1:3] # or df[&#x27;b&#x27;:&#x27;c&#x27;] A B C D b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 Use .loc1df.loc[&#x27;a&#x27;:&#x27;c&#x27;, [&#x27;A&#x27;,&#x27;B&#x27;]] A B a 0.122248 0.581777 b 0.476451 0.453979 c 0.954790 0.747131 Select all columns1df.loc[&#x27;a&#x27;:&#x27;c&#x27;, :] A B C D a 0.122248 0.581777 0.972888 0.869366 b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 Select with boolean1df[df[&#x27;A&#x27;]&gt;0.2] A B C D b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 e 0.921520 0.157199 0.371028 0.825792 Select with callable(lambda)1df[lambda df: df[&#x27;A&#x27;]&gt;0.2] A B C D b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 e 0.921520 0.157199 0.371028 0.825792","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"What is Attention and how to use","slug":"What-is-Attention-and-how-to-use","date":"2018-09-20T09:06:45.000Z","updated":"2020-08-23T20:28:10.855Z","comments":true,"path":"2018/09/20/What-is-Attention-and-how-to-use/","link":"","permalink":"http://yoursite.com/2018/09/20/What-is-Attention-and-how-to-use/","excerpt":"","text":"IntroductionAttention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT) and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow) Why AttentionAttention is a mechanism derived from the seq-seq model which started the era of NMT, in this paper Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation. The model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus. Attention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder. Maybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector. What is AttentionAttention is merely a context vector that provides a richer encoding of the source sequence. The vector is computed at every decoder time step. As illustrated in the figure above, the attention computation can be summarized into the following three steps: Compute attention weights based on the current target hidden state and all source state(Figure 1) The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2) Context vector combined with the current target hidden state yields the attention vector(Figure 3) The attention vector is then fed to the next decoding step. the score in Figure 1 is computed as follows: Regarding the score, the methods by which it is calculated lead to different performance. Coding Attention with TensorflowSuppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3). 123456789101112# Transfer encoder_outputs to attention_states attention_states = tf.transpose(encoder_outputs, [1, 0, 2])# Apply existing attention mechanism attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( num_units, attention_states, memory_sequence_length=source_sequence_length)# Feed to the decoderdecoder_cell = tf.contrib.seq2seq.AttentionWrapper( decoder_cell, attention_mechanism, attention_layer_size=num_units) The rest codes are mostly the same as standard encoder-decoder.","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"AI算法","slug":"AI算法","permalink":"http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"}]},{"title":"[Pandas]How to drop columns/rows","slug":"Pandas-How-to-drop-columns-rows","date":"2018-09-18T20:39:25.000Z","updated":"2020-08-23T20:26:40.596Z","comments":true,"path":"2018/09/18/Pandas-How-to-drop-columns-rows/","link":"","permalink":"http://yoursite.com/2018/09/18/Pandas-How-to-drop-columns-rows/","excerpt":"","text":"12345import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;&#x27;A#&#x27;:[1,2,3], &#x27;B#&#x27;:[4,5,6], &#x27;C#&#x27;:[7,8,9]&#125;)df A# B# C# 1 4 7 2 5 8 3 6 9 Drop columns123df.drop([&#x27;B#&#x27;, &#x27;C#&#x27;], axis = 1)#ordf.drop(columns=[&#x27;B#&#x27;, &#x27;C#&#x27;]) A# 1 2 3 Drop rows1df.drop([0,1]) A# B# C# 3 6 9","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"[Pandas]How to rename columns","slug":"Pandas-How-to-rename-columns","date":"2018-09-17T21:11:55.000Z","updated":"2020-08-23T20:28:57.830Z","comments":true,"path":"2018/09/17/Pandas-How-to-rename-columns/","link":"","permalink":"http://yoursite.com/2018/09/17/Pandas-How-to-rename-columns/","excerpt":"","text":"12345import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;&#x27;A#&#x27;:[1,2], &#x27;B#&#x27;:[3,4], &#x27;C#&#x27;:[5,6]&#125;)df A# B# C# 1 3 5 2 4 6 Rename with DataFrame.rename() 1df.rename(index= str, columns=&#123;&quot;A#&quot;: &quot;a&quot;, &quot;B#&quot;: &quot;b&quot;&#125;) a b C# 1 3 5 2 4 6 Print df again A# B# C# 1 3 5 2 4 6 Column names did’nt change. To change names permanently, use “inplace=True” 12df.rename(index= str, columns=&#123;&quot;A#&quot;: &quot;a&quot;, &quot;B#&quot;: &quot;b&quot;&#125;, inplace = True)df a b C# 1 3 5 2 4 6 Rename in batch 1df.rename(index= str, columns=lambda x:x.replace(&#x27;#&#x27;,&#x27;&#x27;)) A B C 1 3 5 2 4 6 Lambda is amazing","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"[Pandas]How to import CSV","slug":"Pandas-How-to-import-csv","date":"2018-09-16T15:30:37.000Z","updated":"2020-08-23T20:26:35.426Z","comments":true,"path":"2018/09/16/Pandas-How-to-import-csv/","link":"","permalink":"http://yoursite.com/2018/09/16/Pandas-How-to-import-csv/","excerpt":"","text":"Locate the fileThere are two ways to locate the csv file, “absolute path” or “relative path”e.g.Absolute path:“/work/project/data/great.csv“(Mac) or “C:\\work\\project\\data\\great.csv“(Windows)Relative path:“data/greate.csv“(Mac) or “data\\great.csv“(Windows) Read with pandas.read_csv()The result is a dataframe Object 12345import pandas as pdimport numpy as np df = pd.read_csv(&#x27;data/great.csv&#x27;)df company locate employees avenue orange New York 10000 4000.0 banana London 2000 1000.0 pinch Paris 4000 5000.0 pear Berlin 3000 NaN No infered header12df = pd.read_csv(&#x27;data/great.csv&#x27;, header= None)df 0 1 2 3 company locate employees avenue orange New York 10000 4000 banana London 2000 1000 pinch Paris 4000 5000 pear Berlin 3000 NaN Filter rows12df = pd.read_csv(&#x27;data/great.csv&#x27;, skiprows=[3,4])df company locate employees avenue orange New York 10000 4000 banana London 2000 1000 lambda is also welcomed 12df = pd.read_csv(&#x27;data/great.csv&#x27;, skiprows=lambda x:x/2==1)df company locate employees avenue orange New York 10000 4000.0 pear Berlin 3000 NaN Filter columns(or keep wanted)12df = pd.read_csv(&#x27;data/great.csv&#x27;, usecols=[&#x27;company&#x27;])df company orange banana pinch pear Other options refer https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"Tensorflow101","slug":"Tensorflow101","date":"2018-07-10T21:25:42.000Z","updated":"2020-08-23T20:28:03.972Z","comments":true,"path":"2018/07/10/Tensorflow101/","link":"","permalink":"http://yoursite.com/2018/07/10/Tensorflow101/","excerpt":"","text":"Tensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning. How to InstallIf you have ‘pip’, everything is simplepip install tensorflow Basic Concepts Graph Graph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is h=ReLU(Wx+b) In the view of Tensorflow, the function looks like this Nodes Variables such as W and b , placeholders such as x, are operations such as MatMul, Add are all nodes in the graph. Edges The edges between nodes indicate the data which flow between nodes, in tensorflow data is represented as “tensor” . “tensor” + “flow” = “tensorflow” Codes1234567891011121314151617181920import tensorflow as tfimport numpy as npif __name__ &#x3D;&#x3D; &#39;__main__&#39;: b &#x3D; tf.Variable(tf.zeros((10,))) W &#x3D; tf.Variable(tf.random_uniform((20, 10), -1, 1)) x &#x3D; tf.placeholder(tf.float32, (10, 20)) h &#x3D; tf.nn.relu(tf.matmul(x, W) + b) writer &#x3D; tf.summary.FileWriter(&#39;.&#x2F;graphs&#39;, tf.get_default_graph()) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(h, &#123;x: np.random.random((10, 20))&#125;) writer.close() Here are the steps described in the above codes. Create a graph using Variables and placeholders. Start a tensorflow session and deploy the graph into the session. Run the session, let the tensors flow. Write processing logs using tools such as tf.summary. Session is the so called execution environment. It needs two parameters which are “Fetches” and “Feeds”. sess.run(fetches, feeds) Fetches: List of graph nodes Feeds: Dictionary mapping from graph nodes to concrete values. In the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))} It would be interesting to see what happened during the running time. We can try TensorBoard. TensorBoard Result The Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.","categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/tags/tensorflow/"}]},{"title":"我所理解的分布式调度","slug":"我所理解的分布式调度","date":"2017-03-25T19:37:04.000Z","updated":"2020-08-23T20:28:26.759Z","comments":true,"path":"2017/03/25/我所理解的分布式调度/","link":"","permalink":"http://yoursite.com/2017/03/25/%E6%88%91%E6%89%80%E7%90%86%E8%A7%A3%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6/","excerpt":"","text":"对于规模以上的应用来说，调度系统已经是必不可少的组成部分，尤其在基于数据分析的后台应用大量增长的今天，健壮的调度任务管理已经是非常重要的一环，因此多花些时间来分析研究调度系统的设计对于日常的开发与运维具有比较重要的意义。 调度问题是怎么来的当你的网站是个简单的blog，而且并不需要跟外部交互的时候，你大概不需要调度任务，因为此时网站需要处理的任务仅限于 即时交互 ， 即用户想使用一个功能，你就立即给他就是了，如同你在简书上写一篇文章，一点保存，这篇文章立即就保存到网站的后台服务器中去了，这也是互联网刚出现时候的最早的应用模式。 之后因为网站发展的不错，用户多了起来，就发现需要大量处理一些非即时的任务，比如要定时将用户访问日志归档清理，这个时候一般情况下会在服务器启动一个定时任务，在每天固定时间发起清理归档，又如你想显示一下网站的访客流量、发表文章数、评论统计，由于并非每次用户或者后台管理员每次需要看的时候都去计算一遍，所以可能又需要启动另一个任务来去处理这些数据，这样的任务多了，就需要思考一个问题，哪些任务要先处理，哪些任务要后处理，每个任务要占用多少资源，从而任务调度问题开始出现。 调度什么时候变得复杂在一个单机的系统，任务并不多的情况下，生活还依然是美好的，利用Linux自带的定时器或者系统框架提供的定时任务，实现好任务执行器，配置好任务触发的时间和频率，然后每天等待它自动触发，数据生成，任务搞定，似乎调度也没那么困难。 然而好景不长，伴随着网站的发展，你发现任务处理的越来越慢，甚至偶尔会有任务超时的情况，原因是每天用户产生的数据量越来越大，每次任务捞起的数量已经超载，依次执行完每个任务，可能已是最初执行时间的几倍。 这时稍有点经验你便会想到，任务没必要顺序执行啊，所以弄个线程池，起多个线程同时来捞数据然后执行，同时配置动态调整每次数据捞取的数量，增大执行频率，一系列组合拳打出去之后，调度任务又恢复正常，由于增加了并发数，甚至执行的比开始还更快了，就这样业务高峰便又顺利度过了。 调度什么时候变得更加复杂之前所描述的情况基本上都在单机的情况，网站的QPS（每秒处理的任务数量）基本在500以下，通过一系列的参数调优，串行变并行，任务运行的很平稳。然而当任务变的规模更大，比如十倍于当前，一台机器已经不能处理所有的任务，这时候需要增加更多的机器，将任务分配到不同的机器上，于是便有了所谓的分布式调度的问题。 分布式是目前稍大型的网站不可避免的问题，这种处理方案有很多好处，比如可以利用廉价的机器，可以（理论上）无限水平拓展，同时也带来了一系列棘手的问题，机器之间的网络通信，如何把流量均匀的分布于不同流量，如果有机器宕机了如何处理，每个问题都已经是一个庞大的技术领域。 对于调度的分布式问题，首先要解决的便是如何把任务分发到不同的机器上，这要求调度系统通常要至少分为两层，第一层决定一共要处理哪些任务并把任务分发到哪些机器上处理，第二层接到任务后具体执行。虽然描述起来很简单，但是这个处理过程实际上需要大量支撑系统的支持，比如在任务分发时如何判断哪些机器还活着可以处理任务，这可能需要一个可以感知整个集群运行状态的配置中心，又比如任务如何分发，是采用消息还是实时服务接口，如果是用消息派发则需要消息系统，如果是实时服务，又需要类似dubbo这样的分布式服务框架。当系统达到这个复杂度，已经不是将任务捞起并处理这么单纯，而是多个关联系统复杂性的叠加。 分布式调度的难题虽然分布式调度带来了复杂度的上升，但是它的水平拓展能力完美的适配了网站的规模发展，直到有一天你看到了类似这个错误： org.springframework.transaction.CannotCreateTransactionException 数据库连接池已经打满，由于单个数据库的连接数是一定的，这意味着数据库变成了资源瓶颈，这时候给任务处理happy的加机器已经不能提高系统的整体处理能力，这就如同你要运走一群人，给你提供的车辆是无限的，但是汽油确是一定的。当然数据库也是可以拓展的，但是考虑到数据迁移的复杂性，这几乎将问题的复杂度提高了一个等级。因此，一个分布式系统的吞吐量，在很大程度上是与数据库处理能力做权衡的结果。 写在最后分布式调度是一个巨大的话题，并且还在随着实际任务复杂度的提高而快速的更新，上面这些思考与总结也只是浮光掠影，还需要在实际工作中再深入体会与仔细研究。","categories":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://yoursite.com/tags/Distributed-System/"}]}],"categories":[{"name":"数据&AI","slug":"数据-AI","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE-AI/"},{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"AI算法","slug":"AI算法","permalink":"http://yoursite.com/tags/AI%E7%AE%97%E6%B3%95/"},{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/tags/tensorflow/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://yoursite.com/tags/Distributed-System/"}]}