{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2018-09-21T13:50:40.000Z","updated":"2020-08-21T21:56:24.929Z","comments":true,"path":"index.html","permalink":"http://yoursite.com/index.html","excerpt":"","text":"Machine Learning Stochastic Gradient Descent Backward propagation of Neural Network explained What is Attention and how to use Basics of words embedding Pandas How-to [Pandas]How to select data [Pandas]How to import CSV [Pandas]How to rename columns [Pandas]How to drop columns rows [Pandas]Handle missing data [Pandas]How to import from Sql Server [Pandas]How to list all columns [Pandas]How to calculate datetime difference in years [Pandas]How to plot counts of each value Tensorflow How-to Tensorflow101 What is tf.data and how to use"},{"title":"tags","date":"2020-08-24T20:52:19.000Z","updated":"2020-08-24T20:54:35.209Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-08-24T20:52:11.000Z","updated":"2020-08-24T20:54:02.441Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-08-24T20:55:34.000Z","updated":"2020-08-30T08:17:28.904Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"我是Feng, 一个在英国工作的程序员/架构， 同时也会做一些机器学习方面的研究。来英前在阿里巴巴有过六年工作经历。 邮箱 tf.wang.seu@gmail.com I’m Feng, a programmer and machine learning researcher in the UK. I worked at Alibaba for six years before I came to the UK. Email: tf.wang.seu@gmail.com (British countryside, Pic from The Times)"}],"posts":[{"title":"聊聊英国的报刊","slug":"UK-papers","date":"2020-09-12T10:13:59.000Z","updated":"2020-09-12T11:12:42.888Z","comments":true,"path":"2020/09/12/UK-papers/","link":"","permalink":"http://yoursite.com/2020/09/12/UK-papers/","excerpt":"","text":"虽然现在已经进入了电子时代，但是在英国你还是能看到数量繁多的传统报刊，周末的时候到市中心总是习惯性走进WHS（一家连锁书刊文具店）然后在报刊栏停留很久。陈列的杂志报刊各种题材最新的事件一目了然，随便翻翻就很满足。所有这些报刊中，我有订阅的有三个，经济学人、金融时报和卫报，下面聊下我对这几份报刊的感觉。 经济学人 经济学人（The economist，这里简称E了）是我是来英国之前就开始读的了，当时主要是为了学英语。最开始看的时候感觉真的是困难啊，太多的陌生词汇，往往是看到前两页就功力耗尽了。后来词汇量上来慢慢开始能看懂了，结合平时的一些写作需求，愈发体会到读E文章的好处。 其实先撇开内容，E的封面图就经常很吸引人，比如下面这期，“碰瓷”了之前大热的西班牙电影《消失的客人》， 描述了最近由于疫情原因各种远程上学甚至毕业典礼的情况 E的内容比较宏大，包含了政治、世界各地热点、商业、科技，感觉每次浏览就像是世界的国王浏览这一周的国际民生大事。 值得提一句的是，E有一版专门报道中国，其中有个专栏叫“茶馆”，也是很接地气了，记得最近有一期还报道了“乘风破浪的姐姐”，能从西方人视角看中国现象也是比较有趣。 然而其中最多的报道还是批评中国， 说实话有些还是失真，但这确实就代表了西方精英的主流看法。其实倒也不是E对中国有偏见，如果你翻开英国版，你能发现更多抨击本国政府文章，有些把政府已经说得感觉一无是处了。。当然国内朋友也不必介意，E早就在我国墙单之列。 E始创于1843年，快200年历史没有间断，就这一点，想想也是挺厉害的，并不只是其本身厉害，也在于这个社会的包容度，允许这么观点鲜明的文字留存这么长时间。 金融时报 金融时报（Financial Times）是我来到英国后发现的一个宝藏报纸，最初是在学校图书馆因为有免费的订阅，就时不时拿来翻看。开始主要是为了追踪股票市场信息，后来越看越上瘾，干脆定了一个纸质版的FT Weekend 每周会有人投递到家中。周六早晨，伴着一杯咖啡阅读，看来模仿有钱人生活也没那么难么。。哈哈 金融时报虽然是一个一看主题就很明显的报纸，但其实其周末版也是涵盖了很大其他领域的信息量，而且有意思的是，虽然你可能期待看到很多金融专业分析，但这这个报纸的编辑似乎刻意淡化理财方面的具体信息，表现就是其Money板块只是小版面的几页夹在社会时事、文化艺术、书籍推荐这些大版面之间，想告诉你的是钱虽然重要，但是重要的还是生活啊。虽然我每次还是先检出来Money看一下。。格局还是不高啊。 最关心的专栏作者就是理财建议方面的首席专栏Merryn Somrest Webb，之前看还没什么感觉，只是觉得每期都能看到这个大姐的置顶文章并配有其全身像照片（。。） 直到大概三个月前，美股在疫情影响下暴跌接近一个月后，在几乎所有证券分析、投行大佬都在看空经济的时候，Merryn大姐反其道而行之在专栏首页呼吁大家大胆抄底。嗯，我没有听，现在还在捶大腿。。 FT 还有一个有名的专栏叫与FT一起午餐，邀请世界各热点人物共进午餐，边吃边聊，内容很有趣，都是平时新闻稿上看不到的一面。FT甚至把一些午餐聊天的内容编辑成了一本书，在Amazon上也是热卖。 FT的政治观点是比较中立稍微偏左的，也可以说是带着金融家的冷酷无情的分析习惯，比如对于中国的态度，虽然也是随大流批评居多，但是面对这两年的中国经济，还是理性的服气的。比如最新一期这位Merryn女士用了一大篇幅论证现在是时候投资中国了，还披露了自己持有的中国有关的基金。 卫报 卫报（The Guardian）其实是最近才开始订阅其电子版，说来惭愧，开始关注主要是因为其UI设计的太好了，可以说是我看过报刊里版面设计最让人舒服的。板块内容也非常简洁，主要关注新闻、体育类，时政评论的不多，也不太爱讲中国的事情，其实其他像美国也不太关注，其主要新闻就是英国国内信息，也是偏向于客观的报道，感觉浏览起来没什么压力，比起经济学人和FT，卫报更像是一份中规中矩的资讯类报纸，不过深受英国本土居民喜欢。 本来是不需要订阅就可以看电子版，但是之前其在首页上呼吁读者订阅以支撑其因为疫情倍受打击的线下订阅。想着价格也不高（5.9镑一月）就支持了一下，因为实在是不想看到设计这么优秀的报纸干不下去啊。 卫报电子版有一个很有特色的Live板块会滚动播放最近事件，当有大事发生，比如恐袭之类的我会习惯性的进到这个页面追踪进度，时间线非常清楚。 写在最后 英国有特色的的报刊还有好多，比如专门写讽刺类新闻的Private Eye也是非常有趣，不一一列举。其实想找下国内哪些报纸风格可以对比一下这几大英报，但实在找不出来，或许之前的南方周末可以抗衡一下，但是也是好久之前的事情了。。想起这篇我也大概发不了在简书或者微信上，不禁一阵唏嘘。","categories":[{"name":"杂文","slug":"杂文","permalink":"http://yoursite.com/categories/%E6%9D%82%E6%96%87/"}],"tags":[]},{"title":"持仓风险分析--要不要买特斯拉","slug":"tsla-cov-analysis","date":"2020-09-05T14:35:15.000Z","updated":"2020-09-05T15:19:11.019Z","comments":true,"path":"2020/09/05/tsla-cov-analysis/","link":"","permalink":"http://yoursite.com/2020/09/05/tsla-cov-analysis/","excerpt":"","text":"声明：文中所述股票仅为分析举例之用，请独立作出投资决策。Quantopian 为免费分析平台，与本文无利益关系。 如果你有买股票，尤其是美股股票，今年不太可能没注意到特斯拉TSLA。特斯拉股票今年从低点涨了将近10倍，近两天股价大幅度回调，那么你应该去买进么？ 如果你是“信仰投资者”， 认定了一龙马教主的神功，那么就不必往下看了，闭着眼买然后把炒股软件删掉。 我长期也是看好特斯拉的，但是凯恩斯大师曾说过：“长期来看，我们都死了。。。”， 所以短期还是要尽可能分析一下。本文试图从持仓风险分析的角度来探讨下要不要买进TSLA， 所用量化分析平台为Quantopian，所用语言为Python 关于持仓风险，你大概听过最基本的是要多买一些股票，不要把鸡蛋放在一个篮子里，但是随便多买几只就可以降低风险了么？ 也不然，还是要想办法量化这个风险。有一种比较简单的分析方法，就是算一下现有持仓股票的协方差. 这个数值越高，表示你所持仓的股票其实价格走势越是类似， 这在持仓总回报上来讲如果上涨会涨的很高，但是下跌也会跌的很惨，就是所谓的波动率很大。我们显然不希望大的波动率，想象一下，你的持仓今天下跌10%明天上涨10%你是否还能睡好，可能早早就抛掉了，这就可能就会浪费很多机会。 原始持仓分析 假设你现在持仓是美股四大神兽[‘FB’, ‘GOOG’, ‘AMZN’, ‘AAPL’]，每只股票持仓比例是均等的 所以我们有持仓列表以及持股比例如下 universe = [‘FB’, ‘GOOG’, ‘AMZN’, ‘AAPL’] weights = np.array([0.25, 0.25, 0.25, 0.25]) 通过Quantopian获得这些股票过去一年的价格时间序列，并且将其正规化（Normalize）到（0，1）区间 prices = get_pricing(universe, fields=‘price’, start_date=start, end_date=end) normal_prices = (prices-prices.min())/(prices.max()-prices.min()) 接下来利用Pandas 提供的计算covariance_matrix的方法.corr()即可以获得当前持仓的协方差矩阵cov_matrix 这些数值从0到1， 数值越大表明两支股票走势越是相关，可以看到这几支股票相关性还是很高的（大部分&gt;0.8） 我们也可以绘制这个矩阵的热力图，利用seaborn 的.heatmap()方法 sns.heatmap(cov,xticklabels=cov.columns,yticklabels=cov.columns) 这样就可以根据颜色深浅直观的看到两支股票的相关性。 我们也可以计算持仓总体方差 total_variance = np.dot(np.dot(weights, cov_matrix.values), weights.T) 计算得到数值为 0.895608093887 原始持仓+TSLA 下面我们看下如果这个持仓我们加入了TSLA会有什么变化 现在的持仓列表以及权重分别为 universe_T = [‘FB’, ‘GOOG’, ‘AMZN’, ‘AAPL’, ‘TSLA’] weights_T = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) 同样计算方式可得，协方差矩阵 热力图 总体方差 0.895463150009 可以看到TSLA跟其他四支股票，尤其是AMZN，AAPL相关性非常高，总体的方差同之前持仓基本一致，在比较高的水平。那么面临是否要买特斯拉，你可能要问一下自己这个问题，首先，你是否要继续增加已经比较高的持仓风险水平。如果答案是肯定的，你就是非常看好，那么好消息是增加了特斯拉并没有大幅提高整体的风险，当然前提是根据过去一年表现来看。 原始持仓+BRK.B 作为对比，我们也可以分析一下，如果增加的不是TSLA而是股神的BRK.B 会有什么结果，此时的持仓列表和权重将会是 universe_B = [‘FB’, ‘GOOG’, ‘AMZN’, ‘AAPL’, ‘BRK.B’] weights_B = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) 我们可以得到协方差矩阵 热力图 可以看到BRK.B与其他四支股票走势有较大区别，这也符合我们的直觉，但是巴菲特也投资了很多苹果股票啊，怎么这里并没有看到很高的相关性呢，是不是可以思考一下。。 总方差计算为0.601009311705， 相对之前有大幅下降 可见如果加入了BRK.B 是会大幅降低我们当前的持仓风险，但是是否就是意味着我们要买进BRK.B 而不是特斯拉呢，这又到了投资永恒的话题，风险和收益是正相关的，承担多少风险就会有可能有多大的收益（或亏损）。但是用一些简单的量化方法来分析至少让你心中有数，不会盲目的投奔于风险之中而不自知。 本文代码 https://github.com/niuguy/blog/blob/master/ofeng/source/codes/cov-analysis.ipynb 参考文献 https://www.quantopian.com/lectures/linear-correlation-analysis https://www.quantopian.com/lectures/position-concentration-risk","categories":[{"name":"Quant","slug":"Quant","permalink":"http://yoursite.com/categories/Quant/"}],"tags":[]},{"title":"The guide to predictive data analysis on MIMIC","slug":"The-guide-to-predictive-data-analysis-on-MIMIC","date":"2020-05-30T07:40:46.000Z","updated":"2020-08-30T08:13:05.971Z","comments":true,"path":"2020/05/30/The-guide-to-predictive-data-analysis-on-MIMIC/","link":"","permalink":"http://yoursite.com/2020/05/30/The-guide-to-predictive-data-analysis-on-MIMIC/","excerpt":"","text":"Below is my teaching materials when I served as assistant teacher on Digital Health and data analysis at Reading University. The dataset I used for this course is MIMIC. If you are interested in healthcare data analysis, this might be what you want to check. 1. SQL basics 2. Introduction to Python 3. Numpy and Pandas 4. How to do feature engineering for mortality prediction 5. Introduction to machine learning methods 6. Explaination analysis on model results 7. Introduction to unsupervisd learning References","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Health","slug":"Health","permalink":"http://yoursite.com/tags/Health/"}]},{"title":"Kaggle竞赛指南 —— 问题求解套路","slug":"How-to-Kaggle-5","date":"2019-11-28T07:26:52.000Z","updated":"2020-08-30T08:59:09.245Z","comments":true,"path":"2019/11/28/How-to-Kaggle-5/","link":"","permalink":"http://yoursite.com/2019/11/28/How-to-Kaggle-5/","excerpt":"","text":"顶级的Kaggler都有自己的一套问题解决流程，熟悉这些套路能极大提高求解问题的效率。今天我们来看一下目前Kaggle总排名第四的kazanova总结出来的一套解题套路。 具体如下： 下面分别看一下各阶段主要做什么事情： 理解问题 这是个什么问题。分类还是回归，预测还是优化，需要有一个直观理解。 数据特征。图像数据，文本数据还是声音数据，数据集是表格数据么，是否是与时间相关的。 数据规模。从而可能需要准备什么样的软硬件，需不需要用GPU， 大概需要用多少内存以及硬盘。 探索性数据分析 画出每个变量的直方图，看一下训练集及测试集的分布相似程度， 画出特征变量相对于目标变量的关系图，特征变量相对于时间的变化图（如果是时间序列问题） 通过看数据分布可以导出相应的交叉验证策略。 制定交叉验证（CV）策略 在对数据充分理解基础上，要确定验证策略，这一步非常重要，实际上很多排名靠前的方案最主要就是因为找到了正确的CV策略。可以从以下几点思考： 时间是否是个重要的变量。如果是需要根据时间做训练集测试集的分割，且要注意要用历史数据来预测未来数据。 如果数据分布是分层的，是否要考虑用分层验证（Stratified Validation） 如果数据是随机分布的，是否考虑采用随机验证，如随机K折验证 特征工程 特征工程做的好坏在大多数竞赛问题中起到了决定性的作用，面对不同问题需要采取不同策略，具体思考方向如下。 图像识别：考虑正规化（scaling），平移(shifting)，翻转(rotation)以及CNN。 参考https://www.kaggle.com/c/data-science-bowl-2018 声音识别：傅立叶变换（Fourier），mfcc, 波普分析（specgram），正规化。 参考https://www.kaggle.com/c/tensorflow-speech-recognition-challenge 文本分类：Tf-idf, svd, Stemming, 拼写检查（spell checking）, stop words’ removal, x-grams 。参考https://www.kaggle.com/c/stumbleupon/overview 时间序列： 时间延迟（Lags）,加权平均（weighted average）, 指数平滑（expotional smoothing） 。参考https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting 类别数据：目标特征，频率，one-hot， 顺序，label encoding。 参考https://www.kaggle.com/c/amazon-employee-access-challenge 数字数据：正规化，绑定（binning）, 求导（derivatives），离群值（outlier remove）,降纬。参考 https://www.kaggle.com/c/afsis-soil-properties/overview 交互数据：相乘，相除，相加，groupby。 参照https://www.kaggle.com/c/homesite-quote-conversion 推荐系统: 历史交易数据，货品流行程度，购买频度。参考https://www.kaggle.com/c/acquire-valued-shoppers-challenge 方法建模 类似于特征工程， 根据不同问题有不同的建模策略 图像识别：CNN(ResNet, VGG) 声音识别：CNNs(CRNN), LSTM 文本分类：GBM, Linear, DL, Naive Bayes, KNNs, LibFM, Libffm 时间序列： Augtoregressive, ARIMA, Linear, GBMs, DL, LSTMs 类别数据：GBMs, Linear, DL, LibFM, Libffm 数字数据：GBMs, Linear, DL, SVMs 交互数据：GBMs, Linear, DL 推荐系统: CF, DL, LibFM, Libffm, GBMs 应用中每种方法独立运行评估， 在不同的数据集上评估，可考虑做bagging. 集成学习 这个阶段邻近竞赛结束，此时已经保存了基于不同方法的预测结果，可根据这些结果做有机整合争取最优化预测模型并提交。有多种集成学习策略，比如针对小数据集可以考虑简单的平均化（averaging）， 对于大规模数据集可以考虑做stacking。","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Kaggle竞赛指南 —— 探索性数据分析","slug":"How-to-Kaggle-4","date":"2019-11-02T07:26:52.000Z","updated":"2020-08-30T08:58:30.233Z","comments":true,"path":"2019/11/02/How-to-Kaggle-4/","link":"","permalink":"http://yoursite.com/2019/11/02/How-to-Kaggle-4/","excerpt":"","text":"之前说到在模型选择方面没有银弹，那么如何确定最合适的模型提出假设，探索性数据分析（EDA）是一个必不可少的环节。 为什么要做EDA 更好的理解数据。数据主要是什么类型，数据量有多少，大概分布如何。。 构建模型直觉。根据对数据的了解可以构建对于模型的直觉，这可能需要一些经验。 提出模型假设。当你对数据有基本的了解以及直觉之后就可以预设从那个范围选取模型，线性、决策树、聚类还是NN。 分析什么 这里我们以iris数据集为例 12import pandas as pdiris &#x3D; pd.read_csv(&#39;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;mwaskom&#x2F;seaborn-data&#x2F;master&#x2F;iris.csv&#39;) 数据取样 随机获取5个样例，可以看到有多少列，每列的数据类型。类似的你也可以用.head()或者.tail()方法， 得到的分别是数据集的头部和尾部数据。 概要统计信息 主要用到的是Pandas的describe() 这个方法输出非常丰富，包括数据总量、平均数、方差、最小最大值以及各种分位数。通过这个结果可以对数据大概分布有一个了解 可视化 数据可视化在EDA过程中非常重要，常常可以通过可视化过程发现可能的建模方向。可视化主要有两个目的，一是探索单个特征的分布规律 ， 二是探索特征之间的相互关系。 在单特征探索用到的主要工具有直方图、散点图 直方图 直方图将单个特征值的数据划分为不同的数据区段，可以直观看到数据在各个区段的分布情况。需要注意的是，直方图是一种聚合方法，我们无法看到一个数据区间里面的数据分布情况，这有时候可能造成判断偏差。 在存在数据缺失或有异常值的情况这种误差可能尤其明显，这时候可能需要做一些预处理，比如填补缺失值或者取log等操作。 点图 以index为x轴， 各特征值为y值画散点图，可以看到数据随index变化情况，用以检查数据有没有shuffle, 比如上图petal_length是均匀分布的，petal_width就呈现出阶段性特点。 在多特征相互作用探索用到的主要工具有散点图和相关矩阵图 散点图 可以看两个特征之间的相互关系。值得注意的是，有时散点图会 矩阵图 矩阵图囊括了散点图和直方图，是Pandas提供的一种非常方便的可视化工具，但是需要注意的是对于大型数据集渲染速度可能有些慢 写在最后 EDA 由于其探索方式多种多样，其本身可以称之为一种艺术，其效果好坏来自于你对数据的直觉以及对于各种工具技巧的熟悉程度，想要提高这方面的能力需要不断的练习并且学习各种先进的思路。","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Kaggle竞赛指南 —— 数据预处理","slug":"How-to-Kaggle-3","date":"2019-10-27T18:26:35.000Z","updated":"2020-08-30T08:58:35.500Z","comments":true,"path":"2019/10/27/How-to-Kaggle-3/","link":"","permalink":"http://yoursite.com/2019/10/27/How-to-Kaggle-3/","excerpt":"","text":"数据预预处理对于机器学习结果来说是至关重要，有时甚至是决定性的。本篇我们将讨论针对不同的数据类型，如何根据不同的数据模型来做预处理。具体我们将讨论四种最常见类型的数据，分别是 数字数据，类别数据，时间数据和坐标数据。 数字数据 标准化（Scaling） 为什么要标准化？ 非标准化的数据会造成计算数据之间距离时，在一个数据纬度上的距离被过度放大，可能会极大的影响分类的准确性。 如下图，如果我们采用k-NN计算节点间距离，左右两个x1坐标系的尺度不同直接影响了绿色节点的分类结果。 标准化有四种常规实现方式，对应于Sklearn 以及Numpy中的实现，分别是MinMaxScaler， StandardScaler MinMaxScaler 转化数据到[0, 1]区间 X = (X-X.min())/(X.max() - X.min()) 具体可使用 sklearn.preprocessing.MinMaxScaler StandardScaler 转化数据到 平均值=0， 方差=1 X = (X-X.min())/X.std() 具体可使用sklearn.preprocessing.StandardScaler 是否所有模型都需要标准化？ 答案是否定的，实际上决策树相关模型就不需要进行数值标准化，因为树模型不用考虑节点之间的相互距离。 离群值（Outlier）与排序 有时一个数据集中可能有几个数据点偏离其他数据点太远，这可能会极大影响分类效果，如下图所示： 这时候我们需要把离群点找出来剔除或单独处理，一种最简单可行的方法是对数据点进行排序，并用排序后的index代替原数值，这种方式把数据节点之间的距离尺度拉平到1个单位。 比如 12rank([-100, 0, 1e5]) &#x3D;&#x3D;[0,1,2]rank([1000,1,10]) &#x3D; [2,0,1] sklearn.stats.RankData可以实现上述排序+映射的功能 取对数或开平方 对于神经网络模型来说，如果数值太靠近0， 可以采取对数或开平方的方式来使数据变得更加可区分，借助于Numpy 比如： np.log(1 + x) np.sqrt(1+x) 创造新的特征值 有时候你不必局限于已有数据的特征，可以创造一些新的特征值来使数据的表达更为丰富，在部分竞赛题目中，新特征值的构建对结果也产生了至关重要的作用 例如房地产数据，假设你已经有房子面积和总价数据，你可以很自然的想到房屋单价这一可能的新特征。有时候新特征可能需要一些统计知识，比如时间序列数据中每个窗口数据，可以添加其均值方差等新特征。总体来说，对于新特征的挖掘能力取决于两方面：一是领域知识的了解程度；二是通过探索性数据分析（EDA）所获得的发现 类别数据 类别数据，又称可枚举数据，是一种广泛存在的数据类型，比如性别，职业，学历等。 序数词是指一个序列中的一部分数据，比如考试名词。这些数据需要转换成数值数据以供模型使用。典型的处理方式有适用于决策树相关模型的LabelEncoding 和FrequencyEncoding ， 以及适用于线性及其他非决策树模型的OneHot Encoding LabelEncoding 将类别或序数词直接映射为数值数据，具体有两种映射方式，一是按照字母顺序，二是按照出现顺序，比如下面这组数据：[S, C, S, S, S Q] 按照字母顺序映射结果 为[2, 1, 2, 2, 2, 3] 按照 出现顺序则为[1, 2, 1, 1, 1, 3] 分别采用的软件包为 sklearn.preprocessing.LabelEncoder 和 Pandas.factorize Frequency Encoding FrequencyEncoding 是指计算每个类别的出现频率，按照上述例子计算 那么我们有P(S) = 4/6 P(C) = 1/6 P(Q)=1/6 One-hot Encoding 对于非决策树模型来说，One-Hot Encoding 是一个更合适的预处理方法，其具体处理过程如下所示： One-Hot 将一维特征拓展为N维（N=类别总数）， 每行对应的列为1，其他为0。 由于处理后的数值只能为0或1，所以OneHot的过程也同时是数据标准化的过程。 One-Hot一个潜在的缺点是，如果类别数过多会生成一个庞大的稀疏矩阵，使计算效率大打折扣，这个时候可以考虑用其他方式比如广泛用于自然语言处理的Word2Vec。 时间和地理坐标数据 时间和地理坐标数据有一些相似点是，时间和地理坐标本身是一种标尺不是数据，但是我们可以根据时间和地理坐标的特性生成新的数字或者类别特征。 时间数据 时间数据可以从三个方面考虑其新特征： 周期性 秒、分、时、星期、月、年。例如病人服药记录，一种药需要三天吃一次，可能就需要3天为一个时间周期来计算相关特征。 距离某个时间距离 分析电商销售数据比较常用的是计算比如开店后多少星期，从上一个节假日之后多少天的销售额。 时间差 两个时间点之间的数据差也可能成为新的特征值，比如股市每日收盘和开盘之间的价格变化。 地理坐标数据 类似于时间数据，地理坐标数据也可以产生很多有用的特征，假设要预测一个地区的房价信息，在给定了一个城市比如伦敦 我们可以从几个方向去探索： 聚类中心点 可以尝试依据房价的经纬度进行聚类分析，分析发现关键性的地标中心，比如伦敦塔、海德公园、大英博物馆等 。 距离信息 可计算预测的地点到中心点的距离值 聚合分析 在预测点周围比如1英里、3英里 半径范围内房子的类型价格、教育设施、生活设施等统计信息。","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Kaggle竞赛指南 —— 主流机器学习算法","slug":"How-to-Kaggle-2","date":"2019-08-27T17:08:51.000Z","updated":"2020-08-30T08:58:42.388Z","comments":true,"path":"2019/08/27/How-to-Kaggle-2/","link":"","permalink":"http://yoursite.com/2019/08/27/How-to-Kaggle-2/","excerpt":"","text":"目前竞赛中（其实也是常规实际问题）的主流算法有四大门类： Linear, Tree-based, kNN 以及 Neural Networks 下面分别简单介绍一下： Linear相关模型 假设我们有两组点， 灰色点属于一类，绿色点属于另一类，如上图所示， 那么我们可以很方便的拟合条直线将之区分为两类，落在右上方的为A 落在左下方的为B。 上述是两维的情况， 这种思路还可可以拓展到高维空间， 这个时候分界展现形式就可能是一个平面而非直线了， 具体算法比如Logistic Regression 以及SVM。 Linear相关模型非常适用于稀疏高维数据的分类。不过也有很多数据没有那么容易用Linear区分，比如下面这种“环”数据： 在应用过程中你不必从头实现Linear算法，各种现成的库可以在Scikit-learn或者 Vowpal Wabbit找到， 后者专门用于处理大型数据集。 Tree-based模型 Tree-based(决策树)模型通常是作为构建复杂模型的一个基础模型。类似于线性模型，我们假设有两组数据点，我们尝试用一条平行于坐标轴的直线将之分开 现在我们得到了两片区域，上面这片区域是灰色点的概率为1， 下面这片是灰色的概率为0.2 。 那么上面空间不用再拆分了，我们继续画一条直线拆分下面这片区域 这样我们得到左下区域的灰色概率为0， 剩余区域灰色概率为1。这个过程就是决策树工作的典型过程， 简单来讲就是使用分治的策略不断划分子空间直到不可再划分为止。 决策树算法的策略简单但是非常有效，其有两个最有名的变形分别是随机森林（Random Forest）和梯度提升决策树（Gradient Boost Decision Trees, GBDT）。决策树及其变形在Kaggle竞赛中应用极为广泛，不夸张的说几乎所有表格数据相关的竞赛其冠军方案都使用了这种策略。 不过决策树也不是万能的，在很多有明显线性依赖关系的数据集上，决策树表现要比线性模型要差很多，因为为了分割空间其需要生成大量树分岔，而且在决策边界也可能会变得不准确。 对于基本的决策树和Random Forest，我们依然可以在Scikit-learn或者 Vowpal Wabbit中找到其实现，对于GBDT有两个主流的实现分别是 XGBoost和LightGBM k-NN k-NN 是 k-Nearest 即K 邻近算法的缩写，我们还是从分类预测问题入手，类似的，我们希望预测下图中带问好的这个点的分类 我们做一个假设， 即彼此接近的点可能会有相同的标签，因此我们希望能找到距离目标点相近的点，并把这个点的标签作为目标点的标签，因此某个点属于那个分类是由其邻居“投票”决定的。这个寻找过程就是邻近算法的基本思路。如果我们再拓展到k 个目标找其邻近的点，那么这个算法就会产出k个分类, 也就是k-NN算法。 尽管K-NN思路简单，但是却是竞赛中非常常用的一种前期处理思路。类似于前面两个模型，k-NN的实现也可以在Scikit-learn或者 Vowpal Wabbit中找到。 Neural Networks(NN) NN是近几年耀眼的明星，这其实不是一个算法，而是一类特殊的机器学习模型，直观来讲，NN 类似于一个黑盒，与决策树不同，NN 可以生成一个平滑的分割曲线，在一些特定问题如图像、声音、文本和序列上，NN都取得了革命性的进步。NN的具体原理是需要专门系统学习的，但是你可以在 https://playground.tensorflow.org/获得一个直观的认识 实现NN的软件包非常多，主流的有Tensorflow, Keras, Pytorch, Mxnet 。其中Keras是对Tensorflow的易用化封装， Pytorch近来由于其定义网络的灵活性受到越来越多资深Kaggler的青睐。 One more thing Here is no method which outperforms all others for all tasks 没有银弹 没有一种方法可以适用于所有数据，每种方法都限定与其适用的数据类型假设。通常我们看到大多数竞赛题目上，GBDT 和 NN 方法都表现良好，但并不能因此低估其他比如Linear和K-NN方法，在某些问题上后者可能表现更好,比如这个结合了逻辑回归和随机森林算法的方案，得分超过了CNN获得了第一（http://blog.kaggle.com/2017/03/24/leaf-classification-competition-1st-place-winners-interview-ivan-sosnovik/ ）","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Kaggle竞赛指南 —— 简介","slug":"How-to-kaggle-1","date":"2019-08-26T21:06:54.000Z","updated":"2020-08-30T08:58:49.182Z","comments":true,"path":"2019/08/26/How-to-kaggle-1/","link":"","permalink":"http://yoursite.com/2019/08/26/How-to-kaggle-1/","excerpt":"","text":"一、为什么Kaggle Kaggle是目前最大的数据科学竞赛与技能分享平台。在Kaggle上你可以查找和发布数据集，探索和构建模型，与其他数据科学家和机器学习工程师合作，并参加竞赛以解决数据科学挑战。参加Kaggle竞赛，你至少可以有以下几种收获： 赢得奖金。奖金从1000-100000美金不等，只要能在参赛队伍中获取前三名即可分享。 获取影响力以及offer。 比起有限的奖金来说，获得一个好排名似乎是大部分Kaggler的现实目标，一个好的Kaggle名次是入职各大公司的敲门金砖。 学习经验。得益于数据科学家们对结果的极致追求，在Kaggle上你可以看到最前端的数据科学技巧，学习到比论文中更加实用的方法。 二、竞赛相关概念 1. 数据（Data） 这个板块上提供可下载的竞赛数据以及数据描述，小规模的数据可以在线预览。注意，有时你也可以用外部公开数据集来增强你的模型，但是要注意查看竞赛规则。 2. 模型（Model） 提起模型，我们脑中浮现的似乎是一个个算法，但实际上你应该更广泛的认识它，从广义上来说，模型是一种 从数据到答案的解决方法。 模型可能异常复杂，用到各种算法的叠加，各种特征处理方法（包括手工），以及各种工具。比如下面这个来自一次竞赛的冠军解决方案： 会有一些Kaggler把他们的方案以Jupeter Notebook的形式发布在【Kernels】板块，这是一个学习各种先进思路的好地方（不局限于竞赛）。 3. 提交（Submission） 你需要将你的模型产出的预测结果提交给平台获取评分， 提交文件通常是类似于csv的文件： 4. 评估（Evaluation） 你需要了解一下常用的评估方法，比如： Accuracy Logistic Loss AUC RMSE MAE 5.排行榜(Leaderboard) 关于排行榜，你需要了解的是，在竞赛截止前你所看到的竞赛分数是基于公开发布的数据进行评估的。竞赛截止后最终计分时会用未公开数据进行评估。所以有很多时候你看到在截止前一直排名前列的选手，最终计分时落差很大，那可能是他（她）在公共数据上过拟合了。 三、 硬件准备 可以解决大多数竞赛问题的配置（除去图像处理外）： 16G 内存 4 核CPU 更好一点的配置： 32 G 内存 6核CPU 几个关键概念： 内存： 如果你能把所有数据都装入内存，将极大提高处理效率 芯片核数： 越多核，就可以同时做越多的实验 存储： 如果处理大型数据集或者图片数据库，SSD 硬盘至关重要 当然你可以选择用各种云平台，常见的有 Amazon AWS Microsoft Azure Google Cloud 四、软件准备 语言：Python （前两年你可能还在纠结于R和Python之间，但现在请直接选Python) 技术栈 IDE 三方包 外部工具 关于软件安装，你可以用pip一个个安装，也可以直接使用Anaconda大礼包，里面已经包含了大部分常用工具。 Next…","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"}]},{"title":"Stochastic gradient descent","slug":"Stochastic-gradient-descent","date":"2018-11-08T11:14:28.000Z","updated":"2020-08-31T09:38:13.783Z","comments":true,"path":"2018/11/08/Stochastic-gradient-descent/","link":"","permalink":"http://yoursite.com/2018/11/08/Stochastic-gradient-descent/","excerpt":"","text":"Stochastic Gradient decent is one of the fundamental algorithm in deep learning. It is used when we perform optimization of the cost function.Suppose the function is $ f(x) $ As what illustrated above, we want to approach the minimum value of f(x) which is the point C. If we are now at point A, the derivation of A is f’(x)&gt;0, so we need to go right down which is the opposite direction of f’(x). If we stand at B,the derivation of A is f’(x)&gt;0, we should go left down which is also the opposite direction of f’(x). According to the optimization strategy , we should update the parameter like this: x=x−ηf′(x)x = x - \\eta f&#x27;(x) x=x−ηf′(x) η\\etaη is so-called the learning rate, it determines the size of steps we take to reach a minimum value of f(x)f(x)f(x) Batch Gradient Follow the rules of gradient descent, when we perform one update of parameters the intuitive strategy is to calculate the gradients for all the examples in the dataset and sum them up, this computation process is also called batch gradient. The pseudocode described above looks like this. 123for i in range(epoches): param_grad = calculate_gradient(loss_function, [examples]) param = param - learning_rate * params_grad For each update we need to walk through all these examples, if there are millions of them, the computation cost would be a problem, let alone there usually are several epochs of updates. Stochastic Gradient The inefficiency of the batch gradient leads to another strategy that is widely used in deep learning, which is stochastic gradient descent(SGD). Instead of updating the params based on the gradients of all examples, SGD updates the param after computing the gradient of one example, what particular is the dataset should be shuffled in each epoch. The poseudocode of SGC looks like this: 12345for i in range(epochs): np.random.shuffle(data) for example in data: param_grad = calculate_gradient(loss_function , example) param = param - learning_rate * param_grad The algorithm is not as complicated as its name implies. Actually, it’s too simple that we may doubt about its effectiveness in convergence. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, the convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. A simple improvement of SGD is to perform an update for every mini-batch examples, which is called mini-batch gradient. It is a compromise between batch gradient and SGD and is proved effective in practices. The code looks like this: 12345for i in range(epochs): np.random.shuffle(data) for batch in batches(data, batch_size = 50): param_grad = calculate_gradient(loss_function , batch) param = param - learning_rate * param_grad","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"}]},{"title":"Backward propagation of Neural Network explained","slug":"Backward-propagation-of-Neural-Network-explained","date":"2018-11-08T11:14:28.000Z","updated":"2020-08-31T09:35:37.833Z","comments":true,"path":"2018/11/08/Backward-propagation-of-Neural-Network-explained/","link":"","permalink":"http://yoursite.com/2018/11/08/Backward-propagation-of-Neural-Network-explained/","excerpt":"","text":"Backpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of ‘dark magic’ we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way. Outline Overview of the architecture Initialize parameters Implement forward propagation Compute Loss Implement Backward propagation Update parameters 1. The architecture This neural network I’m going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. The architecture in the math formula f(X)=Relu(Sigmoid(X×Wih+b1)×Who+b2)f(X)= Relu( Sigmoid( X \\times W_{ih} +b_{1}) \\times W_{ho} + b_{2}) f(X)=Relu(Sigmoid(X×Wih​+b1​)×Who​+b2​) ##2.Initialize parameters We take one example which has two features like below X=[x1,x2]=[0.1,0.2]X= [x1, x2]= [0.1, 0.2] X=[x1,x2]=[0.1,0.2] The parameters are taken randomly. Wis=[Wi1h1Wi1h2Wi2h1Wi2h2]=[0.70.60.50.4]W_{is}= \\begin{bmatrix} W_{i1h1} &amp; W_{i1h2}\\\\ W_{i2h1} &amp; W_{i2h2} \\end{bmatrix} = \\begin{bmatrix} 0.7 &amp; 0.6\\\\ 0.5 &amp; 0.4 \\end{bmatrix} Wis​=[Wi1h1​Wi2h1​​Wi1h2​Wi2h2​​]=[0.70.5​0.60.4​] Wsr=[Wh1o1Wh1o2Wh2o1Wh2o2]=[0.40.60.90.8]W_{sr}= \\begin{bmatrix} W_{h1o1}&amp;W_{h1o2}\\\\ W_{h2o1}&amp;W_{h2o2} \\end{bmatrix} = \\begin{bmatrix} 0.4&amp;0.6\\\\ 0.9&amp;0.8 \\end{bmatrix} Wsr​=[Wh1o1​Wh2o1​​Wh1o2​Wh2o2​​]=[0.40.9​0.60.8​] b1=[b11,b12]=[0.5,0.6]b_{1}=[b_{11}, b_{12}] =[0.5,0.6] b1​=[b11​,b12​]=[0.5,0.6] b2=[b21,b22]=[0.7,0.9]b_{2}= [b_{21}, b_{22}] = [0.7, 0.9] b2​=[b21​,b22​]=[0.7,0.9] 3. Forward Propagation 3.1 Layer1: Linear X×Wih+b1=[x1,x2]×[Wi1h1Wi1h2Wi2h1Wi2h2]+[b11,b12]=[0.1,0.2]×[0.70.60.50.4]+[0.5,0.6]=[0.67,0.74]X \\times W_{ih} + b_{1} = [x1, x2] \\times \\begin{bmatrix} W_{i1h1} &amp; W_{i1h2}\\\\ W_{i2h1} &amp; W_{i2h2} \\end{bmatrix} + [b_{11}, b_{12}] = [0.1, 0.2] \\times \\begin{bmatrix} 0.7 &amp; 0.6\\\\ 0.5 &amp; 0.4 \\end{bmatrix} + [0.5, 0.6] = [0.67, 0.74] X×Wih​+b1​=[x1,x2]×[Wi1h1​Wi2h1​​Wi1h2​Wi2h2​​]+[b11​,b12​]=[0.1,0.2]×[0.70.5​0.60.4​]+[0.5,0.6]=[0.67,0.74] Sigmoid Sigmoid(x)=1/(1+e−x)Sigmoid(x)=1/(1+e^{-x}) Sigmoid(x)=1/(1+e−x) [hout1,hout2]=Sigmoid(X×Wih+b1)=[Sigmoid(0.67),Sigmoid(0.74)]=[0.6615,0.6770][h_{out1},h_{out2}] = Sigmoid(X \\times W_{ih} + b_{1}) = [Sigmoid(0.67),Sigmoid(0.74)] = [0.6615,0.6770] [hout1​,hout2​]=Sigmoid(X×Wih​+b1​)=[Sigmoid(0.67),Sigmoid(0.74)]=[0.6615,0.6770] 3.2 Layer2: Linear [oin1,oin2]=[hout1,hout2]×Who+b2=[hout1,hout2]×[Wh1o1Wh1o2Wh2o1Wh2o2]+[b21,b22]=[0.6615,0.6770]×[0.70.50.60.4]+[0.5,0.6]=[1.3693,1.0391][o_{in1}, o_{in2}] = [h_{out1}, h_{out2}] \\times W_{ho} + b_{2} = [h_{out1}, h_{out2}] \\times \\begin{bmatrix} W_{h1o1} &amp; W_{h1o2}\\\\ W_{h2o1} &amp; W_{h2o2} \\end{bmatrix} + [b_{21}, b_{22}] =\\\\ [0.6615, 0.6770] \\times \\begin{bmatrix} 0.7 &amp; 0.5\\\\ 0.6 &amp; 0.4 \\end{bmatrix} + [0.5, 0.6] = [1.3693, 1.0391] [oin1​,oin2​]=[hout1​,hout2​]×Who​+b2​=[hout1​,hout2​]×[Wh1o1​Wh2o1​​Wh1o2​Wh2o2​​]+[b21​,b22​]=[0.6615,0.6770]×[0.70.6​0.50.4​]+[0.5,0.6]=[1.3693,1.0391] Softmax Softmax(x)j=exj∑i=1nexi(j=1,2...n)Softmax(x)_{j}= \\frac{e^{xj}}{\\sum_{i=1}^n e^{xi}} (j=1,2...n) Softmax(x)j​=∑i=1n​exiexj​(j=1,2...n) [oout1,oout2]=Softmax(X×Who+b2)=[e1.3693e1.3693+e1.0391,e1.0391e1.3693+e1.0391]=[0.5818,0.4182][o_{out1},o_{out2}] = Softmax(X\\times W_{ho} + b_{2})= [ \\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} , \\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}} ] = [0.5818,0.4182] [oout1​,oout2​]=Softmax(X×Who​+b2​)=[e1.3693+e1.0391e1.3693​,e1.3693+e1.0391e1.0391​]=[0.5818,0.4182] ##4. Compute Loss The Loss function here we use is cross-entropy cost Crossentropy=−1m∑i=1m(y(i)log⁡(a[L](i))+(1−y(i))log⁡(1−a[L](i)))Crossentropy= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) Crossentropy=−m1​i=1∑m​(y(i)log(a[L](i))+(1−y(i))log(1−a[L](i))) The actual output should be [y1,y2]=[1.00,0.00][y_{1},y_{2}] = [1.00, 0.00] [y1​,y2​]=[1.00,0.00] Since we only have one example, that means ‘m = 1’, the total loss is computed as follows : −11∑i=11(y1log⁡(oout1)+0+0+1∗log(1−oout2))=−(1∗log(0.5818)+0+0+1∗log(1−0.4182))==0.4704-\\frac{1}{1} \\sum\\limits_{i = 1}^{1} (y_{1}\\log\\left(o_{out1}\\right) + 0 +0 + 1*log(1-o_{out2}))= -(1*log(0.5818)+0+0+1*log(1-0.4182))==0.4704 −11​i=1∑1​(y1​log(oout1​)+0+0+1∗log(1−oout2​))=−(1∗log(0.5818)+0+0+1∗log(1−0.4182))==0.4704 5. Backward Propagation In this section, we will go through backward propagation stage by stage. 5.1 Basic Derivatives ####Sigmoid: ∂Sigmoid(x)∂x=∂1(1+e−x)∂x=e−x(1+e−x)2=(1+e−x−11+e−x)11+e−x=(1−Sigmoid(x))×Sigmoid(x)\\frac{\\partial Sigmoid(x)}{\\partial x} = \\frac{\\partial \\frac{1}{(1+e^{-x})}}{\\partial x} = \\frac{ e^{-x}}{(1+e^{-x})^2} = (\\frac{1+e^{-x}-1}{1+e^{-x}})\\frac{1}{1+e^{-x}} = (1 - Sigmoid(x))\\times Sigmoid(x) ∂x∂Sigmoid(x)​=∂x∂(1+e−x)1​​=(1+e−x)2e−x​=(1+e−x1+e−x−1​)1+e−x1​=(1−Sigmoid(x))×Sigmoid(x) ####Softmax: At first we know: For f(x)=g(x)h(x)f(x)=\\frac{g(x)}{h(x)} f(x)=h(x)g(x)​ f′(x)=g′(x)h(x)−g(x)h′(x)[h(x)]2f&#x27;(x) = \\frac{g&#x27;(x)h(x)-g(x)h&#x27;(x)}{[h(x)]^2} f′(x)=[h(x)]2g′(x)h(x)−g(x)h′(x)​ Then the derivation of Softmax is ∂Softmax(x)∂x1=ex1(ex1+ex2)−ex1ex1(ex1+ex2)2=ex1+x2(ex1+ex2)2\\frac{\\partial Softmax(x)}{\\partial x1}=\\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \\frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2} ∂x1∂Softmax(x)​=(ex1+ex2)2ex1(ex1+ex2)−ex1ex1​=(ex1+ex2)2ex1+x2​ ###5.2 The backward Pass 5.2.1 Layer1-Layer2 ####Weight derivatives with respect to the error Consider Who , we want to know how Who will affect the total error, aka the value of ∂Etotal∂Who\\frac{\\partial E_{total}}{\\partial W_{ho}} ∂Who​∂Etotal​​ Chain Rule states that: ∂z∂x=∂z∂y⋅∂y∂x\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x} ∂x∂z​=∂y∂z​⋅∂x∂y​ So we have ∂Etotal∂Wh2o1=∂Etotal∂oout1⋅∂oout1∂oint1⋅∂oint1∂Wh2o1\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= \\frac{\\partial E_{total}}{\\partial o_{out1}} \\cdot \\frac{\\partial o_{out1}}{\\partial o_{int1}} \\cdot \\frac{\\partial o_{int1}}{\\partial W_{h2o1}} ∂Wh2o1​∂Etotal​​=∂oout1​∂Etotal​​⋅∂oint1​∂oout1​​⋅∂Wh2o1​∂oint1​​ Let’s break this through stage by stage Stage1 ∂Etotal∂oout1=∂(−(y1∗log(oout1)+(1−y1)∗log(1−oout1)))∂oout1+0=−1oout1=−1/0.5818=−1.719\\frac{\\partial E_{total}}{\\partial o_{out1}} = \\frac{\\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\\partial o_{out1}}+0 =-\\frac{1}{o_{out1}}=-1/0.5818=-1.719 ∂oout1​∂Etotal​​=∂oout1​∂(−(y1​∗log(oout1​)+(1−y1​)∗log(1−oout1​)))​+0=−oout1​1​=−1/0.5818=−1.719 Stage2 ∂o1∂i1=∂Softmax(i1)∂i1=ei1ei2(ei1+ei2)2=ei1ei2(ei1+ei2)2=e1.3693⋅e1.0391(e1.3693+e1.0391)2=0.2433\\frac{\\partial o_{1}}{\\partial i_{1}}= \\frac{\\partial Softmax(i_{1})}{\\partial i_{1}} = \\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2} = \\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2} = \\frac{e^{1.3693}\\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433 ∂i1​∂o1​​=∂i1​∂Softmax(i1​)​=(ei1​+ei2​)2ei1​ei2​​=(ei1​+ei2​)2ei1​ei2​​=(e1.3693+e1.0391)2e1.3693⋅e1.0391​=0.2433 Stage3 ∂oin1∂Wh2o1=∂(hout1∗Wh1o1+hout2∗Wh2o1+b21)∂Wh2o1=hout2=0.6770\\frac{\\partial o_{in1}}{\\partial W_{h2o1}} = \\frac{\\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\\partial W_{h2o1}} = h_{out2}=0.6770 ∂Wh2o1​∂oin1​​=∂Wh2o1​∂(hout1​∗Wh1o1​+hout2​∗Wh2o1​+b21​)​=hout2​=0.6770 Finally we apply the chain rule: ∂Etotal∂Wh2o1=−1.719∗0.2433∗0.677=−0.2831\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831 ∂Wh2o1​∂Etotal​​=−1.719∗0.2433∗0.677=−0.2831 Let’s go through all the weights in Layer2 Who′=[Wh1o1′Wh1o2′Wh2o1′Wh2o2′]=[∂Etotal∂Wh1o1∂Etotal∂Wh1o2∂Etotal∂Wh2o1∂Etotal∂Wh2o2]=[∂Etotal∂oout1⋅∂oout1∂oin1⋅∂oin1∂Wh1o1∂Etotal∂oout2⋅∂oout2∂oin2⋅∂oin2∂Wh1o2∂Etotal∂oout1⋅∂oout1∂oin1⋅∂oin1∂Wh2o1∂Etotal∂oout2⋅∂oout2∂oin2⋅∂oin2∂Wh2o2]=[−1.719∗0.2433∗0.6615−2.3912∗0.2433∗0.6615−1.719∗0.2433∗0.6770−2.3912∗0.2433∗0.6770]=[−0.2767−0.3733−0.2831−0.3853]W_{ho}&#x27;= \\begin{bmatrix} W_{h1o1}&#x27; &amp; W_{h1o2}&#x27;\\\\ W_{h2o1}&#x27; &amp; W_{h2o2}&#x27; \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial E_{total}}{\\partial W_{h1o1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{h1o2}} \\\\ \\frac{\\partial E_{total}}{\\partial W_{h2o1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{h2o2}} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial E_{total}}{\\partial o_{out1}} \\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \\cdot \\frac{\\partial o_{in1}}{\\partial W_{h1o1}} &amp; \\frac{\\partial E_{total}}{\\partial o_{out2}} \\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \\cdot \\frac{\\partial o_{in2}}{\\partial W_{h1o2}} \\\\ \\frac{\\partial E_{total}}{\\partial o_{out1}} \\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \\cdot \\frac{\\partial o_{in1}}{\\partial W_{h2o1}} &amp; \\frac{\\partial E_{total}}{\\partial o_{out2}} \\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \\cdot \\frac{\\partial o_{in2}}{\\partial W_{h2o2}} \\end{bmatrix} \\\\ = \\begin{bmatrix} -1.719*0.2433*0.6615 &amp; -2.3912*0.2433*0.6615 \\\\ -1.719*0.2433*0.6770 &amp; -2.3912*0.2433*0.6770 \\end{bmatrix} = \\begin{bmatrix} -0.2767 &amp; -0.3733\\\\ -0.2831 &amp; -0.3853 \\end{bmatrix} Who′​=[Wh1o1′​Wh2o1′​​Wh1o2′​Wh2o2′​​]=[∂Wh1o1​∂Etotal​​∂Wh2o1​∂Etotal​​​∂Wh1o2​∂Etotal​​∂Wh2o2​∂Etotal​​​]=[∂oout1​∂Etotal​​⋅∂oin1​∂oout1​​⋅∂Wh1o1​∂oin1​​∂oout1​∂Etotal​​⋅∂oin1​∂oout1​​⋅∂Wh2o1​∂oin1​​​∂oout2​∂Etotal​​⋅∂oin2​∂oout2​​⋅∂Wh1o2​∂oin2​​∂oout2​∂Etotal​​⋅∂oin2​∂oout2​​⋅∂Wh2o2​∂oin2​​​]=[−1.719∗0.2433∗0.6615−1.719∗0.2433∗0.6770​−2.3912∗0.2433∗0.6615−2.3912∗0.2433∗0.6770​]=[−0.2767−0.2831​−0.3733−0.3853​] Update weights according to learning rate Our training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02. We got the updated weight matrix as folows Who∗=[Wh1o1−ηWh1o1′Wh1o2−ηWh1o2′Wh2o1−ηWh2o1′Wh2o2−ηWh2o2′]=[0.4−0.02∗(−0.2767)0.6−0.02∗(−0.3733)0.9−0.02∗(−0.2831)0.8−0.02∗(−0.3853)]=[0.40550.60750.90570.8077]W_{ho}^* = \\begin{bmatrix} W_{h1o1} - \\eta W_{h1o1}&#x27; &amp; W_{h1o2} - \\eta W_{h1o2}&#x27;\\\\ W_{h2o1} - \\eta W_{h2o1}&#x27; &amp; W_{h2o2} - \\eta W_{h2o2}&#x27; \\end{bmatrix} = \\begin{bmatrix} 0.4 - 0.02*(-0.2767)&amp;0.6 - 0.02*(-0.3733)\\\\ 0.9 - 0.02*(-0.2831)&amp;0.8 - 0.02*(-0.3853) \\end{bmatrix}\\\\ = \\begin{bmatrix} 0.4055 &amp; 0.6075\\\\ 0.9057 &amp; 0.8077 \\end{bmatrix} Who∗​=[Wh1o1​−ηWh1o1′​Wh2o1​−ηWh2o1′​​Wh1o2​−ηWh1o2′​Wh2o2​−ηWh2o2′​​]=[0.4−0.02∗(−0.2767)0.9−0.02∗(−0.2831)​0.6−0.02∗(−0.3733)0.8−0.02∗(−0.3853)​]=[0.40550.9057​0.60750.8077​] That is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows. ####5.2.2 Layer0(Input Layer) - Layer1 Follow the path of the previous chapter Stage1: ∂hout1∂hin1=∂Sigmoid(hin1)∂hin1=Sigmoid(hin1)∗(1−Sigmoid(hin1))=Sigmoid(0.67)∗(1−Sigmoid(0.67))=0.2239\\frac{\\partial h_{out1}}{\\partial h_{in1}} = \\frac{\\partial Sigmoid(h_{in1})}{\\partial h_{in1}} = Sigmoid(h_{in1})*(1-Sigmoid(h_{in1})) =\\\\ Sigmoid(0.67)*(1-Sigmoid(0.67))=0.2239 ∂hin1​∂hout1​​=∂hin1​∂Sigmoid(hin1​)​=Sigmoid(hin1​)∗(1−Sigmoid(hin1​))=Sigmoid(0.67)∗(1−Sigmoid(0.67))=0.2239 Stage2: ∂hin1∂Wi2h1=∂(x1∗Wi1h1+x2∗Wi2h1+b11)∂Wi2h1=x2=0.2\\frac{\\partial h_{in1}}{\\partial W_{i2h1}} = \\frac{\\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\\partial W_{i2h1}} = x2=0.2 ∂Wi2h1​∂hin1​​=∂Wi2h1​∂(x1∗Wi1h1​+x2∗Wi2h1​+b11)​=x2=0.2 Apply the chain rule: ∂Etotal∂Wi2h1=∂Etotal∂hout1⋅∂hout1∂hin1⋅∂hin1∂Wi2h1\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= \\frac{\\partial E_{total}}{\\partial h_{out1}} \\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}} ∂Wi2h1​∂Etotal​​=∂hout1​∂Etotal​​⋅∂hin1​∂hout1​​⋅∂Wi2h1​∂hin1​​ We already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction. ∂Etotal∂hout1=∂Etotal∂oout1∂oout1∂oin1∂oin1∂h1out\\frac{\\partial E_{total}}{\\partial h_{out1}} = \\frac{\\partial E_{total}}{\\partial o_{out1}} \\frac{\\partial o_{out1}}{\\partial o_{in1}} \\frac{\\partial o_{in1}}{\\partial h1_{out}} ∂hout1​∂Etotal​​=∂oout1​∂Etotal​​∂oin1​∂oout1​​∂h1out​∂oin1​​ We have computed the first and second results, and the third one is merely a deviation of the linear function ∂Etotal∂hout1=−1.719∗0.2433∗∂(hout1∗Wh1o1+hout2∗Wh2o1)∂hout1=−1.719∗0.2433∗Wh1o1=−1.719∗0.2433∗0.4=−0.1673\\frac{\\partial E_{total}}{\\partial h_{out1}} = -1.719*0.2433* \\frac{\\partial(h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1})}{\\partial h_{out1}} =\\\\ -1.719*0.2433*W_{h1o1} =-1.719*0.2433*0.4 =-0.1673 ∂hout1​∂Etotal​​=−1.719∗0.2433∗∂hout1​∂(hout1​∗Wh1o1​+hout2​∗Wh2o1​)​=−1.719∗0.2433∗Wh1o1​=−1.719∗0.2433∗0.4=−0.1673 Then we got ∂Etotal∂Wi2h1=−0.1673∗0.2239∗0.2=−0.0075\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= -0.1673*0.2239 * 0.2=-0.0075 ∂Wi2h1​∂Etotal​​=−0.1673∗0.2239∗0.2=−0.0075 Similarly , we can get the Layer0-Layer1 derivatives with respective to the total error Wih′=[∂Etotal∂Wi1h1∂Etotal∂Wi1h2∂Etotal∂Wi2h1∂Etotal∂Wi2h2]=[∂Etotal∂hout1⋅∂h1out∂h1in⋅∂h1in∂Wi1h1∂Etotal∂hout2⋅∂h2out∂h2in⋅∂h2in∂Wi1h2∂Etotal∂hout1⋅∂hout1∂hin1⋅∂hin1∂Wi2h1∂Etotal∂hout2⋅∂hout2∂hin2⋅∂hin2∂Wi2h2]=[−0.1673∗0.2239∗0.1−0.4654∗0.2187∗0.1−0.1673∗0.2239∗0.2−0.4654∗0.2187∗0.2]=[−0.0037−0.0102−0.0075−0.0204]W_{ih}&#x27; = \\begin{bmatrix} \\frac{\\partial E_{total}}{\\partial W_{i1h1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{i1h2}}\\\\ \\frac{\\partial E_{total}}{\\partial W_{i2h1}} &amp; \\frac{\\partial E_{total}}{\\partial W_{i2h2}} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial E_{total}}{\\partial h_{out1}} \\cdot \\frac{\\partial h1_{out}}{\\partial h1_{in}} \\cdot \\frac{\\partial h1_{in}}{\\partial W_{i1h1}} &amp; \\frac{\\partial E_{total}}{\\partial h_{out2}} \\cdot \\frac{\\partial h2_{out}}{\\partial h2_{in}} \\cdot \\frac{\\partial h2_{in}}{\\partial W_{i1h2}} \\\\ \\frac{\\partial E_{total}}{\\partial h_{out1}} \\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}} &amp; \\frac{\\partial E_{total}}{\\partial h_{out2}} \\cdot \\frac{\\partial h_{out2}}{\\partial h_{in2}} \\cdot \\frac{\\partial h_{in2}}{\\partial W_{i2h2}} \\end{bmatrix} \\\\ = \\begin{bmatrix} -0.1673*0.2239 * 0.1 &amp; -0.4654*0.2187*0.1\\\\ -0.1673*0.2239 * 0.2 &amp; -0.4654*0.2187*0.2 \\end{bmatrix} \\\\ = \\begin{bmatrix} -0.0037 &amp; -0.0102\\\\ -0.0075 &amp; -0.0204 \\end{bmatrix} Wih′​=[∂Wi1h1​∂Etotal​​∂Wi2h1​∂Etotal​​​∂Wi1h2​∂Etotal​​∂Wi2h2​∂Etotal​​​]=[∂hout1​∂Etotal​​⋅∂h1in​∂h1out​​⋅∂Wi1h1​∂h1in​​∂hout1​∂Etotal​​⋅∂hin1​∂hout1​​⋅∂Wi2h1​∂hin1​​​∂hout2​∂Etotal​​⋅∂h2in​∂h2out​​⋅∂Wi1h2​∂h2in​​∂hout2​∂Etotal​​⋅∂hin2​∂hout2​​⋅∂Wi2h2​∂hin2​​​]=[−0.1673∗0.2239∗0.1−0.1673∗0.2239∗0.2​−0.4654∗0.2187∗0.1−0.4654∗0.2187∗0.2​]=[−0.0037−0.0075​−0.0102−0.0204​] Update weights according to learning rate Update the weights with learning rate 0.02，we got the final weight matrix Wih∗=[Wi1h1−η(Wi1h1′)Wi1h2−η(Wi1h2′)Wi2h1−η(Wi2h1′)Wi2h2−η(Wi2h2′)]=[(0.7−0.2∗(−0.0037))(0.6−0.2∗(−0.0102))(0.5−0.2∗(−0.0075))(0.4−0.2∗(−0.0204))]=[0.70070.60200.50150.4041]W_{ih}^*= \\begin{bmatrix} W_{i1h1} - \\eta (W_{i1h1}&#x27;) &amp; W_{i1h2} - \\eta (W_{i1h2}&#x27;)\\\\ W_{i2h1} - \\eta (W_{i2h1}&#x27;) &amp; W_{i2h2} - \\eta (W_{i2h2}&#x27;) \\end{bmatrix} = \\begin{bmatrix} (0.7 -0.2*(-0.0037))&amp; (0.6 - 0.2 *(-0.0102)) \\\\ (0.5 - 0.2*(-0.0075)) &amp; (0.4 - 0.2 * (-0.0204)) \\end{bmatrix} \\\\ = \\begin{bmatrix} 0.7007 &amp; 0.6020\\\\ 0.5015 &amp; 0.4041 \\end{bmatrix} Wih∗​=[Wi1h1​−η(Wi1h1′​)Wi2h1​−η(Wi2h1′​)​Wi1h2​−η(Wi1h2′​)Wi2h2​−η(Wi2h2′​)​]=[(0.7−0.2∗(−0.0037))(0.5−0.2∗(−0.0075))​(0.6−0.2∗(−0.0102))(0.4−0.2∗(−0.0204))​]=[0.70070.5015​0.60200.4041​] ###5.3 Wrap up Finally we get all the weights updated Who∗=[0.40550.60750.90570.8077]W_{ho}^* = \\begin{bmatrix} 0.4055 &amp; 0.6075\\\\ 0.9057 &amp; 0.8077 \\end{bmatrix} Who∗​=[0.40550.9057​0.60750.8077​] Wih∗=[0.70070.60200.50150.4041]W_{ih}^*= \\begin{bmatrix} 0.7007 &amp; 0.6020\\\\ 0.5015 &amp; 0.4041 \\end{bmatrix} Wih∗​=[0.70070.5015​0.60200.4041​] 6. Conclusion Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these “smart” gates make it possible for complicated prediction or classification tasks. The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates “vanish” significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice. If we look back to the computing process, a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions. 7. Reference https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b http://cs231n.github.io/optimization-2/ I https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/ https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural Networks and Deep Learning/Building your Deep Neural Network - Step by Step.ipynb","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"}]},{"title":"[Pandas]How to plot counts of each value","slug":"Pandas-How-to-plot-counts-of-each-value","date":"2018-10-22T19:40:47.000Z","updated":"2020-08-25T17:34:07.485Z","comments":true,"path":"2018/10/22/Pandas-How-to-plot-counts-of-each-value/","link":"","permalink":"http://yoursite.com/2018/10/22/Pandas-How-to-plot-counts-of-each-value/","excerpt":"","text":"With pandas build in function(actually matplotlib) 12345import pandas as pd% matplotlib inlinecol_values = (&#x27;x&#x27;, &#x27;x&#x27;, &#x27;y&#x27;, &#x27;y&#x27; , &#x27;y&#x27;, &#x27;z&#x27;)df = pd.DataFrame(&#123;&#x27;col&#x27;:col_values&#125;)df[&#x27;col&#x27;].value_counts().plot.bar() With seaborn 12import seaborn as snssns.countplot(df[&#x27;col&#x27;])","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"[Pandas]How to calculate datetime difference in years","slug":"Pandas-How-to-calculate-datetime-difference-in-years","date":"2018-10-22T15:43:34.000Z","updated":"2020-08-25T17:33:43.942Z","comments":true,"path":"2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/","link":"","permalink":"http://yoursite.com/2018/10/22/Pandas-How-to-calculate-datetime-difference-in-years/","excerpt":"","text":"Suppose you got a person’s regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result. With the help of relativedelta 1234from dateutil import relativedeltafor i in df.index: df.loc[i, &#x27;age&#x27;] = relativedelta.relativedelta(df.loc[i, &#x27;regDate&#x27;], df.loc[i, &#x27;DateOfBirth&#x27;]).years With the help of np.timedelta64 12345import numpy as npdf[&#x27;age&#x27;] = (df[&#x27;regDate&#x27;] - df[&#x27;DateOfBirth&#x27;])/np.timedelta64(1, &#x27;Y&#x27;)# You may need to convert to integerdf[&#x27;age&#x27;].apply(np.int64)","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"[Pandas]How to list all columns","slug":"Pandas-How-to-list-all-columns","date":"2018-10-19T21:31:23.000Z","updated":"2020-08-25T17:34:03.176Z","comments":true,"path":"2018/10/19/Pandas-How-to-list-all-columns/","link":"","permalink":"http://yoursite.com/2018/10/19/Pandas-How-to-list-all-columns/","excerpt":"","text":"1list(dataframe.columns.values)","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"[Pandas] How to import from Sql Server","slug":"Pandas-How-to-import-from-Sql-Server","date":"2018-10-18T19:08:25.000Z","updated":"2020-08-25T17:33:58.378Z","comments":true,"path":"2018/10/18/Pandas-How-to-import-from-Sql-Server/","link":"","permalink":"http://yoursite.com/2018/10/18/Pandas-How-to-import-from-Sql-Server/","excerpt":"","text":"We need to rely on pyodbc, the sample code is as belows. 12import pyodbccnxn = pyodbc.connect(&#x27;DRIVER=&#123;ODBC Driver 13 for SQL Server&#125;;SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes&#x27;) Write SQL and execute with pandas.read_sql 123import pandasquery = &quot;SELECT * from a&quot;df = pd.read_sql(query, sql_conn)","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"Basics of words embedding","slug":"Basics-of-words-embedding","date":"2018-10-01T10:14:28.000Z","updated":"2020-08-25T15:52:29.370Z","comments":true,"path":"2018/10/01/Basics-of-words-embedding/","link":"","permalink":"http://yoursite.com/2018/10/01/Basics-of-words-embedding/","excerpt":"","text":"Why embedding Natural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing: These encodings provide no useful information regarding the relationships that may exist between the individual symbols. Discrete ids furthermore lead to data sparsity. We may need more data to train statistical models successfully. To address these two problems, word embeddings provide a solution to represent words and their relative meanings densely. Overview Embedding derives from Vector Space Models(VSMs), one of its well-known schemes is Tf-Idf weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that words that appear in the same contexts share semantic meaning. VSMs have two main approaches: Count-based methods and predictive methods. Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense embedding vectors (considered parameters of the model). Among all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular. Count-based Embedding GloVe is an unsupervised learning algorithm for obtaining vector representations for words. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words to capture as much as possible the meaning specified by the juxtaposition of two words. The project page of glove gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration: The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. It is necessary for a model to associate more than a single number to the word pair. Training GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost. project page of glove also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example 12the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ... ‘‘the’’ is followed by 100 floats which are the vector values of this word We can build a dict whose key is words and value is their glove vector 12345678embeddings_index = dict()f = open(&#x27;./glove.6B.100d.txt&#x27;)for line in f: values = line.split() word = values[0] coefs = asarray(values[1:], dtype = &#x27;float32&#x27;) embeddings_index[word] = coefsf.close() When we need to build a embedding layer , we just look up the vectors for input words in the dict. Word2vec Word2vec, as illustrated in the first part, is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. ‘mat’) from source context words (‘the cat sits on the’), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore, CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. We will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model. The Skip-gram model is trained like this, Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. The coding example of how to build and train the Skip-gram model can be found here Next post I will try to use embedding to solve an interesting real world problem.","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"}]},{"title":"What is tf.data and how to use","slug":"What-is-tf-data-and-how-to-use","date":"2018-09-26T19:30:31.000Z","updated":"2020-08-25T17:34:30.899Z","comments":true,"path":"2018/09/26/What-is-tf-data-and-how-to-use/","link":"","permalink":"http://yoursite.com/2018/09/26/What-is-tf-data-and-how-to-use/","excerpt":"","text":"Tf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. Here is the defination of Dataset given by tensorflow.org A Dataset can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a “logical plan” of transformations that act on those elements. To summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed, the following part will explain that as well as some useful APIs in preprocessing data. 1. Data input Dataset can be built from several sources including csv file, numpy array and tensors. From CSV Tf.data provides a convenient API make_csv_dataset to read records from one or more csv files. Suppose the csv file is 123a,b,c,dhow,are,you,mateI,am,fine,thanks We can build a dataset from the above csv in the following way 1dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2) Here batch_size represents how many records would be aquired in a batch We can use Iterator to see what contains in this dataset 12batch = dataset.make_one_shot_iterator().get_next()print(batch[&#x27;a&#x27;]) The result is 1tf.Tensor([&#39;how&#39; &#39;I&#39;], shape&#x3D;(2,), dtype&#x3D;string) make_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this 12how,are,you,mateI,am,fine,thanks We can set header=Falseand column_names=['a','b','c''d'] 1dataset2 &#x3D; tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size&#x3D;2, header&#x3D;False,column_names&#x3D;[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;]) Dataset2 should have the same value with dataset1 From Tensor slices We can also create a dataset from tensors, the related API is tf.data.Dataset.from_tensor_slices() 1dataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([10, 5])) Actually the input of this API is not necessarily tensors, numpy arrays are also adaptable . 1dataset3 &#x3D; tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5))) 2. Data consuming The only way to retrieve the data is Iterator(), Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are make_one_shot_iterator and make_initializable_iterator. make_one_shot_iterator The examples can be find in the first part when we show how to import csv files make_initializable_iterator Compared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example 12345678910# using a placeholderx = tf.placeholder(tf.float32, shape=[None,2])dataset = tf.data.Dataset.from_tensor_slices(x)data = np.random.sample((100,2))iter = dataset.make_initializable_iterator() # create the iteratorel = iter.get_next()with tf.Session() as sess: # feed the placeholder with data sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) print(sess.run(el)) # output [ 0.11342909, 0.81430183] 3. Data proprocessing Tf.data provides several tools for data preprocessing such as batch and shuffle Batch dataset.batch(BATCH_SIZE) given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time. 12345678# BATCHINGtf.enable_eager_execution()BATCH_SIZE = 2x = np.array([1,2,3,4])# make a dataset from a numpy arraydataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()iter.get_next() Output 1&lt;tf.Tensor: id=102, shape=(2,), dtype=int64, numpy=array([1, 2])&gt; Shuffle When preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that. 123456BATCH_SIZE = 2x = np.array([1,2,3,4])# make a dataset from a numpy arraydataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = 10).batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()iter.get_next() Output 1&lt;tf.Tensor: id=115, shape=(2,), dtype=int64, numpy=array([2, 3])&gt;","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://yoursite.com/tags/Tensorflow/"}]},{"title":"[Pandas]Handle missing data","slug":"Pandas-Handle-missing-data","date":"2018-09-23T19:40:37.000Z","updated":"2020-08-25T17:33:39.579Z","comments":true,"path":"2018/09/23/Pandas-Handle-missing-data/","link":"","permalink":"http://yoursite.com/2018/09/23/Pandas-Handle-missing-data/","excerpt":"","text":"Missing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it. ###Produce some data with missing values. 12345678910import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(5, 3), index=[&#x27;a&#x27;, &#x27;c&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;h&#x27;], columns=[&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;])df[&#x27;four&#x27;]=&#x27;bar&#x27;df[&#x27;five&#x27;]= df[&#x27;one&#x27;] &gt; 0df[&#x27;date&#x27;]= pd.Timestamp(&#x27;2018-01-01&#x27;)df2 = df.reindex([&#x27;a&#x27;,&#x27;b&#x27;, &#x27;c&#x27;,&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;,&#x27;g&#x27;, &#x27;h&#x27;])df2 one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 b NaN NaN NaN NaN NaN NaT c -0.297019 1.357266 -0.665749 bar False 2018-01-01 d NaN NaN NaN NaN NaN NaT e 0.311524 -0.328388 0.777467 bar True 2018-01-01 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 g NaN NaN NaN NaN NaN NaT h -1.303842 1.239911 -0.909255 bar False 2018-01-01 Notice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime Detect missing value pandas provides two methods, isna() and notna() 1df2.isna() one two three four five date a False False False False False False b True True True True True True c False False False False False False d True True True True True True e False False False False False False f False False False False False False g True True True True True True h False False False False False False 1df2.notna() one two three four five date a True True True True True True b False False False False False False c True True True True True True d False False False False False False e True True True True True True f True True True True True True g False False False False False False h True True True True True True Cleaning drop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0 1df2.dropna() one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 c -0.297019 1.357266 -0.665749 bar False 2018-01-01 e 0.311524 -0.328388 0.777467 bar True 2018-01-01 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 h -1.303842 1.239911 -0.909255 bar False 2018-01-01 Drop columns with missing data 1df2.dropna(axis = 1) a b c d e f g h All columns are removed… Fill na Na value can be easily filled by fillna() 1df2.fillna(0) one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 00:00:00 b 0.000000 0.000000 0.000000 0 0 0 c -0.297019 1.357266 -0.665749 bar False 2018-01-01 00:00:00 d 0.000000 0.000000 0.000000 0 0 0 e 0.311524 -0.328388 0.777467 bar True 2018-01-01 00:00:00 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 00:00:00 g 0.000000 0.000000 0.000000 0 0 0 h -1.303842 1.239911 -0.909255 bar False 2018-01-01 00:00:00 We can also fill data with respect to their column property 1df2.fillna(value = &#123;&#x27;one&#x27;: 0, &#x27;four&#x27;: &#x27;missing&#x27;, &#x27;five&#x27;: False, &#x27;date&#x27;: pd.Timestamp(&#x27;2000-01-01&#x27;)&#125;) one two three four five date a -0.614301 0.628725 -0.903163 bar False 2018-01-01 b 0.000000 NaN NaN missing False 2000-01-01 c -0.297019 1.357266 -0.665749 bar False 2018-01-01 d 0.000000 NaN NaN missing False 2000-01-01 e 0.311524 -0.328388 0.777467 bar True 2018-01-01 f 0.572373 -0.309563 -0.296276 bar True 2018-01-01 g 0.000000 NaN NaN missing False 2000-01-01 h -1.303842 1.239911 -0.909255 bar False 2018-01-01 Remind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this. 1df2.fillna(value = 0, inplace = True)","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"[Pandas]How to select data","slug":"Pandas-How-to-select-data","date":"2018-09-20T20:49:51.000Z","updated":"2020-08-25T17:34:16.082Z","comments":true,"path":"2018/09/20/Pandas-How-to-select-data/","link":"","permalink":"http://yoursite.com/2018/09/20/Pandas-How-to-select-data/","excerpt":"","text":"This article shows the most common methods regarding data selection First, let’s create a dataframe. 12345import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.rand(5,4), index = [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;], columns= [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;])df A B C D a 0.122248 0.581777 0.972888 0.869366 b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 d 0.077122 0.407514 0.019102 0.553546 e 0.921520 0.157199 0.371028 0.825792 Use [] for columns selection 1df[[&#x27;A&#x27;, &#x27;B&#x27;]] A B a 0.122248 0.581777 b 0.476451 0.453979 c 0.954790 0.747131 d 0.077122 0.407514 e 0.921520 0.157199 Select a range 1df[1:3] # or df[&#x27;b&#x27;:&#x27;c&#x27;] A B C D b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 Use .loc 1df.loc[&#x27;a&#x27;:&#x27;c&#x27;, [&#x27;A&#x27;,&#x27;B&#x27;]] A B a 0.122248 0.581777 b 0.476451 0.453979 c 0.954790 0.747131 Select all columns 1df.loc[&#x27;a&#x27;:&#x27;c&#x27;, :] A B C D a 0.122248 0.581777 0.972888 0.869366 b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 Select with boolean 1df[df[&#x27;A&#x27;]&gt;0.2] A B C D b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 e 0.921520 0.157199 0.371028 0.825792 Select with callable(lambda) 1df[lambda df: df[&#x27;A&#x27;]&gt;0.2] A B C D b 0.476451 0.453979 0.004705 0.644530 c 0.954790 0.747131 0.652936 0.758767 e 0.921520 0.157199 0.371028 0.825792","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"What is Attention and how to use","slug":"What-is-Attention-and-how-to-use","date":"2018-09-20T09:06:45.000Z","updated":"2020-08-25T15:28:27.954Z","comments":true,"path":"2018/09/20/What-is-Attention-and-how-to-use/","link":"","permalink":"http://yoursite.com/2018/09/20/What-is-Attention-and-how-to-use/","excerpt":"","text":"Introduction Attention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT) and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow) Why Attention Attention is a mechanism derived from the seq-seq model which started the era of NMT, in this paper Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation. The model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus. Attention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder. Maybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector. What is Attention Attention is merely a context vector that provides a richer encoding of the source sequence. The vector is computed at every decoder time step. As illustrated in the figure above, the attention computation can be summarized into the following three steps: Compute attention weights based on the current target hidden state and all source state(Figure 1) The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2) Context vector combined with the current target hidden state yields the attention vector(Figure 3) The attention vector is then fed to the next decoding step. the score in Figure 1 is computed as follows: Regarding the score, the methods by which it is calculated lead to different performance. Coding Attention with Tensorflow Suppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3). 123456789101112# Transfer encoder_outputs to attention_states attention_states = tf.transpose(encoder_outputs, [1, 0, 2])# Apply existing attention mechanism attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( num_units, attention_states, memory_sequence_length=source_sequence_length)# Feed to the decoderdecoder_cell = tf.contrib.seq2seq.AttentionWrapper( decoder_cell, attention_mechanism, attention_layer_size=num_units) The rest codes are mostly the same as standard encoder-decoder.","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"}]},{"title":"[Pandas]How to drop columns/rows","slug":"Pandas-How-to-drop-columns-rows","date":"2018-09-18T20:39:25.000Z","updated":"2020-08-25T17:33:48.421Z","comments":true,"path":"2018/09/18/Pandas-How-to-drop-columns-rows/","link":"","permalink":"http://yoursite.com/2018/09/18/Pandas-How-to-drop-columns-rows/","excerpt":"","text":"12345import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;&#x27;A#&#x27;:[1,2,3], &#x27;B#&#x27;:[4,5,6], &#x27;C#&#x27;:[7,8,9]&#125;)df A# B# C# 1 4 7 2 5 8 3 6 9 Drop columns 123df.drop([&#x27;B#&#x27;, &#x27;C#&#x27;], axis = 1)#ordf.drop(columns=[&#x27;B#&#x27;, &#x27;C#&#x27;]) A# 1 2 3 Drop rows 1df.drop([0,1]) A# B# C# 3 6 9","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"[Pandas]How to rename columns","slug":"Pandas-How-to-rename-columns","date":"2018-09-17T21:11:55.000Z","updated":"2020-08-25T17:34:11.571Z","comments":true,"path":"2018/09/17/Pandas-How-to-rename-columns/","link":"","permalink":"http://yoursite.com/2018/09/17/Pandas-How-to-rename-columns/","excerpt":"","text":"12345import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;&#x27;A#&#x27;:[1,2], &#x27;B#&#x27;:[3,4], &#x27;C#&#x27;:[5,6]&#125;)df A# B# C# 1 3 5 2 4 6 Rename with DataFrame.rename() 1df.rename(index= str, columns=&#123;&quot;A#&quot;: &quot;a&quot;, &quot;B#&quot;: &quot;b&quot;&#125;) a b C# 1 3 5 2 4 6 Print df again A# B# C# 1 3 5 2 4 6 Column names did’nt change. To change names permanently, use &quot;inplace=True&quot; 12df.rename(index= str, columns=&#123;&quot;A#&quot;: &quot;a&quot;, &quot;B#&quot;: &quot;b&quot;&#125;, inplace = True)df a b C# 1 3 5 2 4 6 Rename in batch 1df.rename(index= str, columns=lambda x:x.replace(&#x27;#&#x27;,&#x27;&#x27;)) A B C 1 3 5 2 4 6 Lambda is amazing","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"[Pandas]How to import CSV","slug":"Pandas-How-to-import-csv","date":"2018-09-16T15:30:37.000Z","updated":"2020-08-25T17:33:53.343Z","comments":true,"path":"2018/09/16/Pandas-How-to-import-csv/","link":"","permalink":"http://yoursite.com/2018/09/16/Pandas-How-to-import-csv/","excerpt":"","text":"Locate the file There are two ways to locate the csv file, “absolute path” or “relative path” e.g. Absolute path: “/work/project/data/great.csv”(Mac) or “C:\\work\\project\\data\\great.csv”(Windows) Relative path: “data/greate.csv”(Mac) or “data\\great.csv”(Windows) Read with pandas.read_csv() The result is a dataframe Object 12345import pandas as pdimport numpy as np df = pd.read_csv(&#x27;data/great.csv&#x27;)df company locate employees avenue orange New York 10000 4000.0 banana London 2000 1000.0 pinch Paris 4000 5000.0 pear Berlin 3000 NaN No infered header 12df = pd.read_csv(&#x27;data/great.csv&#x27;, header= None)df 0 1 2 3 company locate employees avenue orange New York 10000 4000 banana London 2000 1000 pinch Paris 4000 5000 pear Berlin 3000 NaN Filter rows 12df = pd.read_csv(&#x27;data/great.csv&#x27;, skiprows=[3,4])df company locate employees avenue orange New York 10000 4000 banana London 2000 1000 lambda is also welcomed 12df = pd.read_csv(&#x27;data/great.csv&#x27;, skiprows=lambda x:x/2==1)df company locate employees avenue orange New York 10000 4000.0 pear Berlin 3000 NaN Filter columns(or keep wanted) 12df = pd.read_csv(&#x27;data/great.csv&#x27;, usecols=[&#x27;company&#x27;])df company orange banana pinch pear Other options refer https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"分布式事务与最终一致性","slug":"Distributed-transactions-and-eventual-consistency","date":"2018-08-26T20:38:34.000Z","updated":"2020-08-26T20:42:54.703Z","comments":true,"path":"2018/08/26/Distributed-transactions-and-eventual-consistency/","link":"","permalink":"http://yoursite.com/2018/08/26/Distributed-transactions-and-eventual-consistency/","excerpt":"","text":"为什么需要分布式事务 先摆一个例子： A账户要向B账户转账，100元， 操作的步骤是 A账户先减掉100元 B账户再加上100元 非常简单。。在单个数据库实例的情况下，开启事务，先执行A-100 ，再执行B+100, 事务提交，结束。 然而，由于业务增长， 导致一个数据库实例已经不能满足所有数据存储，此时A与B存在于不同的数据库实例上，这个时候执行这两部操作就要每个实例分别启动一个事务，A减掉后提交，B增加时，其所在机器挂掉了。。 那么A就损失了100元，这显然是不可接受的。 那么这个问题怎么解？ 二阶段提交 解这个问题，需要让跨越多个数据源的一次业务行为，还能保持事务的ACID（原子、一致、隔离、持久）。 如果只是各个参与方各自为战，显然不行，因为缺少一个可以观看全局的组织者， 每个参与者都不知道其他参与者的情况，也就无法判断当前的全局情况，自己是不是要提交。有一个组织者出现后，每个参与者在准备提交前都可以问一下组织者，其他人好了没有，如果得到回答说，都好了，那么各自就开始最终提交。 这个过程其实就是二阶段提交（2PC）的主要思想。 关于二阶段协议，左耳朵耗子的博客（ 分布式系统的事务处理 | | 酷 壳 - CoolShell ）讲了一个绝妙的例子： 一对准夫妻在教堂举办婚礼， 神父通常会先问新娘：你愿意爱你面前的这个人，生老病死。。 ， 新娘回答：我愿意。 然后神父再问新郎：你愿意。。。 新郎回答：我愿意。 最后神父宣布新郎新娘成为一对夫妻。 跟上述描述的二阶段提交是相同的过程， 双方都同意了，神父才会宣布婚姻成立，不可以说只有一个人同意，神父就宣布，你已经结了一半婚，没有意义。 这个过程，画出来流程图是这样的： 分布式事务的问题 二阶段提交过程是分布式事务的一个比较通用的解决方案了， 然而也存在它的局限。由于一次操作可能涉及到几个参与者，那么一次操作所需要花费的时间，就是多个参与者prepare+commit时间的总和。这可能带来两个方面的问题，一是总体耗时的增加，二是由第一个问题带来的资源损耗。 第一个问题，带来的是用户体验上的下降，设想你去便利店消费，要转账给店员10元， 等了好几分钟才转成功，这时后面排队都十几人了。 第二个问题，则是给系统带来的服务容量的挑战，设想淘宝双十一这种场景，每秒钟上万笔交易，涉及大量转账划拨操作，如果全部由二阶段的分布式事务实现，延迟不说，光是机器性能就会很容易到极限。 另一种方案 那么有没有替代方案？ 先回归到问题的本质，分布式事务解决的其实是数据的一致性问题，A给B转账100元， 效果是A减了100，B要增加100， 一增一减要相等，至于这个相等是A操作完之后等着B加了100才结束，还是A操作完之后，有另外一种机制，保证B在后面某个时刻不多不少只增加100。 后一种解法，就是通过引入异步消息的方式，来替换第一种分布式事务的方案。 简单示意图如下； 可以看到，整个过程分为三步： 发起转账一方，记录一条扣减流水。 转账发起方发送消息给转账接收方（接收方也可能是自己）。 接收方写入一条增加流水。 总体思路是先暂存转账记录凭证，利用消息系统的可靠性，保证整个转账操作达到最终一致。 上面只是简略步骤，如果要达到替代两阶段的效果，还需要一些支撑，比如： 消息可能发送失败，需要发送端有恢复机制，定时重试失败流水。 消息真实发送后，A的本地事务不可以失败，这要求要么把消息发送放到最后，要么需要发送消息本身要跟A处于同一个事务，所谓事务消息（这又是一个很大的话题） 消息可能重复发送，但是B不可以重复处理， 需要B保持一个接收消息的记录，每次比对排除重复。 这样看来，使用异步消息的方案，也并不简单，或许一致性问题本身就是一个复杂的课题。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Transaction","slug":"Transaction","permalink":"http://yoursite.com/tags/Transaction/"}]},{"title":"CPU／Load偏高的排查步骤","slug":"How-to-troubleshooting-CPU-load-problems","date":"2018-08-26T20:34:35.000Z","updated":"2020-08-26T20:37:38.997Z","comments":true,"path":"2018/08/26/How-to-troubleshooting-CPU-load-problems/","link":"","permalink":"http://yoursite.com/2018/08/26/How-to-troubleshooting-CPU-load-problems/","excerpt":"","text":"1. 查找 java进程 ps -ef|grep java 2. 查找耗时最高线程 top -Hp #进程id# 3. 将线程id转换为16进制 printf “%x\\n” #线程id# 4. 使用stack查找相应线程栈信息 jstack -l #进程id#| grep #16进制线程id# 5. 开始分析线程堆栈 先复习一下线程的各种状态， 包括 NEW(新建)， Runnable(可运行)， Running(运行) ，Block(阻塞)， Dead(死亡) 其中阻塞还分为 等待阻塞、同步阻塞。 转换关系如下图： (图片来自 https://my.oschina.net/mingdongcheng/blog/139263) 接下来看一下jstack打出来的线程堆栈，大概是这样的： 12345678910&quot;Timer-0&quot; #25 daemon prio&#x3D;5 os_prio&#x3D;31 tid&#x3D;0x00007fd051bb1800 nid&#x3D;0xfabb in Object.wait() [0x0000700008e90000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at java.util.TimerThread.mainLoop(Timer.java:526) - locked &lt;0x00000007a22af328&gt; (a java.util.TaskQueue) at java.util.TimerThread.run(Timer.java:505) Locked ownable synchronizers: - None 需要重点关注有没有以下几个状态： DeadLock 死锁 Waiting on Condition 资源等待 Blocked 阻塞 具体的case分析可以参考这篇文章 Java线程Dump分析工具—jstack - 残雪余香 - 博客园","categories":[{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Profile","slug":"Profile","permalink":"http://yoursite.com/tags/Profile/"}]},{"title":"Tensorflow101","slug":"Tensorflow101","date":"2018-07-10T21:25:42.000Z","updated":"2020-08-31T09:41:24.238Z","comments":true,"path":"2018/07/10/Tensorflow101/","link":"","permalink":"http://yoursite.com/2018/07/10/Tensorflow101/","excerpt":"","text":"Tensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning. How to Install If you have ‘pip’, everything is simple pip install tensorflow Basic Concepts Graph Graph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is h=ReLU(Wx+b) In the view of Tensorflow, the function looks like this Nodes Variables such as W and b , placeholders such as x, are operations such as MatMul, Add are all nodes in the graph. Edges The edges between nodes indicate the data which flow between nodes, in tensorflow data is represented as “tensor” . “tensor” + “flow” = “tensorflow” Codes 1234567891011121314151617181920import tensorflow as tfimport numpy as npif __name__ &#x3D;&#x3D; &#39;__main__&#39;: b &#x3D; tf.Variable(tf.zeros((10,))) W &#x3D; tf.Variable(tf.random_uniform((20, 10), -1, 1)) x &#x3D; tf.placeholder(tf.float32, (10, 20)) h &#x3D; tf.nn.relu(tf.matmul(x, W) + b) writer &#x3D; tf.summary.FileWriter(&#39;.&#x2F;graphs&#39;, tf.get_default_graph()) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(h, &#123;x: np.random.random((10, 20))&#125;) writer.close() Here are the steps described in the above codes. Create a graph using Variables and placeholders. Start a tensorflow session and deploy the graph into the session. Run the session, let the tensors flow. Write processing logs using tools such as tf.summary. Session is the so called execution environment. It needs two parameters which are “Fetches” and “Feeds”. sess.run(fetches, feeds) Fetches: List of graph nodes Feeds: Dictionary mapping from graph nodes to concrete values. In the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))} It would be interesting to see what happened during the running time. We can try TensorBoard. TensorBoard Result The Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.","categories":[{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://yoursite.com/tags/Tensorflow/"}]},{"title":"写一个并发请求的Case","slug":"A-concurrency-requesting-case","date":"2017-08-26T20:31:03.000Z","updated":"2020-08-26T20:32:12.481Z","comments":true,"path":"2017/08/26/A-concurrency-requesting-case/","link":"","permalink":"http://yoursite.com/2017/08/26/A-concurrency-requesting-case/","excerpt":"","text":"有些场景下，你可能要检查代码有没有并发问题，验证幂等或者做一下简单的压力测试， 这时候需要写一个并发请求的程序，那么怎么做呢？ 先上代码 1234567891011121314151617181920212223242526272829303132final long time_start &#x3D; System.currentTimeMillis();ExecutorService service &#x3D; Executors.newFixedThreadPool(10);int client_num &#x3D; 10;final CountDownLatch countDownLatch &#x3D; new CountDownLatch(1);for (int i &#x3D; 0; i &lt; client_num; i++) &#123; Runnable job &#x3D; new Runnable() &#123; @Override public void run() &#123; Worker worker &#x3D; new Worker(); try &#123; countDownLatch.await(); worker.doWork(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125;; service.submit(job);&#125;countDownLatch.countDown();service.shutdown();while (!service.isTerminated()) &#123;&#125;System.out.println(&quot;time consuming is &quot; + (System.currentTimeMillis() - time_start)); 我们来拆解下这个Case，看看都做了些什么： 1. 容器准备 首先，你需要一个线程池来调用任务处理，虽然你可以每次new一个线程来做，但是无论是在生产环境还是测试，都不太建议这样做。本示例用的是 ExecutorService service = Executors.newFixedThreadPool(10) 其效果是构造一个固定线程数的线程池，这个线程池的coreSize和maxSize都是10， 也就是说，最多可以同时处理10个任务，多了的就要排队。 线程池装载完任务后，执行submit , 但是此时任务并未执行，具体原因看下面。 2. 并发执行 这里是比较tricky的一个点，因为要模拟完全并发的情况，就要保证所有任务同一时间触发，这里我们用了一个java.util.concurrent的工具类 CountDownLatch， 从名字大概可以猜测到，这个是一个像栅栏一样的工具，先把羊圈在里面，等count数到0的时候，打开栅栏，放出群羊。。 使用步骤： 首先，初始化countDownLatch计数为1。 然后， 在业务代码里在执行业务逻辑之前我们嵌入了一行 countDownLatch.await() 。最后， 当所有任务都装载完毕提交后，执行countDownLatch.countDown()，如同发令枪一般，所有的任务同时触发，并发量达到client_num的数量10。 3. 任务结束 任务结束的时候调用的是线程池的shutdown方法, 看一下这个方法的注释 Attempts to stop all actively executing tasks, halts the processing of waiting tasks, and returns a list of the tasks that were awaiting execution. 意思是这个方法只是尝试去停止所有任务，但是并不保证都能结束，并且没有结束的任务是以list返回的。 所以后面又跟了一个 12while (!service.isTerminated()) &#123; &#125; 循环一直等待所有的任务结束完毕，最后再统计所有任务的执行时间。 结语 这是一段比较简单的并发测试代码，单元测试足够用，但是如果用做规模化的压力测试，还需要考虑压力机与测试机的连接以及数据热点等一系列问题，这又是个很大的话题了。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"What you need to know for memory-leaks troubleshooting(排查内存泄漏你需要知道的套路)","slug":"What-you-need-to-know-for-memory-leaks-troubleshooting","date":"2017-08-25T16:12:06.000Z","updated":"2020-08-25T17:35:11.324Z","comments":true,"path":"2017/08/25/What-you-need-to-know-for-memory-leaks-troubleshooting/","link":"","permalink":"http://yoursite.com/2017/08/25/What-you-need-to-know-for-memory-leaks-troubleshooting/","excerpt":"","text":"内存泄漏是一个比较棘手的问题，大多数时候没有明显的报错，这给问题排查带来很大困难，好在前人已经遇到很多类似问题，也留下了一些典型的排查套路，大部分时候只需要按图索骥，一步步来即可。 套路之工具篇 ps或者jps ps:查找活跃的进程 典型用法：ps -ef|grep java jps:查找活跃的 java进程* 典型用法：jps -v jmap jmap 可以将java内存打印出来，作为内存分析工具（如MAT）的输入 jinfo 可以用来查看java进程的启动信息，可以看下内存堆大小的设置 jstat 可以用来打印JVM的运行时GC统计信息, 典型用法如 jstat -gcutil 21891 250 2 表示链接到21891这个进程，每250ms用gcutil打印一次，一共取2个实例，通常打印出来的效果是这样的 1234$ jstat -gcutil 19677 1000 2 S0 S1 E O P YGC YGCT FGC FGCT GCT 85.04 0.00 15.15 3.61 44.32 4 0.834 1 0.705 1.538 85.04 0.00 15.15 3.61 44.32 4 0.834 1 0.705 1.538 MAT 可以分析jmaps生成的内存快照，如果你用Eclipse可以直接在eclipse-&gt;help-&gt;install software 输入 http://download.eclipse.org/mat/1.6.1/update-site/ 按照提示安装 PS：如果命令有任何不清楚，男人(man)是你的好朋友：） 套路之实战篇 1. 获取内存dump 进程还活着： 使用ps查找java进程ID ps -ef|grep java 或者 jps jmap 将内存dump到本地 jmap -dump:live,format=b,file=dump.bin 19677 会输出dump.bin文件 已经OOM挂了： 提前在java 运行参数添加 -XX:+HeapDumpOnOutOfMemoryError 表示挂了需要dump当时内存，再添加 -XX:HeapDumpPath=/tmp表示dump内存存放的位置, 这样当应用down掉之后，会输出一个.hprof文件 2. 内存分析 使用MAT导入上一步生成的dump文件，分析之后你会看到类似这样一张图 从这里为入口，可以查看哪些对象占用内存较大，内存泄漏的可疑点在哪里，比如这个case, 点到Leak Suspects这个链接可以看到 这是个比较直观的问题，HashMapEntry对象占用了超过99%的内存空间，可以直接看业务代码哪里HashMap使用出了问题。 这是MAT比较简单的应用，其他高级用法可以看MAT官方文档，或这篇文章 Java程序内存分析：使用mat工具分析内存占用 - 孤剑 - 博客园 结语 相信了解了这些工具与套路，下次遇到可疑内存泄漏或者OOM这样的严重问题，就不会茫然无措了。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"线程池核心知识点","slug":"Key-points-of-threadpool","date":"2017-06-26T20:44:08.000Z","updated":"2020-08-26T20:44:59.294Z","comments":true,"path":"2017/06/26/Key-points-of-threadpool/","link":"","permalink":"http://yoursite.com/2017/06/26/Key-points-of-threadpool/","excerpt":"","text":"—读《java并发编程》笔记 为什么需要线程执行框架 不用线程池是否也有异步执行任务的方案？ 有两种方案，一是单独起一个线程，任务由这个线程顺序执行。二是每个任务创建一个新线程，每个任务都由一个新线程去执行。 然而两种方案各有缺点： 第一种方案，因为是顺序执行，在任务的响应效率有较大影响。 第二种方案，创建线程是有资源消耗的，对于web应用这样的大流量请求，很容易就会达到资源瓶颈。 另外需要考虑的一些问题： 可否知道当前任务在被哪个线程运行了。 任务的执行顺序是怎样的，先入先出？优先队列？ 多少任务正在同时执行 如果当前已经达到了系统性能瓶颈，怎么决定任务的优先级，哪些任务需要被移出执行队列。 任务执行前的准备，以及执行后的清理工作（释放资源）如何做。 为解决上面这些问题，客观上需要有一个管理线程资源的框架。这个框架可以优化线程执行所占资源，定制灵活的执行策略，让异步任务的执行更为健壮。 线程池及其创建 在java体系中，管理线程资源的框架便是线程池（thread pool）。线程池，顾名思义就是线程的资源池，是一种容器资源。它的主要作用就是让线程可以重用，并且保持一定数量的活跃线程，从而使任务响应时间缩短，通过对线程池运行参数的调优，可以达到任务吞吐量和相应时间的最优。 线程池的创建，一般情况下，是通过concurrent包提供的工厂方法Executors 比如 Executors.newFixedThreadPool(int nThreads) 会返回一个固定线程数量的实现了ExecutorService接口的线程池服务，如果当前任务的提交数量超过了这个限定数量，会将其排入一个LinkedBlockingQueue中排队。 除此之外，还有 Executors.newSingleThreadExecutor， Executors.newCachedThreadPool 等等 当然，如果看到过Executors 的源代码， 你就知道，还可以自己创建一个线程池，比如上述Executors.newFixedThreadPool(int nThreads) 其源码就是 return new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue&lt;Runnable&gt;(),threadFactory); 线程池的生命周期 线程池的生命周期有三个阶段，首先是running 然后是 shut down 最终是terminated running 顾名思义就是正常运行状态 shut down 执行shutdownNow()后达到的状态，这个状态下线程池不再接受新任务，等待任务也阻*止。但是此时任务不一定运行完，事实上这个方法会返回一个list包括了还在运行的任务列表。 terminated 就是在shutdown后所有任务已经完成的状态，此时没有任务在运行了。 线程池的参数 corePoolSize 核心线程数。这个参数的含义是线程池需要保持的最低线程数，不管当前有没有任务在运行，线程池中都会保持这个配置数量的活跃线程。当任务数量超过corePoolSize时，会被丢到工作队列中，直到队列满之前，线程池中的线程数不会增加。 maxPoolSize ：最大线程数。该参数指明了一个线程池中同时可以有多少活跃线程同时运行。 keepAliveTime 线程最大存活时间。 当池子中的线程数量超过了corePoolSize时，这个数值代表了那些超出的空闲线程可以存活的最大时间。 workQueue：自定义的线程池队列实例。有三种队列类型可以选择，分别是无界队列，有界队列，以及 同步队列（SynchronousQueue），其中Executors.newFixedThreadPool 使用的LinkedBlockingQueue 属于无界队列， Executors.newCachedThreadPool 使用的是同步队列。其中，使用默认的无界队列，可能会存在在并发量比较高时，队列占用内存过高可能导致其溢出的风险。 queue的配置可以同前面几个参数配合，以达到不同的效果： 比如： 按照优先级处理一组数据，可以传入PriorityQueue 根据指定参数配置优先级， 这个时候队列长队可以设置相对较长，而corePoolSize可以设置相对小，从而达到吞吐量与占用资源的最优配置。 对于大量短时间访问的请求，考虑用SynchronousQueue。同时设置一个相对较短的keepAliveTime以及极大的maxPoolSize, SynchronousQueue的特性是如果要把一个元素加入队列，需要有另外一个线程等待接收，如果另外一个线程没有，则创建一个。这个特性很适合处理即时的消息传递，可以支撑较大的容量。 RejectedExecutionHandler: 队列饱和处理器。 当工作队列已经满时，要决定哪些任务保留哪些任务要丢弃，有几种策略可以选择，比如：AbortPolicy， CallerRunsPolicy，DiscardPolicy。 可以根据实际情况灵活选择。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"My take on distributed system(我所理解的分布式调度)","slug":"My-take-on-distributed-system","date":"2017-03-25T19:37:04.000Z","updated":"2020-08-25T15:54:19.113Z","comments":true,"path":"2017/03/25/My-take-on-distributed-system/","link":"","permalink":"http://yoursite.com/2017/03/25/My-take-on-distributed-system/","excerpt":"","text":"我所理解的分布式调度 对于规模以上的应用来说，调度系统已经是必不可少的组成部分，尤其在基于数据分析的后台应用大量增长的今天，健壮的调度任务管理已经是非常重要的一环，因此多花些时间来分析研究调度系统的设计对于日常的开发与运维具有比较重要的意义。 调度问题是怎么来的 当你的网站是个简单的blog，而且并不需要跟外部交互的时候，你大概不需要调度任务，因为此时网站需要处理的任务仅限于 即时交互 ， 即用户想使用一个功能，你就立即给他就是了，如同你在简书上写一篇文章，一点保存，这篇文章立即就保存到网站的后台服务器中去了，这也是互联网刚出现时候的最早的应用模式。 之后因为网站发展的不错，用户多了起来，就发现需要大量处理一些非即时的任务，比如要定时将用户访问日志归档清理，这个时候一般情况下会在服务器启动一个定时任务，在每天固定时间发起清理归档，又如你想显示一下网站的访客流量、发表文章数、评论统计，由于并非每次用户或者后台管理员每次需要看的时候都去计算一遍，所以可能又需要启动另一个任务来去处理这些数据，这样的任务多了，就需要思考一个问题，哪些任务要先处理，哪些任务要后处理，每个任务要占用多少资源，从而任务调度问题开始出现。 调度什么时候变得复杂 在一个单机的系统，任务并不多的情况下，生活还依然是美好的，利用Linux自带的定时器或者系统框架提供的定时任务，实现好任务执行器，配置好任务触发的时间和频率，然后每天等待它自动触发，数据生成，任务搞定，似乎调度也没那么困难。 然而好景不长，伴随着网站的发展，你发现任务处理的越来越慢，甚至偶尔会有任务超时的情况，原因是每天用户产生的数据量越来越大，每次任务捞起的数量已经超载，依次执行完每个任务，可能已是最初执行时间的几倍。 这时稍有点经验你便会想到，任务没必要顺序执行啊，所以弄个线程池，起多个线程同时来捞数据然后执行，同时配置动态调整每次数据捞取的数量，增大执行频率，一系列组合拳打出去之后，调度任务又恢复正常，由于增加了并发数，甚至执行的比开始还更快了，就这样业务高峰便又顺利度过了。 调度什么时候变得更加复杂 之前所描述的情况基本上都在单机的情况，网站的QPS（每秒处理的任务数量）基本在500以下，通过一系列的参数调优，串行变并行，任务运行的很平稳。然而当任务变的规模更大，比如十倍于当前，一台机器已经不能处理所有的任务，这时候需要增加更多的机器，将任务分配到不同的机器上，于是便有了所谓的分布式调度的问题。 分布式是目前稍大型的网站不可避免的问题，这种处理方案有很多好处，比如可以利用廉价的机器，可以（理论上）无限水平拓展，同时也带来了一系列棘手的问题，机器之间的网络通信，如何把流量均匀的分布于不同流量，如果有机器宕机了如何处理，每个问题都已经是一个庞大的技术领域。 对于调度的分布式问题，首先要解决的便是如何把任务分发到不同的机器上，这要求调度系统通常要至少分为两层，第一层决定一共要处理哪些任务并把任务分发到哪些机器上处理，第二层接到任务后具体执行。虽然描述起来很简单，但是这个处理过程实际上需要大量支撑系统的支持，比如在任务分发时如何判断哪些机器还活着可以处理任务，这可能需要一个可以感知整个集群运行状态的配置中心，又比如任务如何分发，是采用消息还是实时服务接口，如果是用消息派发则需要消息系统，如果是实时服务，又需要类似dubbo这样的分布式服务框架。当系统达到这个复杂度，已经不是将任务捞起并处理这么单纯，而是多个关联系统复杂性的叠加。 分布式调度的难题 虽然分布式调度带来了复杂度的上升，但是它的水平拓展能力完美的适配了网站的规模发展，直到有一天你看到了类似这个错误： org.springframework.transaction.CannotCreateTransactionException 数据库连接池已经打满，由于单个数据库的连接数是一定的，这意味着数据库变成了资源瓶颈，这时候给任务处理happy的加机器已经不能提高系统的整体处理能力，这就如同你要运走一群人，给你提供的车辆是无限的，但是汽油确是一定的。当然数据库也是可以拓展的，但是考虑到数据迁移的复杂性，这几乎将问题的复杂度提高了一个等级。因此，一个分布式系统的吞吐量，在很大程度上是与数据库处理能力做权衡的结果。 写在最后 分布式调度是一个巨大的话题，并且还在随着实际任务复杂度的提高而快速的更新，上面这些思考与总结也只是浮光掠影，还需要在实际工作中再深入体会与仔细研究。","categories":[{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://yoursite.com/tags/Distributed-System/"}]}],"categories":[{"name":"杂文","slug":"杂文","permalink":"http://yoursite.com/categories/%E6%9D%82%E6%96%87/"},{"name":"Quant","slug":"Quant","permalink":"http://yoursite.com/categories/Quant/"},{"name":"Data&AI","slug":"Data-AI","permalink":"http://yoursite.com/categories/Data-AI/"},{"name":"Backend","slug":"Backend","permalink":"http://yoursite.com/categories/Backend/"}],"tags":[{"name":"Health","slug":"Health","permalink":"http://yoursite.com/tags/Health/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/tags/Algorithm/"},{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://yoursite.com/tags/Tensorflow/"},{"name":"Transaction","slug":"Transaction","permalink":"http://yoursite.com/tags/Transaction/"},{"name":"Profile","slug":"Profile","permalink":"http://yoursite.com/tags/Profile/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://yoursite.com/tags/Distributed-System/"}]}